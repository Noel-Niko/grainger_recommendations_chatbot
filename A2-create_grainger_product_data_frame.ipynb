{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a11e5f910934738",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T14:01:41.190143Z",
     "start_time": "2024-06-26T14:01:41.185289Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "domain = \"https://www.grainger.com\"\n",
    "local_domain = urlparse(domain).netloc\n",
    "# Create necessary directories if they don't exist\n",
    "if not os.path.exists(\"text/\"):\n",
    "    os.mkdir(\"text/\")\n",
    "if not os.path.exists(f\"text/{local_domain}/\"):\n",
    "    os.mkdir(f\"text/{local_domain}/\")\n",
    "if not os.path.exists(\"processed\"):\n",
    "    os.mkdir(\"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a61fd1fe2b624523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T14:01:41.988839Z",
     "start_time": "2024-06-26T14:01:41.986386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches in '1DKW3_1.pdf': ['1DKW3']\n",
      "Matches in '3VE59C-Operating-Instructions-and-Parts-Manual.pdf': ['3VE59C']\n",
      "Matches in '_3M-Disposable-Respirator-Dual-4JF99?opr=PDPBRDSP&analytics=dsbrItems_5ZZZ6.txt': ['4JF99', 'PDPBRDS', '5ZZZ6']\n"
     ]
    }
   ],
   "source": [
    "# TEST \n",
    "import re\n",
    "\n",
    "# Pattern to match product skus/codes\n",
    "regex_pattern = re.compile(r'[A-Z0-9]{5,7}')\n",
    "# Test strings\n",
    "test_strings = [\n",
    "    \"1DKW3_1.pdf\",\n",
    "    \"3VE59C-Operating-Instructions-and-Parts-Manual.pdf\",\n",
    "    \"_3M-Disposable-Respirator-Dual-4JF99?opr=PDPBRDSP&analytics=dsbrItems_5ZZZ6.txt\"\n",
    "]\n",
    "\n",
    "# Extract product codes from test strings\n",
    "for test in test_strings:\n",
    "    matches = regex_pattern.findall(test)\n",
    "    print(f\"Matches in '{test}': {matches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d225d8e17be55ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T14:01:42.816356Z",
     "start_time": "2024-06-26T14:01:41.990122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not read file GraingerWebScrape/www.grainger.com/1DKW3_2.pdf: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n",
      "Could not read file GraingerWebScrape/www.grainger.com/4NHG9_1.pdf: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n",
      "Could not read file GraingerWebScrape/www.grainger.com/1DKW3_3.pdf: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n",
      "Could not read file GraingerWebScrape/www.grainger.com/1DKW3_1.pdf: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n",
      "Could not read file GraingerWebScrape/www.grainger.com/4NHG9_2.pdf: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n",
      "Could not read file GraingerWebScrape/www.grainger.com/TraumaCube__XE7K_v1.pdf: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n",
      "Could not read file GraingerWebScrape/www.grainger.com/1106-1600_PI_en_US1__JQ1I.pdf: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n",
      "Could not read file GraingerWebScrape/www.grainger.com/3VE59C-Operating-Instructions-and-Parts-Manual.pdf: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n",
      "Could not read file GraingerWebScrape/www.grainger.com/QMARK-Radiant-Desk-Heater-Installation-Instructions.pdf: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n",
      "Could not read file GraingerWebScrape/www.grainger.com/3PEA8_1.pdf: 'utf-8' codec can't decode byte 0xc7 in position 10: invalid continuation byte\n",
      "Could not read file GraingerWebScrape/www.grainger.com/53TY91_1.pdf: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n",
      "Could not read file GraingerWebScrape/www.grainger.com/55KH88Manual__LFV2_v1.pdf: 'utf-8' codec can't decode byte 0xa1 in position 11: invalid start byte\n",
      "Could not read file GraingerWebScrape/www.grainger.com/2MY17_1.pdf: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n",
      "Could not read file GraingerWebScrape/www.grainger.com/53TY90_1.pdf: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n",
      "Total product codes found: 9651\n",
      "Product codes saved in 'all_product_codes.json'\n"
     ]
    }
   ],
   "source": [
    "# PULL product codes from web scraped data and save as json\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Define the regex pattern for product codes\n",
    "regex_pattern = re.compile(r'[A-Z0-9]{5,7}')\n",
    "\n",
    "# Directory containing the files\n",
    "directory = 'GraingerWebScrape/www.grainger.com'\n",
    "\n",
    "# List to store found product codes\n",
    "product_codes = []\n",
    "\n",
    "# Function to extract product codes from text\n",
    "def extract_product_codes(text):\n",
    "    return regex_pattern.findall(text)\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        # Check for product codes in the file name\n",
    "        codes_in_filename = extract_product_codes(file)\n",
    "        product_codes.extend(codes_in_filename)\n",
    "\n",
    "        # Check for product codes in the file content\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                codes_in_content = extract_product_codes(content)\n",
    "                product_codes.extend(codes_in_content)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read file {file_path}: {e}\")\n",
    "\n",
    "# Remove duplicates by converting the list to a set and back to a list\n",
    "product_codes = list(set(product_codes))\n",
    "\n",
    "# Save all product codes to a single JSON file\n",
    "with open('all_product_codes.json', 'w') as f:\n",
    "    json.dump(product_codes, f, indent=4)\n",
    "\n",
    "print(f\"Total product codes found: {len(product_codes)}\")\n",
    "print(f\"Product codes saved in 'all_product_codes.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a23af246e9826876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T01:42:47.283516Z",
     "start_time": "2024-06-29T01:42:30.363990Z"
    }
   },
   "source": [
    "# FETCH DATA ON PRODUCT CODES FROM URL AND SAVE AS DATA FRAME\n",
    "# UPDATE THE JSON FILE WITH VALID PRODUCT CODES\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Base URL and headers for the API\n",
    "base_url = \"https://mobile-rest-qa.nonprod.graingercloud.com/v1/product/detail\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Function to fetch and process data\n",
    "def fetch_product_details(skus):\n",
    "    params = {\n",
    "        \"partNumbers\": skus,\n",
    "        \"extraInfo\": \"false\"\n",
    "    }\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            results = []\n",
    "            for item in data:\n",
    "                brand = item.get(\"brand\", {}).get(\"name\", \"N/A\")\n",
    "                code = item.get(\"code\", \"N/A\")\n",
    "                name = item.get(\"name\", \"N/A\")\n",
    "                picture_url = item.get(\"pictureUrl600\", \"N/A\")\n",
    "                price = item.get(\"priceData\", {}).get(\"formattedPrice\", \"N/A\")\n",
    "                description = item.get(\"productDetailsDescription\", \"N/A\")\n",
    "\n",
    "                results.append({\n",
    "                    \"Brand\": brand,\n",
    "                    \"Code\": code,\n",
    "                    \"Name\": name,\n",
    "                    \"PictureUrl600\": picture_url,\n",
    "                    \"Price\": price,\n",
    "                    \"Description\": description\n",
    "                })\n",
    "\n",
    "            return pd.DataFrame(results) if results else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response for {skus}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to fetch details for {skus}: Status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Load the product codes from the JSON file\n",
    "with open('all_product_codes.json', 'r') as f:\n",
    "    product_codes = json.load(f)\n",
    "\n",
    "print(f\"Total product codes found: {len(product_codes)}\")\n",
    "\n",
    "# Product codes in chunks of 100\n",
    "chunk_size = 1\n",
    "chunks = [product_codes[i:i + chunk_size] for i in range(0, len(product_codes), chunk_size)]\n",
    "\n",
    "# Iterate over each chunk for API requests\n",
    "df = pd.DataFrame(columns=[\"Brand\", \"Code\", \"Name\", \"PictureUrl600\", \"Price\", \"Description\"])\n",
    "failed_chunks = []\n",
    "for chunk in chunks:\n",
    "    try:\n",
    "        details = fetch_product_details(chunk)\n",
    "        if details is not None:\n",
    "            df = pd.concat([df, details], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No details fetched for chunk: {chunk}. Removing from source.\")\n",
    "            failed_chunks.extend(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch details for chunk: {chunk}, Error: {e}\")\n",
    "        failed_chunks.extend(chunk)\n",
    "\n",
    "# # Remove failed product codes from the source list\n",
    "# product_codes = [code for code in product_codes if code not in failed_chunks]\n",
    "\n",
    "# # Save the updated product codes to the JSON file\n",
    "# with open('all_product_codes.json', 'w') as f:\n",
    "#     json.dump(product_codes, f, indent=4)\n",
    "\n",
    "# # Remove rows where all columns are NaN\n",
    "# df = df.dropna(how='all')\n",
    "\n",
    "# Ensure all column names are strings\n",
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "# Save to Parquet\n",
    "os.makedirs('processed', exist_ok=True)\n",
    "df.to_parquet('processed/grainger_products.parquet', index=False)\n",
    "print(\"Product details have been saved to 'processed/grainger_products.parquet'\")\n",
    "print(\"\\nHead of DataFrame:\")\n",
    "print(df.head(), \"\\n\")\n",
    "print(\"Tail of DataFrame:\")\n",
    "print(df.tail(), \"\\n\")\n",
    "print(\"Size of DataFrame:\", df.size, \"\\n\")\n",
    "print(\"Values in DataFrame:\")\n",
    "print(df.values)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total product codes found: 3237\n",
      "Failed to fetch details for ['1A912']: Status code 404\n",
      "No details fetched for chunk: ['1A912']. Removing from source.\n",
      "Failed to fetch details for ['3XRG7']: Status code 404\n",
      "No details fetched for chunk: ['3XRG7']. Removing from source.\n",
      "Failed to fetch details for ['1CXK5']: Status code 404\n",
      "No details fetched for chunk: ['1CXK5']. Removing from source.\n",
      "Failed to fetch details for ['39R838']: Status code 404\n",
      "No details fetched for chunk: ['39R838']. Removing from source.\n",
      "Failed to fetch details for ['4KN42']: Status code 404\n",
      "No details fetched for chunk: ['4KN42']. Removing from source.\n",
      "Failed to fetch details for ['447Y46']: Status code 404\n",
      "No details fetched for chunk: ['447Y46']. Removing from source.\n",
      "Failed to fetch details for ['404K04']: Status code 404\n",
      "No details fetched for chunk: ['404K04']. Removing from source.\n",
      "Failed to fetch details for ['492U19']: Status code 404\n",
      "No details fetched for chunk: ['492U19']. Removing from source.\n",
      "Failed to fetch details for ['60NT66']: Status code 404\n",
      "No details fetched for chunk: ['60NT66']. Removing from source.\n",
      "Failed to fetch details for ['12Y499']: Status code 404\n",
      "No details fetched for chunk: ['12Y499']. Removing from source.\n",
      "Failed to fetch details for ['493R87']: Status code 404\n",
      "No details fetched for chunk: ['493R87']. Removing from source.\n",
      "Failed to fetch details for ['8PN28']: Status code 404\n",
      "No details fetched for chunk: ['8PN28']. Removing from source.\n",
      "Failed to fetch details for ['56JZ89']: Status code 404\n",
      "No details fetched for chunk: ['56JZ89']. Removing from source.\n",
      "Failed to fetch details for ['20TK81']: Status code 404\n",
      "No details fetched for chunk: ['20TK81']. Removing from source.\n",
      "Failed to fetch details for ['491V86']: Status code 404\n",
      "No details fetched for chunk: ['491V86']. Removing from source.\n",
      "Failed to fetch details for ['2TUY8']: Status code 404\n",
      "No details fetched for chunk: ['2TUY8']. Removing from source.\n",
      "Failed to fetch details for ['2TGA9']: Status code 404\n",
      "No details fetched for chunk: ['2TGA9']. Removing from source.\n",
      "Failed to fetch details for ['11Z968']: Status code 404\n",
      "No details fetched for chunk: ['11Z968']. Removing from source.\n",
      "Failed to fetch details for ['2E392']: Status code 404\n",
      "No details fetched for chunk: ['2E392']. Removing from source.\n",
      "Failed to fetch details for ['43A739']: Status code 404\n",
      "No details fetched for chunk: ['43A739']. Removing from source.\n",
      "Failed to fetch details for ['491V10']: Status code 404\n",
      "No details fetched for chunk: ['491V10']. Removing from source.\n",
      "Failed to fetch details for ['53UJ35']: Status code 404\n",
      "No details fetched for chunk: ['53UJ35']. Removing from source.\n",
      "Failed to fetch details for ['60PT11']: Status code 404\n",
      "No details fetched for chunk: ['60PT11']. Removing from source.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 65\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunks:\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m         details \u001B[38;5;241m=\u001B[39m \u001B[43mfetch_product_details\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m details \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     67\u001B[0m             df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([df, details], ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[1], line 20\u001B[0m, in \u001B[0;36mfetch_product_details\u001B[0;34m(skus)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch_product_details\u001B[39m(skus):\n\u001B[1;32m     16\u001B[0m     params \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     17\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpartNumbers\u001B[39m\u001B[38;5;124m\"\u001B[39m: skus,\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mextraInfo\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     19\u001B[0m     }\n\u001B[0;32m---> 20\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[1;32m     22\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/requests/api.py:73\u001B[0m, in \u001B[0;36mget\u001B[0;34m(url, params, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mget\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/requests/api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/requests/adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    664\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    786\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[1;32m    805\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    534\u001B[0m \u001B[38;5;66;03m# Receive the response from the server\u001B[39;00m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 536\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    538\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/urllib3/connection.py:464\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresponse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPResponse\n\u001B[1;32m    463\u001B[0m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[0;32m--> 464\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    466\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    467\u001B[0m     assert_header_parsing(httplib_response\u001B[38;5;241m.\u001B[39mmsg)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/http/client.py:1375\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1374\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1375\u001B[0m         \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1376\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[1;32m   1377\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/http/client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 318\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[1;32m    320\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/http/client.py:279\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 279\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    707\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/ssl.py:1307\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1303\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1304\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1305\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1306\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1307\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1308\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1309\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/ssl.py:1163\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1161\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1162\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1163\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1164\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1165\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f5676aa9f97a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
