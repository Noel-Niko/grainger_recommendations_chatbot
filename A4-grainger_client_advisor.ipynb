{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd7228-5049-4f1e-ac7e-04696aaea0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f657c",
   "metadata": {},
   "source": [
    "### <font color='red'>Setup</font> \n",
    "---\n",
    "<font color='red'>⚠️ ⚠️ ⚠️</font> \n",
    "Before running this notebook, ensure you've run the [Set-Up Bedrock notebook](./set-up_bedrock.ipynb) notebook. <font color='red'>⚠️ ⚠️ ⚠️</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06fbee0-9db9-4a9f-8792-b71a670d82f8",
   "metadata": {},
   "source": [
    "## Configure Bedrock\n",
    "\n",
    "Create the necessary clients to invoke Bedrock models. If you need to pass in a certain role then set those values by uncommenting the section below.\n",
    "\n",
    "First we instantiate using Anthropic Claude V2 for text generation, and Titan Embeddings G1 - Text for text embeddings.\n",
    "\n",
    "Note: Many different models are available with Bedrock. Replace the `model_id` to change the model.\n",
    "\n",
    "`llm = Bedrock(model_id=\"anthropic.claude-v2\")`\n",
    "\n",
    "Information on available model IDs [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5873db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2628ef76-b545-4bf9-ade6-5b9f5c2c62b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n",
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from IPython.display import Image\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=False)\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n",
    "\n",
    "model_parameter = {\n",
    "    \"temperature\": 0.0, \n",
    "    \"top_p\": .5, \n",
    "    \"top_k\": 250, \n",
    "    \"max_tokens_to_sample\": 2000, \n",
    "    \"stop_sequences\": [\"\\n\\n Human: bye\"]\n",
    "}\n",
    "llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    model_kwargs=model_parameter, \n",
    "    client=bedrock_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473baff",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "This notebook is using the LangChain framework where it has integrations with different services and the following tools:\n",
    "\n",
    "- **LLM (Large Language Model)**: Anthropic Claude V2 available through Amazon Bedrock\n",
    "\n",
    "  This model will be used to understand the document chunks and provide an answer in human friendly manner.\n",
    "- **Embeddings Model**: Amazon Titan Embeddings available through Amazon Bedrock\n",
    "\n",
    "  This model will be used to generate a numerical representation of the textual documents\n",
    "- **Document Loader**: [S3FileLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.s3_file.S3FileLoader.html) and PDF Loader available through LangChain\n",
    "\n",
    "  This is the loader that can load the documents from a source, for the sake of this notebook we are loading the sample files from a local path. This could easily be replaced with a loader to load documents from enterprise internal systems.\n",
    "\n",
    "- **Vector Store**: In-Memory store FAISS\n",
    "\n",
    "  The index helps to compare the input embedding and the document embeddings to find relevant document\n",
    "- **Wrapper**: wraps index, vector store, embeddings model and the LLM to abstract away the logic from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ad4159-24aa-486d-8872-328109f0f642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:57:29.378028Z",
     "start_time": "2024-06-23T17:57:29.368739Z"
    }
   },
   "outputs": [],
   "source": [
    "#TEST\n",
    "customer_input = \"I am  looking for a fan for our warehouses that will be energy efficient and assist with air circulation for the large open indoor warehouse. I need to be able to purchase in bulk for several locations in Indiana.\"\n",
    "\n",
    "\n",
    "# Customer id to infuse order history and delivery address\n",
    "customer_id = \"2\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0c499ed-f5a2-4b26-8ef6-991c39fb2f63",
   "metadata": {},
   "source": [
    "## Extract `product relevant` information\n",
    "\n",
    "Product data is stored in the grainger_product.parquet.\n",
    "\n",
    "# Without market research the assumption was made that Grainger Customers are meaningfully segmented by Find industry, size, Sustainability Focus, Inventory Manager, and the location\n",
    "\n",
    "TODO: Add to initial data icons and provide user with gov approved (JWOD, GSA, TAA) or environmentally conscious status.\n",
    "\n",
    "\n",
    "The prompt instructs the LLM to fetch the relevant information from the user prompt based on the above to facilitate a query of the source product data. \n",
    "\n",
    "Note the specific prompt template for the `entity extraction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c4210f-f08b-49d2-b9b7-45da05657fc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:57:45.048316Z",
     "start_time": "2024-06-23T17:57:44.819506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<attributes>\n",
      "{\n",
      "  \"industry\": \"Warehousing\",\n",
      "  \"size\": \"Large Enterprises\", \n",
      "  \"SustainabilityFocus\": true,\n",
      "  \"InventoryManager\": true,\n",
      "  \"location\": \"Indiana\"\n",
      "}\n",
      "</attributes>\n"
     ]
    }
   ],
   "source": [
    "# Identify product attributes from customer prompt to generate better results\n",
    "ner_prompt = \"\"\"Human: Find industry, size, Sustainability Focus, Inventory Manager, and the location in the customer input.\n",
    "Instructions:\n",
    "The industry can be one of the following: Manufacturing, Warehousing, Government and Public Safety, Education, Food and Beverage Distribution, Hospitality, Property Management, Retail, or Other\n",
    "The size can be one of the following: Small Businesses (Smaller companies might prioritize cost-effective solutions and fast shipping options), or Large Enterprises (Larger organizations may require more comprehensive solutions, including strategic services like inventory management and safety consulting), Womens, Other\n",
    "The Sustainability Focused true or false meaning Environmentally Conscious Buyers: Customers interested in sustainability solutions, looking for products that focus on energy management, water conservation, waste reduction, and air quality improvement, or NOT Environmentally Conscious Buyers,\n",
    "The Inventory Manager true or false meaning a purchaser in large amounts to supply an organizational group, versus an individual user purchasing for personal use, \n",
    "The output must be in JSON format inside the tags <attributes></attributes>\n",
    "\n",
    "If the information of an entity is not available in the input then don't include that entity in the JSON output\n",
    "\n",
    "Begin!\n",
    "\n",
    "Customer input: {customer_input}\n",
    "Assistant:\"\"\"\n",
    "entity_extraction_result = llm(ner_prompt.format(customer_input=customer_input)).strip()\n",
    "print(entity_extraction_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaddb1cd",
   "metadata": {},
   "source": [
    "#### Extract values into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ce47b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'industry': 'Warehousing',\n",
       " 'size': 'Large Enterprises',\n",
       " 'SustainabilityFocus': True,\n",
       " 'InventoryManager': True,\n",
       " 'location': 'Indiana'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "result = re.search('<attributes>(.*)</attributes>', entity_extraction_result, re.DOTALL)\n",
    "attributes = json.loads(result.group(1))\n",
    "attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c556d2c-d133-4bce-a4a2-27721191f8bb",
   "metadata": {},
   "source": [
    "## Use Retrieval Augmented Generation (RAG) \n",
    "\n",
    "Note: documents loaded with [S3FileLoader available under LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/) can be split into smaller chunks. The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limit of the length of input tokens limited to 8k tokens, which roughly translates to ~32000 characters. For the sake of this use-case we are creating chunks of roughly 1000 characters with an overlap of 100 characters using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html).\n",
    "\n",
    "Below: fetching our productdata and creating the embeddings for \n",
    "1. Product catalog description\n",
    "2. Customer reviews\n",
    "\n",
    "# TODO: ADD order history for logged in user\n",
    "3. Order History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a93668d2-22c2-424f-9e92-05d1f5a0194c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load file from: processed/grainger_products.parquet\n",
      "File loaded successfully!\n",
      "                          Brand    Code  \\\n",
      "0  LION FIRE BOOTS BY THOROGOOD   3XRG7   \n",
      "1           GLOWEAR BY ERGODYNE   1CXK5   \n",
      "2                      CARHARTT  491V68   \n",
      "3                      TRIPLETT  794UC5   \n",
      "4                        DEWALT  492U19   \n",
      "\n",
      "                                                Name  \\\n",
      "0  Insulated Firefighter Boots: Insulated, Steel,...   \n",
      "1  GLOWEAR BY ERGODYNE Baseball Cap: Orange, Univ...   \n",
      "2  CARHARTT Bib Overalls: Men's, XL ( 42 in x 32 ...   \n",
      "3  TRIPLETT Combustible Gas Detector: Audible/Vib...   \n",
      "4  DEWALT Heated Jacket: Men's, S, Black, Up to 9...   \n",
      "\n",
      "                                       PictureUrl600    Price  \\\n",
      "0  https://static.grainger.com/rp/s/is/image/Grai...  $197.55   \n",
      "1  https://static.grainger.com/rp/s/is/image/Grai...   $13.93   \n",
      "2  https://static.grainger.com/rp/s/is/image/Grai...   $95.79   \n",
      "3  https://static.grainger.com/rp/s/is/image/Grai...  $134.56   \n",
      "4  https://static.grainger.com/rp/s/is/image/Grai...  $363.87   \n",
      "\n",
      "                                         Description  \n",
      "0  <p>Thorogood® 807-6003 Insulated Boots are des...  \n",
      "1  <p>Baseball hats have a curved brim that shade...  \n",
      "2  <p>Overalls (sometimes called bib overalls) ar...  \n",
      "3                                               None  \n",
      "4  <p>Men's electronically heated jackets cover t...  \n"
     ]
    }
   ],
   "source": [
    "parquet_file_path = \"processed/grainger_products.parquet\"\n",
    "print(\"Attempting to load file from:\", parquet_file_path)\n",
    "\n",
    "# Now attempt to load the file\n",
    "try:\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "    print(\"File loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Error loading file:\", e)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc692daa-6f4d-41bf-a724-1a1448aff55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Titan Embeddings Model...\n",
      "Titan Embeddings Model initialized.\n",
      "Loading data from parquet file: processed/grainger_products.parquet...\n",
      "Data loaded successfully.\n",
      "Concatenating 'Name', 'Description', and 'Code' columns...\n",
      "Creating FAISS vector store based on concatenated text content...\n"
     ]
    }
   ],
   "source": [
    "## Approach 1: Using Textual Columns Directly\n",
    "# To focus on text-based similarity (e.g., based on 'Name' and 'Description'), columns are concatenated\n",
    "# FAISS.from_texts is used. This allows more granular control.\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the Titan Embeddings Model\n",
    "print(\"Initializing Titan Embeddings Model...\")\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_runtime)\n",
    "print(\"Titan Embeddings Model initialized.\")\n",
    "\n",
    "# Load the Grainger products data from parquet file at path relative to the notebook\n",
    "parquet_file_path = \"processed/grainger_products.parquet\"\n",
    "print(f\"Loading data from parquet file: {parquet_file_path}...\")\n",
    "products_df = pd.read_parquet(parquet_file_path)\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# Concatenate 'Name', 'Description', and 'Code' columns\n",
    "print(\"Concatenating 'Name', 'Description', and 'Code' columns...\")\n",
    "products_df['text_content'] = products_df['Name'].fillna('') + ' ' + products_df['Description'].fillna('') + ' ' + products_df['Code'].fillna('')\n",
    "\n",
    "# Create FAISS vector store from concatenated text content\n",
    "print(\"Creating FAISS vector store based on concatenated text content...\")\n",
    "vectorstore_faiss = FAISS.from_texts(products_df['text_content'], bedrock_embeddings)\n",
    "print(\"FAISS vector store created.\")\n",
    "\n",
    "# Display count of vectors added during creation\n",
    "print(f\"Number of vectors in the FAISS vector store: {vectorstore_faiss.num_vectors}\")\n",
    "\n",
    "# Print the dimensionality of the vectors in the FAISS vector store\n",
    "print(f\"Vector dimensionality in FAISS vector store: {vectorstore_faiss.vector_dim}\")\n",
    "\n",
    "# TEST: Process a query based on a product code\n",
    "customer_code = \"1CXK5\"  # Example product code to search\n",
    "print(f\"Processing customer query for product with code: {customer_code}...\")\n",
    "query_result = products_df[products_df['Code'] == customer_code]\n",
    "\n",
    "if not query_result.empty:\n",
    "    print(\"Product details found:\")\n",
    "    print(query_result)\n",
    "else:\n",
    "    print(f\"No product found with code: {customer_code}\")\n",
    "\n",
    "# TEST: Process a query based on customer input\n",
    "customer_input = \"Men's insulated boots\"\n",
    "print(f\"Processing customer input: {customer_input}...\")\n",
    "query_embedding = vectorstore_faiss.embedding_function(customer_input)\n",
    "print(\"Customer input processed.\")\n",
    "\n",
    "# Convert query embedding to numpy array\n",
    "np_array_query_embedding = np.array(query_embedding)\n",
    "print(\"Query embedding converted to numpy array.\")\n",
    "\n",
    "# Print the resulting query embedding\n",
    "print(\"Resulting query embedding:\")\n",
    "print(np_array_query_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47dcae9-07c3-4cee-87d7-371e7f9f18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERSION 2: AS A DOCUMENT\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the Titan Embeddings Model\n",
    "print(\"Initializing Titan Embeddings Model...\")\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_runtime)\n",
    "print(\"Titan Embeddings Model initialized.\")\n",
    "\n",
    "# Example: Creating structured documents\n",
    "documents = [\n",
    "    {\n",
    "        'content': f\"{row['Code']} {row['Name']} {row['Brand']} {row['Description'] if pd.notna(row['Description']) else ''}\",  # Concatenated text content\n",
    "        'metadata': {\n",
    "            'Brand': row['Brand'],\n",
    "            'Code': row['Code'],\n",
    "            'Name': row['Name'],\n",
    "            'Description': row['Description'],\n",
    "            'Price': row['Price']\n",
    "            # Add other metadata fields as needed\n",
    "        }\n",
    "    }\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# Print the structured documents\n",
    "print(\"Structured documents created:\")\n",
    "for doc in documents:\n",
    "    print(doc)\n",
    "\n",
    "# Create FAISS vector store from structured documents\n",
    "print(\"Creating FAISS vector store from structured documents...\")\n",
    "vectorstore_faiss_doc = FAISS.from_documents(documents, bedrock_embeddings)\n",
    "print(\"FAISS vector store created.\")\n",
    "\n",
    "# Example: Print some details about the created vector store\n",
    "print(f\"Number of vectors in the FAISS vector store: {len(vectorstore_faiss_doc)}\")\n",
    "\n",
    "# Assuming `customer_input` is defined elsewhere in your code\n",
    "# Replace this with actual customer input or query as needed\n",
    "customer_input = \"Men's insulated boots\"\n",
    "print(f\"Processing customer input: {customer_input}...\")\n",
    "query_embedding_doc = vectorstore_faiss_doc.embedding_function(customer_input)\n",
    "print(\"Customer input processed.\")\n",
    "\n",
    "# Convert query embedding to numpy array\n",
    "np_array_query_embedding_doc = np.array(query_embedding_doc)\n",
    "print(\"Query embedding converted to numpy array.\")\n",
    "\n",
    "# Print the resulting query embedding\n",
    "print(\"Resulting query embedding:\")\n",
    "print(np_array_query_embedding_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923edcb-2dd4-4df2-ae56-462935eae428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1fd6c9d-a512-4f1e-ba8f-486d8b88f71f",
   "metadata": {},
   "source": [
    "## Generate *`n`* style recommendations\n",
    "\n",
    "Make a query to embed the LLM using customer input. Using LangChain for orchestration of RAG. It also provides a framework for orchestrating RAG flows with what purpose built \"chains\". In this section, we will see how to be a [retrieval chain](https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa) which is more comprehensive and robust than the original retrieval system we built above.\n",
    "\n",
    "The workflow we used above follows the following process:\n",
    "1. User input is received.\n",
    "2. User input is queried against the vector database to retrieve the relevant products\n",
    "3. Product description and chat memory are inserted into a new prompt to respond to the user input.\n",
    "4. This output is fed into the stable diffusion model to return the relevant images\n",
    "\n",
    "However, more complex methods of interacting with the user input can generate more accurate results in RAG architectures. One of the popular mechanisms which can increase accuracy of these retrieval systems is utilizing more than one call to an LLM in order to reformat the user input for more effective search to your vector database. A better workflow is described below compared to the one we already built...\n",
    "\n",
    "1. User input is received.\n",
    "2. An LLM is used to reword the user input to be a better search query for the vector database based on the chat history and product description. \n",
    "3. This could include things like condensing, rewording, addition of chat context, or stylistic changes.\n",
    "4. Reformatted user input is queried against the vector database to retrieve relevant products.The reformatted user input and relevant documents are inserted into a new prompt in order to generate the new style. \n",
    "5. This is then fed into the stable diffusion model to generate the images. \n",
    "\n",
    "In your application the images can come from a pre canned images \n",
    "\n",
    "We will now build out this second workflow using LangChain below. First we need to make a prompt which will reformat the user input to be more compatible for searching of the vector database. The way we do this is by providing the chat history as well as the some basic instructions to Claude and asking it to condense the input into a single output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efff0c14-5bfb-48e6-a63d-0b949f655b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unfortunately the customer input does not seem to match the provided context, which is about\n",
      "clothing and fashion recommendations. The input is asking about purchasing industrial fans for\n",
      "warehouses. I do not have enough relevant context to generate clothing style recommendations based\n",
      "on this input. Please provide clothing/fashion related customer input to match the provided context.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Human: Use the following pieces of context to generate 5 style recommendations for the customer input at the end.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "<example>A navy suit with a light blue dress shirt, conservative tie, black oxford shoes, and a leather belt.</example>\n",
    "<example>A lehenga choli set with a crop top, flowing skirt, and dupatta scarf in lively colors and metallic accents.</example>\n",
    "\n",
    "Customer Input: {question}\n",
    "Each style recommendation must be inside the tags <style></style>.\n",
    "Do not output product physical IDs.\n",
    "Skip the preamble.\n",
    "Assistant: \"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Use RetrievalQA customizations for imprving Q&A experience\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 6}\n",
    "    ),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")\n",
    "styles_response = qa({\"query\": customer_input})['result']\n",
    "\n",
    "# Alternitively we can query using wrapper also\n",
    "# styles_response = wrapper_store_faiss.query(question= customer_input, llm=llm)\n",
    "\n",
    "print_ww(styles_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57741b-8724-4504-9a7f-3be6c9ddc2c9",
   "metadata": {},
   "source": [
    "### Prepare the received response\n",
    "\n",
    "Since we have instructed LLM to return our data is returned as XML wrapping a JSON, we run the necessary extraction steps to fetch the relevant details to generate images for each look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44e6b755-b54f-401e-8742-ce7dc1757ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare input to fetch images for each look\n",
    "styles = re.findall('<style>(.*?)</style>', styles_response)\n",
    "styles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d22783-c906-413e-a889-1c4d3dd1f0fe",
   "metadata": {},
   "source": [
    "## Generate Images for the relevant style\n",
    "\n",
    "Generate an image for each look using the `Stable Diffusion` model\n",
    "\n",
    "![Generate Look](./images/generate_look.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80e7e6ff-7e46-403b-9c38-f98de88bdeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython import display\n",
    "from base64 import b64decode\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Fetching images for each of style\n",
    "gender_map = {\n",
    "    'Womens': 'of a female ',\n",
    "    'Mens': 'of a male '\n",
    "}\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "image_strip = \"\"\n",
    "for i, style in enumerate(styles):\n",
    "    request = json.dumps({\n",
    "        \"text_prompts\": [\n",
    "            {\"text\": f\"Full body view {gender_map.get(attributes.get('gender'))}without a face in \" + style + \"dslr, ultra quality, dof, film grain, Fujifilm XT3, crystal clear, 8K UHD\", \"weight\": 1.0},\n",
    "            {\"text\": \"poorly rendered\", \"weight\": -1.0}\n",
    "        ],\n",
    "        \"cfg_scale\": 9,\n",
    "        \"seed\": 4000,\n",
    "        \"steps\": 50,\n",
    "        \"style_preset\": \"photographic\",\n",
    "    })\n",
    "    modelId = \"stability.stable-diffusion-xl\"\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(body=request, modelId=modelId)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    \n",
    "    base_64_img_str = response_body[\"artifacts\"][0].get(\"base64\")\n",
    "    # display.display(display.Image(b64decode(base_64_img_str), width=200))\n",
    "    image_strip += \"<td><img src='data:image/png;base64, \"+ base_64_img_str + \"'></td>\"\n",
    "\n",
    "display.display(display.HTML(\"<table><tr>\" + image_strip +\"</tr></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0443f-487f-4c49-87c2-ab581be8535f",
   "metadata": {},
   "source": [
    "## Enhance user experience with Chatbot\n",
    "\n",
    "#### Generating detailed overview based on customer reviews of products in catalog \n",
    "We have discussed the key building blocks needed for the chatbot application and now we will start to create them. LangChain's [ConversationBufferMemory](https://python.langchain.com/docs/use_cases/question_answering/chat_vector_db) class provides an easy way to capture conversational memory for LLM chat applications. We will have Claude being able to retrieve context through conversational memory using the prompt template. Note that this time our prompt template includes a {chat_history} variable where our chat history will be included to the prompt.\n",
    "\n",
    "The prompt template has both conversation memory as well as chat history as inputs along with the human input. Notice how the prompt also instructs Claude to not answer questions which it does not have the context for. This helps reduce hallucinations which is extremely important when creating end user facing applications which need to be factual.\n",
    "\n",
    "\n",
    "![Architecture](./images/chatbot_products.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "255e05cd-da0e-4649-95e9-59e2cc9358dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt1 = \"Show me specific reviews that talk about the quality of the fabric for the jacket.\"\n",
    "chat_prompt2 = \"What do people like about the business formal jacket?\"\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "chat_history = [\" \"]\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    retriever=vectorstore_faiss.as_retriever(), \n",
    "    memory=memory_chain,\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    #verbose=True, \n",
    "    chain_type='stuff', # 'refine',\n",
    "    #max_tokens_limit=300\n",
    ")\n",
    "\n",
    "# Generate detailed reviews based on customer reviews of specific clothing in product catalog\n",
    "\n",
    "try:\n",
    "    chat_res1 = conversation.run({'question': chat_prompt1, 'chat_history': chat_history })\n",
    "    print_ww(chat_res1)\n",
    "    chat_history.append([chat_prompt1, chat_res1])\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28cf8288-acbf-464e-b202-174755e7568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    chat_res2 = conversation.run({'question': chat_prompt2 + \" Answer even if embeddings does not return anything.\", 'chat_history': chat_history })\n",
    "    print_ww(chat_res2)\n",
    "    chat_history.append([chat_prompt2, chat_res2])\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c0a1cb-72aa-4d26-bc17-d5846a1e4188",
   "metadata": {},
   "source": [
    "#### Customer order history semantic searches\n",
    "\n",
    "Generating size and color recommendations based on customer's order history. This will help to provide curated content to the customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45f4710f-c185-4356-a850-f2c051cf1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt3 = \"What size and color should I wear?\"\n",
    "chat_res3 = wrapper_store_faiss.query(question= chat_prompt3 + \" based on order history for customer with id \" + customer_id, llm=llm)\n",
    "print_ww(chat_res3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b358e8-fa6b-4cbf-b05b-ea985dcfa260",
   "metadata": {},
   "source": [
    "## Showing final products based on customer style selection \n",
    "\n",
    "Continuing on our architectural pattern we will change the prompt template and leverage the LLM to generate the `recommended` products based on the user selection and weather and other details. The key extraction entities will be \n",
    "\n",
    "1. Leverage the customer initial prompt to generate the relevant ids\n",
    "2. Extract the relevant products from the vector store\n",
    "3. Physical ID for the products needed\n",
    "\n",
    "\n",
    "\n",
    "![Architecture](./images/other_products.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd18a21-5519-4543-8d8f-98ddab109b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "prompt_template2 = \"\"\"Human: Extract list of products and their respective physical IDs from catalog that matches the style given below. \n",
    "The catalog of products is provided under <catalog></catalog> tags below.\n",
    "<catalog>\n",
    "{context}\n",
    "</catalog>\n",
    "Style: {question}\n",
    "\n",
    "The output should be a JSON of the form <products>[{{\"product\": <description of the product from the catalog>, \"physical_id\":<physical id of the product from the catalog>}}, ...]</products>\n",
    "Skip the preamble.\n",
    "Assistant: \"\"\"\n",
    "\n",
    "PROMPT2 = PromptTemplate(\n",
    "    template=prompt_template2, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "qa2 = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 10}\n",
    "    ),\n",
    "    chain_type_kwargs={\"prompt\": PROMPT2},\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "selected_style = styles[3]\n",
    "print(selected_style)\n",
    "cart_items = qa2({\"query\": selected_style })['result']\n",
    "print_ww(cart_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2343e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = json.loads(re.findall('<products>(.*?)</products>', cart_items, re.DOTALL)[0])\n",
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "793cd743-d2d6-4084-8113-68aa2fba8759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython import display\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "cart_item_strip = \"\"\n",
    "for product in products:\n",
    "    url = \"https://sagemaker-example-files-prod-us-east-1.s3.us-east-1.amazonaws.com/datasets/image/howser-bedrock/data/aistylist/images/products/\" + urllib.parse.quote(product['physical_id'].strip(), safe='', encoding=None, errors=None) + \".jpg\"\n",
    "    # im = Image.open(requests.get(url, stream=True).raw)\n",
    "    cart_item_strip += \"<td><img src='\"+ url + \"'></td>\"\n",
    "display.display(display.HTML(\"<table><tr>\" + cart_item_strip +\"</tr></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a073284-c491-4265-82a5-c48db9ea7a93",
   "metadata": {},
   "source": [
    "## Integrating DIY Agents to associate external APIs and databases\n",
    "### Using ReAct: Synergizing Reasoning and Acting in Language Models Framework\n",
    "Large language models can generate both explanations for their reasoning and task-specific responses in an alternating fashion. \n",
    "\n",
    "Producing reasoning explanations enables the model to infer, monitor, and revise action plans, and even handle unexpected scenarios. The action step allows the model to interface with and obtain information from external sources such as knowledge bases or environments.\n",
    "\n",
    "The ReAct framework could enable large language models to interact with external tools to obtain additional information that results in more accurate and fact-based responses. Here we will leverage the user prompt and perform the following actions\n",
    "1. Extract the city \n",
    "2. Get weather information\n",
    "3. Search our product catalog using semantic search to find relevant products\n",
    "4. Display the products for user to add to cart\n",
    "\n",
    "![Architecture](./images/weather.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f33d6499-8a26-446f-9c6d-b518dc7edcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import python_weather\n",
    "\n",
    "async def getweather(city):\n",
    "  # declare the client. the measuring unit used defaults to the metric system (celcius, km/h, etc.)\n",
    "  async with python_weather.Client(unit=python_weather.IMPERIAL) as client:\n",
    "    # fetch a weather forecast from a city\n",
    "    weather = await client.get(city)\n",
    "    \n",
    "    # returns the current day's forecast temperature (int)\n",
    "    return weather.current"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122aacf9-84e2-4aba-968a-6a07f6f98966",
   "metadata": {},
   "source": [
    "## Accessory recommendations \n",
    "\n",
    "We will provide  accessory recommendations based on location provided in customer input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "004c8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "await getweather(entity_extraction_result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "204770ed-f303-4ee0-a258-21f87596e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "accessory_response = None\n",
    "if attributes[\"location\"]:\n",
    "    current_weather = await getweather(entity_extraction_result[2])\n",
    "    accessory_input = \"Suggest list of accessories based on the weather and the selected style. It is \" + current_weather.description + \" with temperature at \" + str(current_weather.temperature) +\" degrees fahrenheit.\\n Selected Style: \" + styles[0]\n",
    "    accessory_response = qa({\"query\": accessory_input})['result']\n",
    "    print_ww(accessory_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad2eae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if accessory_response:\n",
    "    accessories = re.findall('<style>(.*?)</style>', accessory_response)\n",
    "    accessories_items = qa2({\"query\": ', '.join(accessories)})['result']\n",
    "    accessories_items = json.loads(re.findall('<products>(.*?)</products>', accessories_items, re.DOTALL)[0])\n",
    "    accessory_strip = \"\"\n",
    "    for accessory in accessories_items:\n",
    "        url = \"https://sagemaker-example-files-prod-us-east-1.s3.us-east-1.amazonaws.com/datasets/image/howser-bedrock/data/aistylist/images/products/\" + urllib.parse.quote(accessory['physical_id'].strip(), safe='', encoding=None, errors=None) + \".jpg\"\n",
    "        accessory_strip += \"<td><img src='\"+ url + \"'></td>\"\n",
    "    display.display(display.HTML(\"<table><tr>\" + accessory_strip +\"</tr></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e689c0-8862-41a1-8867-e2bb10c2f64f",
   "metadata": {},
   "source": [
    "## Simulate the order check out\n",
    "\n",
    "Add a customer data table to complete the order transaction. This information provides the shipping address for the outfit order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e79f2e20-0c64-44fa-ab3b-8d2401a00c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_table=[{\"id\": 1, \"first_name\": \"John\", \"last_name\": \"Doe\", \"age\": 35, \"address\": \"123 Bedrock st, California 90210\"},\n",
    "  {\"id\": 2, \"first_name\": \"Jane\", \"last_name\": \"Smith\", \"age\": 27, \"address\": \"234 Sagemaker drive, Texas 12345\"},\n",
    "  {\"id\": 3, \"first_name\": \"Bob\", \"last_name\": \"Jones\", \"age\": 42, \"address\": \"111 DeepRacer ct, Virginia 55555\"},\n",
    "  {\"id\": 4, \"first_name\": \"Sara\", \"last_name\": \"Miller\", \"age\": 29, \"address\": \"222 Robomaker ave, New Yotk 13579\"},\n",
    "  {\"id\": 5, \"first_name\": \"Mark\", \"last_name\": \"Davis\", \"age\": 31, \"address\": \"444 Transcribe blvd, Florida 02468\"},\n",
    "  {\"id\": 6, \"first_name\": \"Laura\", \"last_name\": \"Wilson\", \"age\": 24, \"address\": \"555 CodeGuru st, California 98765\" },\n",
    "  {\"id\": 7, \"first_name\": \"Steve\", \"last_name\": \"Moore\", \"age\": 36, \"address\": \"456 DeepLens st, Texas 11223\"},\n",
    "  {\"id\": 8, \"first_name\": \"Michelle\", \"last_name\": \"Chen\", \"age\": 22, \"address\": \"642 DeepCompose st, Colorado 33215\"},\n",
    "  {\"id\": 9, \"first_name\": \"David\", \"last_name\": \"Lee\", \"age\": 29, \"address\": \"777 S3 st, California 99567\"},\n",
    "  {\"id\": 10, \"first_name\": \"Jessica\", \"last_name\": \"Brown\", \"age\": 18, \"address\": \"909 Ec st, Utah 43210\"}]\n",
    "\n",
    "def address_lookup(id):\n",
    "    for customer in customer_table:\n",
    "        if customer[\"id\"] == int(id):\n",
    "            return customer\n",
    "        \n",
    "    return None\n",
    "\n",
    "print(address_lookup(customer_id)[\"address\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22dcfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
