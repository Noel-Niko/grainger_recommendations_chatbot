{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:21:41.235283Z",
     "start_time": "2024-06-30T15:21:32.097528Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -r requirements.txt",
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas<2.0.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.5.3)\r\n",
      "Requirement already satisfied: openai>=1.0.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.35.3)\r\n",
      "Requirement already satisfied: streamlit~=1.36.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.36.0)\r\n",
      "Requirement already satisfied: scipy~=1.13.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.13.1)\r\n",
      "Requirement already satisfied: langchain~=0.2.5 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.2.5)\r\n",
      "Requirement already satisfied: selenium~=4.22.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.22.0)\r\n",
      "Requirement already satisfied: webdriver-manager in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (4.0.1)\r\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.32.3)\r\n",
      "Requirement already satisfied: beautifulsoup4~=4.12.3 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (4.12.3)\r\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (16.1.0)\r\n",
      "Requirement already satisfied: boto3 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (1.34.136)\r\n",
      "Requirement already satisfied: botocore in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (1.34.136)\r\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (0.2.6)\r\n",
      "Requirement already satisfied: sagemaker in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (2.75.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (3.9.5)\r\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (9.5.0)\r\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (2.2.2)\r\n",
      "Requirement already satisfied: tensorflow in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (2.16.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from pandas<2.0.0->-r requirements.txt (line 1)) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from pandas<2.0.0->-r requirements.txt (line 1)) (2024.1)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from pandas<2.0.0->-r requirements.txt (line 1)) (1.26.4)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from openai>=1.0.0->-r requirements.txt (line 2)) (3.7.1)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from openai>=1.0.0->-r requirements.txt (line 2)) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from openai>=1.0.0->-r requirements.txt (line 2)) (0.27.0)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from openai>=1.0.0->-r requirements.txt (line 2)) (2.7.4)\r\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from openai>=1.0.0->-r requirements.txt (line 2)) (1.3.1)\r\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from openai>=1.0.0->-r requirements.txt (line 2)) (4.66.4)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from openai>=1.0.0->-r requirements.txt (line 2)) (4.12.2)\r\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from streamlit~=1.36.0->-r requirements.txt (line 3)) (5.3.0)\r\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from streamlit~=1.36.0->-r requirements.txt (line 3)) (1.8.2)\r\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from streamlit~=1.36.0->-r requirements.txt (line 3)) (5.3.3)\r\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from streamlit~=1.36.0->-r requirements.txt (line 3)) (8.1.7)\r\n",
      "Requirement already satisfied: packaging<25,>=20 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from streamlit~=1.36.0->-r requirements.txt (line 3)) (24.1)\r\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from streamlit~=1.36.0->-r requirements.txt (line 3)) (4.25.3)\r\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from streamlit~=1.36.0->-r requirements.txt (line 3)) (13.7.1)\r\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from streamlit~=1.36.0->-r requirements.txt (line 3)) (8.4.2)\r\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from streamlit~=1.36.0->-r requirements.txt (line 3)) (0.10.2)\r\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from streamlit~=1.36.0->-r requirements.txt (line 3)) (3.1.43)\r\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from streamlit~=1.36.0->-r requirements.txt (line 3)) (0.8.0b4)\r\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from streamlit~=1.36.0->-r requirements.txt (line 3)) (6.4.1)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from langchain~=0.2.5->-r requirements.txt (line 5)) (6.0.1)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from langchain~=0.2.5->-r requirements.txt (line 5)) (2.0.31)\r\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from langchain~=0.2.5->-r requirements.txt (line 5)) (4.0.3)\r\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from langchain~=0.2.5->-r requirements.txt (line 5)) (0.2.10)\r\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from langchain~=0.2.5->-r requirements.txt (line 5)) (0.2.1)\r\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from langchain~=0.2.5->-r requirements.txt (line 5)) (0.1.82)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium~=4.22.0->-r requirements.txt (line 6)) (2.2.2)\r\n",
      "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from selenium~=4.22.0->-r requirements.txt (line 6)) (0.25.1)\r\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from selenium~=4.22.0->-r requirements.txt (line 6)) (0.11.1)\r\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from selenium~=4.22.0->-r requirements.txt (line 6)) (2024.6.2)\r\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from webdriver-manager->-r requirements.txt (line 7)) (1.0.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from requests->-r requirements.txt (line 8)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from requests->-r requirements.txt (line 8)) (3.7)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from beautifulsoup4~=4.12.3->-r requirements.txt (line 9)) (2.5)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 11)) (1.0.1)\r\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 11)) (0.10.2)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from langchain-community->-r requirements.txt (line 13)) (0.6.7)\r\n",
      "Collecting langchain~=0.2.5 (from -r requirements.txt (line 5))\r\n",
      "  Using cached langchain-0.2.6-py3-none-any.whl.metadata (7.0 kB)\r\n",
      "Requirement already satisfied: attrs in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 14)) (23.2.0)\r\n",
      "Requirement already satisfied: google-pasta in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 14)) (0.2.0)\r\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 14)) (0.1.5)\r\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 14)) (1.0.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 14)) (7.2.0)\r\n",
      "Requirement already satisfied: pathos in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 14)) (0.3.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from aiohttp->-r requirements.txt (line 15)) (1.3.1)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from aiohttp->-r requirements.txt (line 15)) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from aiohttp->-r requirements.txt (line 15)) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from aiohttp->-r requirements.txt (line 15)) (1.9.4)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from torch->-r requirements.txt (line 17)) (3.15.4)\r\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from torch->-r requirements.txt (line 17)) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from torch->-r requirements.txt (line 17)) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from torch->-r requirements.txt (line 17)) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from torch->-r requirements.txt (line 17)) (2024.5.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (2.1.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (24.3.25)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (0.6.0)\r\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (3.11.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (18.1.1)\r\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (0.3.2)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (3.3.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (70.1.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (2.4.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (1.16.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (1.64.1)\r\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (2.16.2)\r\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (3.4.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 18)) (0.37.0)\r\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit~=1.36.0->-r requirements.txt (line 3)) (4.22.0)\r\n",
      "Requirement already satisfied: toolz in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit~=1.36.0->-r requirements.txt (line 3)) (0.12.1)\r\n",
      "Requirement already satisfied: exceptiongroup in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.0.0->-r requirements.txt (line 2)) (1.2.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 18)) (0.43.0)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 13)) (3.21.3)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 13)) (0.9.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit~=1.36.0->-r requirements.txt (line 3)) (4.0.11)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.0.0->-r requirements.txt (line 2)) (1.0.5)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->-r requirements.txt (line 2)) (0.14.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from importlib-metadata>=1.4.0->sagemaker->-r requirements.txt (line 14)) (3.19.2)\r\n",
      "Requirement already satisfied: namex in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow->-r requirements.txt (line 18)) (0.0.8)\r\n",
      "Requirement already satisfied: optree in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow->-r requirements.txt (line 18)) (0.11.0)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.7->langchain~=0.2.5->-r requirements.txt (line 5)) (1.33)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain~=0.2.5->-r requirements.txt (line 5)) (3.10.4)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->-r requirements.txt (line 2)) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->-r requirements.txt (line 2)) (2.18.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 17)) (2.1.5)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit~=1.36.0->-r requirements.txt (line 3)) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit~=1.36.0->-r requirements.txt (line 3)) (2.18.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain~=0.2.5->-r requirements.txt (line 5)) (3.0.3)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow->-r requirements.txt (line 18)) (3.6)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow->-r requirements.txt (line 18)) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow->-r requirements.txt (line 18)) (3.0.3)\r\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from trio~=0.17->selenium~=4.22.0->-r requirements.txt (line 6)) (2.4.0)\r\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from trio~=0.17->selenium~=4.22.0->-r requirements.txt (line 6)) (1.3.0.post0)\r\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium~=4.22.0->-r requirements.txt (line 6)) (1.2.0)\r\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium~=4.22.0->-r requirements.txt (line 6)) (1.7.1)\r\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 14)) (1.7.6.8)\r\n",
      "Requirement already satisfied: dill>=0.3.8 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 14)) (0.3.8)\r\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 14)) (0.3.4)\r\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 14)) (0.70.16)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 17)) (1.3.0)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit~=1.36.0->-r requirements.txt (line 3)) (5.0.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain~=0.2.5->-r requirements.txt (line 5)) (3.0.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.36.0->-r requirements.txt (line 3)) (2023.12.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.36.0->-r requirements.txt (line 3)) (0.35.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.36.0->-r requirements.txt (line 3)) (0.18.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit~=1.36.0->-r requirements.txt (line 3)) (0.1.2)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 13)) (1.0.0)\r\n",
      "Using cached langchain-0.2.6-py3-none-any.whl (975 kB)\r\n",
      "Installing collected packages: langchain\r\n",
      "  Attempting uninstall: langchain\r\n",
      "    Found existing installation: langchain 0.2.5\r\n",
      "    Uninstalling langchain-0.2.5:\r\n",
      "      Successfully uninstalled langchain-0.2.5\r\n",
      "Successfully installed langchain-0.2.6\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:21:41.243868Z",
     "start_time": "2024-06-30T15:21:41.238580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "ba5dc8375cee7baa",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:21:42.421635Z",
     "start_time": "2024-06-30T15:21:41.246082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from modules.utils import bedrock\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=False)\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n",
    "\n",
    "model_parameter = {\n",
    "    \"temperature\": 0.0, \n",
    "    \"top_p\": .5, \n",
    "    \"top_k\": 250, \n",
    "    \"max_tokens_to_sample\": 2000, \n",
    "    \"stop_sequences\": [\"\\n\\n Human: bye\"]\n",
    "}\n",
    "llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    model_kwargs=model_parameter, \n",
    "    client=bedrock_runtime\n",
    ")"
   ],
   "id": "917e3339e7e26528",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n",
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Bedrock` was deprecated in LangChain 0.0.34 and will be removed in 0.3. An updated version of the class exists in the langchain-aws package and should be used instead. To use it run `pip install -U langchain-aws` and import as `from langchain_aws import BedrockLLM`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:21:43.460323Z",
     "start_time": "2024-06-30T15:21:42.424484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create Vector Index\n",
    "import pandas as pd\n",
    "parquet_file_path = \"processed/grainger_products.parquet\"\n",
    "print(\"Attempting to load file from:\", parquet_file_path)\n",
    "\n",
    "# Now attempt to load the file\n",
    "try:\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "    print(\"File loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Error loading file:\", e)\n",
    "\n",
    "print(df.head())\n"
   ],
   "id": "8eb2e0061c74496e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load file from: processed/grainger_products.parquet\n",
      "File loaded successfully!\n",
      "                          Brand    Code  \\\n",
      "0  LION FIRE BOOTS BY THOROGOOD   3XRG7   \n",
      "1           GLOWEAR BY ERGODYNE   1CXK5   \n",
      "2                      CARHARTT  491V68   \n",
      "3                      TRIPLETT  794UC5   \n",
      "4                        DEWALT  492U19   \n",
      "\n",
      "                                                Name  \\\n",
      "0  Insulated Firefighter Boots: Insulated, Steel,...   \n",
      "1  GLOWEAR BY ERGODYNE Baseball Cap: Orange, Univ...   \n",
      "2  CARHARTT Bib Overalls: Men's, XL ( 42 in x 32 ...   \n",
      "3  TRIPLETT Combustible Gas Detector: Audible/Vib...   \n",
      "4  DEWALT Heated Jacket: Men's, S, Black, Up to 9...   \n",
      "\n",
      "                                       PictureUrl600    Price  \\\n",
      "0  https://static.grainger.com/rp/s/is/image/Grai...  $197.55   \n",
      "1  https://static.grainger.com/rp/s/is/image/Grai...   $13.93   \n",
      "2  https://static.grainger.com/rp/s/is/image/Grai...   $95.79   \n",
      "3  https://static.grainger.com/rp/s/is/image/Grai...  $134.56   \n",
      "4  https://static.grainger.com/rp/s/is/image/Grai...  $363.87   \n",
      "\n",
      "                                         Description  \n",
      "0  <p>Thorogood® 807-6003 Insulated Boots are des...  \n",
      "1  <p>Baseball hats have a curved brim that shade...  \n",
      "2  <p>Overalls (sometimes called bib overalls) ar...  \n",
      "3                                               None  \n",
      "4  <p>Men's electronically heated jackets cover t...  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:24:50.424378Z",
     "start_time": "2024-06-30T15:21:43.461848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# AS A DOCUMENT\n",
    "# Automates the process and optimizes for large and changing data sets.\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "        \n",
    "# Initialize the Titan Embeddings Model\n",
    "print(\"Initializing Titan Embeddings Model...\")\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_runtime)\n",
    "print(\"Titan Embeddings Model initialized.\")\n",
    "\n",
    "documents = []\n",
    "for _, row in df.iterrows():\n",
    "    page_content = f\"{row['Code']} {row['Name']} {row['Brand']} {row['Description'] if pd.notna(row['Description']) else ''}\"\n",
    "    metadata = {\n",
    "        'Brand': row['Brand'],\n",
    "        'Code': row['Code'],\n",
    "        'Name': row['Name'],\n",
    "        'Description': row['Description'],\n",
    "        'Price': row['Price']\n",
    "    }\n",
    "    documents.append(Document(page_content, metadata))\n",
    "\n",
    "\n",
    "# Print the structured documents\n",
    "print(\"Structured documents created:\")\n",
    "for idx, doc in enumerate(documents[:5], 1):  \n",
    "    print(f\"Document {idx} of {len(documents)}:\")\n",
    "    print(doc.page_content[:200])\n",
    "    print()\n",
    "\n",
    "# Create FAISS vector store from structured documents\n",
    "print(\"Creating FAISS vector store from structured documents...\")\n",
    "vectorstore_faiss_doc = FAISS.from_documents(documents, bedrock_embeddings)\n",
    "print(\"FAISS vector store created.\")\n",
    "\n"
   ],
   "id": "8ee19c6318ea86ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Titan Embeddings Model...\n",
      "Titan Embeddings Model initialized.\n",
      "Structured documents created:\n",
      "Document 1 of 1736:\n",
      "3XRG7 Insulated Firefighter Boots: Insulated, Steel, 10-1/2, M, Structural, 1 PR LION FIRE BOOTS BY THOROGOOD <p>Thorogood® 807-6003 Insulated Boots are designed for use by firefighters in demanding a\n",
      "\n",
      "Document 2 of 1736:\n",
      "1CXK5 GLOWEAR BY ERGODYNE Baseball Cap: Orange, Universal, Baseball Hat Hat, Polyester, Gen Purpose GLOWEAR BY ERGODYNE <p>Baseball hats have a curved brim that shades the eyes from sun to reduce eye \n",
      "\n",
      "Document 3 of 1736:\n",
      "491V68 CARHARTT Bib Overalls: Men's, XL ( 42 in x 32 in ), Navy, Cotton, Zipper, Zipper, 9 Pockets CARHARTT <p>Overalls (sometimes called bib overalls) are sleeveless garments that cover the torso and\n",
      "\n",
      "Document 4 of 1736:\n",
      "794UC5 TRIPLETT Combustible Gas Detector: Audible/Vibration/Visual Indicator, Rechargeable Li-Po Battery TRIPLETT \n",
      "\n",
      "Document 5 of 1736:\n",
      "492U19 DEWALT Heated Jacket: Men's, S, Black, Up to 9 hr, 36 in Max Chest Size, 3 Outside Pockets, Zipper DEWALT <p>Men's electronically heated jackets cover the arms and torso, extending to the hip.<\n",
      "\n",
      "Creating FAISS vector store from structured documents...\n",
      "FAISS vector store created.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:24:50.430321Z",
     "start_time": "2024-06-30T15:24:50.426590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ENTER INITIAL INPUT HERE\n",
    "\n",
    "customer_input = \"I am looking for waterproof insulated boots for my men working on my commercial deep sea fishing boat in the arctic. Must have large sizes\"\n"
   ],
   "id": "953bd0b82f1b2bc3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:24:50.584141Z",
     "start_time": "2024-06-30T15:24:50.432798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "query_embedding_doc = bedrock_embeddings.embed_query(customer_input)\n",
    "print(\"Customer input processed.\")\n",
    "\n",
    "# Convert query embedding to numpy array\n",
    "np_array_query_embedding_doc = np.array(query_embedding_doc)\n",
    "print(\"Query embedding converted to numpy array.\")\n",
    "\n",
    "# Print the resulting query embedding\n",
    "print(\"Resulting query embedding:\")\n",
    "print(np_array_query_embedding_doc)\n"
   ],
   "id": "2d777e20a73a3ce2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer input processed.\n",
      "Query embedding converted to numpy array.\n",
      "Resulting query embedding:\n",
      "[-0.9609375  -0.13574219 -0.06787109 ...  0.16796875 -0.65234375\n",
      " -0.62890625]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:24:50.591386Z",
     "start_time": "2024-06-30T15:24:50.586073Z"
    }
   },
   "cell_type": "code",
   "source": "customer_input = \"I am looking for hats to protect my men from the sun while working out in road construction in Arizona heat. I have a large company and need a solution that I can buy in bulk.\"",
   "id": "751fd5041072c2fa",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:24:50.601226Z",
     "start_time": "2024-06-30T15:24:50.593295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_product_attributes(customer_input):\n",
    "    # Define the NER prompt with placeholders for the customer input\n",
    "    ner_prompt = \"\"\"Human: Find industry, size, Sustainability Focus, Inventory Manager, and the location in the customer input.\n",
    "    Instructions:\n",
    "    The industry can be one of the following: Manufacturing, Warehousing, Government and Public Safety, Education, Food and Beverage Distribution, Hospitality, Property Management, Retail, or Other\n",
    "    The size can be one of the following: Small Businesses (Smaller companies might prioritize cost-effective solutions and fast shipping options), or Large Enterprises (Larger organizations may require more comprehensive solutions, including strategic services like inventory management and safety consulting), Womens, Other\n",
    "    The Sustainability Focused true or false meaning Environmentally Conscious Buyers: Customers interested in sustainability solutions, looking for products that focus on energy management, water conservation, waste reduction, and air quality improvement, or NOT Environmentally Conscious Buyers,\n",
    "    The Inventory Manager true or false meaning a purchaser in large amounts to supply an organizational group, versus an individual user purchasing for personal use,\n",
    "    The output must be in JSON format inside the tags <attributes></attributes>\n",
    "\n",
    "    If the information of an entity is not available in the input then don't include that entity in the JSON output\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    Customer input: {customer_input}\n",
    "    Assistant:\"\"\".format(customer_input=customer_input)\n",
    "\n",
    "    # Process the customer input with the NER model\n",
    "    entity_extraction_result = llm(ner_prompt).strip()\n",
    "\n",
    "    # Extract the attributes from the processed result\n",
    "    result = re.search('<attributes>(.*?)</attributes>', entity_extraction_result, re.DOTALL)\n",
    "    if result:\n",
    "        attributes_str = result.group(1)\n",
    "        # Convert the attributes string to JSON\n",
    "        attributes = json.loads(attributes_str)\n",
    "        return attributes\n",
    "    else:\n",
    "        return {}\n"
   ],
   "id": "d000a7d2a0039369",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:24:50.914406Z",
     "start_time": "2024-06-30T15:24:50.607487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## GET LIST OF PRODUCTS AND CODES\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template2 = \"\"\"Human: Extract list of 5 products and their respective physical IDs from catalog that answer the user question. \n",
    "The catalog of products is provided under <catalog></catalog> tags below.\n",
    "<catalog>\n",
    "{context}\n",
    "</catalog>\n",
    "Question: {question}\n",
    "\n",
    "The output should be a json of the form <products>[{{\"product\": <description of the product from the catalog>, \"code\":<code of the product from the catalog>}}, ...]</products>\n",
    "Skip the preamble and always return valid json.\n",
    "Assistant: \"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template2, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Use RetrievalQA customizations for improving Q&A experience\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss_doc.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 6}\n",
    "    ),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")\n"
   ],
   "id": "7e64cd972402e335",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:24:50.921005Z",
     "start_time": "2024-06-30T15:24:50.916164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Call for reviews:\n",
    "# TODO\n",
    "reviews_dict = None"
   ],
   "id": "73fcf382c57fe0a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:24:50.930218Z",
     "start_time": "2024-06-30T15:24:50.922742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def process_response_to_json(recs_response):\n",
    "    # Ensure recs_response is handled correctly\n",
    "    recs_response = recs_response.strip()  # Remove leading/trailing whitespace\n",
    "    response_json = \"\"\n",
    "\n",
    "    # Check if the response starts and ends with expected JSON markers\n",
    "    if recs_response.startswith(\"<products>\") and recs_response.endswith(\"</products>\"):\n",
    "        json_content = recs_response[len(\"<products>\") : -len(\"</products>\")].strip()\n",
    "\n",
    "        try:\n",
    "            parsed_response = json.loads(json_content)\n",
    "\n",
    "            if isinstance(parsed_response, list):\n",
    "                products_list = []\n",
    "                for product_info in parsed_response:\n",
    "                    # Assuming product_info is a dictionary with 'product' and 'code' keys\n",
    "                    product_data = {\n",
    "                        \"product\": product_info.get(\"product\", \"\"),\n",
    "                        \"code\": product_info.get(\"code\", \"\")\n",
    "                    }\n",
    "                    products_list.append(product_data)\n",
    "\n",
    "                response_json = {\"products\": products_list}\n",
    "                return response_json\n",
    "            else:\n",
    "                print(\"Error: Unexpected format of parsed response\")\n",
    "                return None\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {str(e)}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Error: Unexpected format of recs_response\")\n",
    "        return None"
   ],
   "id": "3fde7a81320657a3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:24:50.936375Z",
     "start_time": "2024-06-30T15:24:50.932323Z"
    }
   },
   "cell_type": "code",
   "source": "chat_history = [\" \"]",
   "id": "6e554a761b3dcfd6",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T16:19:30.995085Z",
     "start_time": "2024-06-30T16:19:30.987985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import conversation\n",
    "\n",
    "\n",
    "def process_chat_question(question, clear_history=False):\n",
    "    try:\n",
    "        if clear_history:\n",
    "            chat_history.clear()  # Clear chat history if specified\n",
    "\n",
    "        # Extract product attributes from the question\n",
    "        customer_attributes = extract_product_attributes(question)\n",
    "\n",
    "        # Format the customer input with the extracted attributes\n",
    "        customer_input = \"{} {}\".format(question, str(customer_attributes))\n",
    "\n",
    "        # Retrieve data based on the formatted customer input\n",
    "        retrieved_data = qa({\"query\": customer_input})['result']\n",
    "\n",
    "        # Append the retrieved data to the chat history\n",
    "        chat_history.append(retrieved_data)\n",
    "\n",
    "        # Prepare the context with the formatted customer input and chat history\n",
    "        context = {\n",
    "            'question': customer_input,\n",
    "            'chat_history': chat_history\n",
    "        }\n",
    "\n",
    "        # # Run conversation with provided context synchronously\n",
    "        # chat_res = conversation.run(**context)\n",
    "\n",
    "        # # Append the chat prompt and result to history\n",
    "        # chat_history.append([question, chat_res])\n",
    "\n",
    "        # Assuming response_json and reviews_dict are defined elsewhere based on the conversation output\n",
    "        # # Optionally add response_json['products'] and reviews_dict to chat history\n",
    "        # if response_json is not None:\n",
    "        #     chat_history.append(response_json['products'])\n",
    "        # \n",
    "        # if reviews_dict is not None:\n",
    "        #     chat_history.append(reviews_dict)\n",
    "\n",
    "        return str(retrieved_data)  # Return chat response as a string\n",
    "\n",
    "    except ValueError as error:\n",
    "        if \"AccessDeniedException\" in str(error):\n",
    "            class StopExecution(ValueError):\n",
    "                def _render_traceback_(self):\n",
    "                    pass\n",
    "            raise StopExecution\n",
    "        else:\n",
    "            raise error"
   ],
   "id": "594a5758c2c52428",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:24:50.954422Z",
     "start_time": "2024-06-30T15:24:50.950108Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9264490c78f6eeb1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:24:50.960983Z",
     "start_time": "2024-06-30T15:24:50.956605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # HERE IS THE CONVERSATION\n",
    "# from langchain.chains import ConversationalRetrievalChain\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "# from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "# \n",
    "# chat_history = [\" \"]\n",
    "# memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "# conversation = ConversationalRetrievalChain.from_llm(\n",
    "#     llm=llm, \n",
    "#     retriever=vectorstore_faiss_doc.as_retriever(), \n",
    "#     memory=memory_chain,\n",
    "#     condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "#     chain_type='stuff',  # 'refine',\n",
    "# )\n",
    "# \n",
    "# # Define a function to process the chat question\n",
    "# def process_chat_question(question, clear_history=False):\n",
    "#     try:\n",
    "#         if clear_history:\n",
    "#             chat_history.clear()  # Clear chat history if specified\n",
    "# \n",
    "#         context = {\n",
    "#             'question': question,\n",
    "#             'chat_history': chat_history\n",
    "#         }\n",
    "# \n",
    "#         # Run conversation with provided context synchronously\n",
    "#         chat_res = conversation.run(**context)\n",
    "# \n",
    "#         # Append the chat prompt and result to history\n",
    "#         chat_history.append([question, chat_res])\n",
    "# \n",
    "#         # Optionally add response_json['products'] and reviews_dict to chat history\n",
    "#         if response_json:\n",
    "#             chat_history.append(response_json['products'])\n",
    "# \n",
    "#         if reviews_dict:\n",
    "#             chat_history.append(reviews_dict)\n",
    "# \n",
    "#         return str(chat_res)  # Return chat response as a string\n",
    "# \n",
    "#     except ValueError as error:\n",
    "#         if \"AccessDeniedException\" in str(error):\n",
    "#             class StopExecution(ValueError):\n",
    "#                 def _render_traceback_(self):\n",
    "#                     pass\n",
    "#             raise StopExecution\n",
    "#         else:\n",
    "#             raise error\n"
   ],
   "id": "d55fc043bbeb9822",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T16:19:51.341816Z",
     "start_time": "2024-06-30T16:19:35.305662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "customer_input = \"I am looking for waterproof insulated boots for my men working for me on a commercial fishing boat in the Arctic cold.\"\n",
    "customer_attributes = extract_product_attributes(customer_input)\n",
    "customer_input = \"{} {}\".format(customer_input, str(customer_attributes))\n",
    "retrieved_data = qa({\"query\": customer_input})['result']\n",
    "chat_history.append(retrieved_data)\n",
    "response = process_chat_question(question=customer_input, clear_history=False)\n",
    "print(response)"
   ],
   "id": "5c8c575299631d60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <products>\n",
      "[\n",
      "  {\"product\": \"Rubber Boot: Cold-Insulated/Electrical Hazard (EH)/Oil-Resistant Sole/Plain Toe/Waterproof, 6, 1 PR THE ORIGINAL MUCK BOOT CO.\", \"code\":\"21A645\"},\n",
      "  {\"product\": \"Rubber Boot: Cold-Insulated/Electrical Hazard (EH)/Oil-Resistant Sole/Plain Toe/Waterproof, 10, 1 PR THE ORIGINAL MUCK BOOT CO.\", \"code\":\"21A649\"}, \n",
      "  {\"product\": \"Rubber Boot: Cold-Insulated/Electrical Hazard (EH)/Oil-Resistant Sole/Plain Toe/Waterproof, 11, 1 PR THE ORIGINAL MUCK BOOT CO.\", \"code\":\"21A650\"},\n",
      "  {\"product\": \"Rubber Boot: Cold-Insulated/Electrical Hazard (EH)/Oil-Resistant Sole/Plain Toe/Waterproof, 13, 1 PR THE ORIGINAL MUCK BOOT CO.\", \"code\":\"21A652\"}\n",
      "]\n",
      "</products>\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example usage:\n",
    "question = (\"What boots do you have in size 14 that are water proof?\")\n",
    "response = process_chat_question(question,clear_history=False)  # Specify clear_history as needed\n",
    "print(response)"
   ],
   "id": "cfc47ecd751830af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Example usage:\n",
    "question = (\"I am looking for hats to protect my men from the sun while working out in road construction in Arizona heat. \")\n",
    "response = process_chat_question(question,  clear_history=True)  # Specify clear_history as needed\n",
    "print(response)"
   ],
   "id": "9f8ebe782830cb83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "56e1aea0cf3bd416",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
