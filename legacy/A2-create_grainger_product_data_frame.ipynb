{
 "cells": [
  {
   "cell_type": "code",
   "id": "2a11e5f910934738",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:21:25.548290Z",
     "start_time": "2024-07-04T08:21:25.540950Z"
    }
   },
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "domain = \"https://www.grainger.com\"\n",
    "local_domain = urlparse(domain).netloc\n",
    "# Create necessary directories if they don't exist\n",
    "if not os.path.exists(\"text/\"):\n",
    "    os.mkdir(\"text/\")\n",
    "if not os.path.exists(f\"text/{local_domain}/\"):\n",
    "    os.mkdir(f\"text/{local_domain}/\")\n",
    "if not os.path.exists(\"processed\"):\n",
    "    os.mkdir(\"processed\")"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a61fd1fe2b624523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:21:25.555253Z",
     "start_time": "2024-07-04T08:21:25.551002Z"
    }
   },
   "source": [
    "# TEST \n",
    "import re\n",
    "\n",
    "# Pattern to match product skus/codes\n",
    "regex_pattern = re.compile(r'[A-Z0-9]{5,7}')\n",
    "# Test strings\n",
    "test_strings = [\n",
    "    \"1DKW3_1.pdf\",\n",
    "    \"3VE59C-Operating-Instructions-and-Parts-Manual.pdf\",\n",
    "    \"_3M-Disposable-Respirator-Dual-4JF99?opr=PDPBRDSP&analytics=dsbrItems_5ZZZ6.txt\"\n",
    "]\n",
    "\n",
    "# Extract product codes from test strings\n",
    "for test in test_strings:\n",
    "    matches = regex_pattern.findall(test)\n",
    "    print(f\"Matches in '{test}': {matches}\")\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1d225d8e17be55ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:21:25.560465Z",
     "start_time": "2024-07-04T08:21:25.556595Z"
    }
   },
   "source": [
    "# # PULL product codes from web scraped data and save as json\n",
    "# import os\n",
    "# import re\n",
    "# import json\n",
    "# \n",
    "# # Define the regex pattern for product codes\n",
    "# regex_pattern = re.compile(r'[A-Z0-9]{5,7}')\n",
    "# \n",
    "# # Directory containing the files\n",
    "# directory = 'GraingerWebScrape/www.grainger.com'\n",
    "# \n",
    "# # List to store found product codes\n",
    "# product_codes = []\n",
    "# \n",
    "# # Function to extract product codes from text\n",
    "# def extract_product_codes(text):\n",
    "#     return regex_pattern.findall(text)\n",
    "# \n",
    "# # Iterate through all files in the directory\n",
    "# for root, dirs, files in os.walk(directory):\n",
    "#     for file in files:\n",
    "#         file_path = os.path.join(root, file)\n",
    "# \n",
    "#         # Check for product codes in the file name\n",
    "#         codes_in_filename = extract_product_codes(file)\n",
    "#         product_codes.extend(codes_in_filename)\n",
    "# \n",
    "#         # Check for product codes in the file content\n",
    "#         try:\n",
    "#             with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                 content = f.read()\n",
    "#                 codes_in_content = extract_product_codes(content)\n",
    "#                 product_codes.extend(codes_in_content)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Could not read file {file_path}: {e}\")\n",
    "# \n",
    "# # Remove duplicates by converting the list to a set and back to a list\n",
    "# product_codes = list(set(product_codes))\n",
    "# \n",
    "# # Save all product codes to a single JSON file\n",
    "# with open('all_product_codes.json', 'w') as f:\n",
    "#     json.dump(product_codes, f, indent=4)\n",
    "# \n",
    "# print(f\"Total product codes found: {len(product_codes)}\")\n",
    "# print(f\"Product codes saved in 'all_product_codes.json'\")\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:21:25.573202Z",
     "start_time": "2024-07-04T08:21:25.561805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # PULL product codes from web scraped data and save as json\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Define the regex pattern for product codes\n",
    "regex_pattern = re.compile(r'[A-Z0-9]{5,7}')\n",
    "\n",
    "# Directory containing the files\n",
    "directory = 'GraingerWebScrape/www.grainger.com'\n",
    "\n",
    "# Load existing product codes from the JSON file\n",
    "existing_product_codes = []\n",
    "print(\"os.path.exists('all_product_codes.json')\", os.path.exists('all_product_codes.json'))\n",
    "if os.path.exists('all_product_codes.json'):\n",
    "    with open('all_product_codes.json', 'r') as f:\n",
    "        existing_product_codes = json.load(f)\n",
    "\n",
    "# List to store found product codes\n",
    "product_codes = []\n",
    "\n",
    "# Function to extract product codes from text\n",
    "def extract_product_codes(text):\n",
    "    return regex_pattern.findall(text)\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        # Check for product codes in the file name\n",
    "        codes_in_filename = extract_product_codes(file)\n",
    "        product_codes.extend(codes_in_filename)\n",
    "\n",
    "        # Check for product codes in the file content\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                codes_in_content = extract_product_codes(content)\n",
    "                product_codes.extend(codes_in_content)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read file {file_path}: {e}\")\n",
    "\n",
    "# Remove duplicates by converting the list to a set and back to a list\n",
    "product_codes = list(set(product_codes))\n",
    "\n",
    "# Combine existing and new product codes\n",
    "product_codes = list(set(existing_product_codes + product_codes))\n",
    "\n",
    "# Save all product codes to the JSON file\n",
    "with open('all_product_codes.json', 'w') as f:\n",
    "    json.dump(product_codes, f, indent=4)\n",
    "\n",
    "print(f\"Total product codes found: {len(product_codes)}\")\n",
    "print(f\"Product codes saved in 'all_product_codes.json'\")"
   ],
   "id": "e75e472252e9edd8",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a23af246e9826876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:21:25.580725Z",
     "start_time": "2024-07-04T08:21:25.576708Z"
    }
   },
   "source": [
    "# # FETCH DATA ON PRODUCT CODES FROM URL AND SAVE AS DATA FRAME\n",
    "# # UPDATE THE JSON FILE WITH VALID PRODUCT CODES\n",
    "# import os\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "# import json\n",
    "#\n",
    "# # Base URL and headers for the API\n",
    "# base_url = \"https://mobile-rest-qa.nonprod.graingercloud.com/v1/product/detail\"\n",
    "# headers = {\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "#\n",
    "# # Function to fetch and process data\n",
    "# def fetch_product_details(skus):\n",
    "#     params = {\n",
    "#         \"partNumbers\": skus,\n",
    "#         \"extraInfo\": \"false\"\n",
    "#     }\n",
    "#     response = requests.get(base_url, headers=headers, params=params)\n",
    "#     if response.status_code == 200:\n",
    "#         try:\n",
    "#             data = response.json()\n",
    "#             results = []\n",
    "#             for item in data:\n",
    "#                 brand = item.get(\"brand\", {}).get(\"name\", \"N/A\")\n",
    "#                 code = item.get(\"code\", \"N/A\")\n",
    "#                 name = item.get(\"name\", \"N/A\")\n",
    "#                 picture_url = item.get(\"pictureUrl600\", \"N/A\")\n",
    "#                 price = item.get(\"priceData\", {}).get(\"formattedPrice\", \"N/A\")\n",
    "#                 description = item.get(\"productDetailsDescription\", \"N/A\")\n",
    "#\n",
    "#                 results.append({\n",
    "#                     \"Brand\": brand,\n",
    "#                     \"Code\": code,\n",
    "#                     \"Name\": name,\n",
    "#                     \"PictureUrl600\": picture_url,\n",
    "#                     \"Price\": price,\n",
    "#                     \"Description\": description\n",
    "#                 })\n",
    "#\n",
    "#             return pd.DataFrame(results) if results else None\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error parsing response for {skus}: {e}\")\n",
    "#             return None\n",
    "#     else:\n",
    "#         print(f\"Failed to fetch details for {skus}: Status code {response.status_code}\")\n",
    "#         return None\n",
    "#\n",
    "# # Load the product codes from the JSON file\n",
    "# with open('all_product_codes.json', 'r') as f:\n",
    "#     product_codes = json.load(f)\n",
    "#\n",
    "# print(f\"Total product codes found: {len(product_codes)}\")\n",
    "#\n",
    "# # Product codes in chunks of 100\n",
    "# chunk_size = 1\n",
    "# chunks = [product_codes[i:i + chunk_size] for i in range(0, len(product_codes), chunk_size)]\n",
    "#\n",
    "# # Iterate over each chunk for API requests\n",
    "# df = pd.DataFrame(columns=[\"Brand\", \"Code\", \"Name\", \"PictureUrl600\", \"Price\", \"Description\"])\n",
    "# failed_chunks = []\n",
    "# for chunk in chunks:\n",
    "#     try:\n",
    "#         details = fetch_product_details(chunk)\n",
    "#         if details is not None:\n",
    "#             df = pd.concat([df, details], ignore_index=True)\n",
    "#         else:\n",
    "#             print(f\"No details fetched for chunk: {chunk}. Removing from source.\")\n",
    "#             failed_chunks.extend(chunk)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to fetch details for chunk: {chunk}, Error: {e}\")\n",
    "#         failed_chunks.extend(chunk)\n",
    "#\n",
    "# # # Remove failed product codes from the source list\n",
    "# # product_codes = [code for code in product_codes if code not in failed_chunks]\n",
    "#\n",
    "# # # Save the updated product codes to the JSON file\n",
    "# # with open('all_product_codes.json', 'w') as f:\n",
    "# #     json.dump(product_codes, f, indent=4)\n",
    "#\n",
    "# # # Remove rows where all columns are NaN\n",
    "# # df = df.dropna(how='all')\n",
    "#\n",
    "# # Ensure all column names are strings\n",
    "# df.columns = df.columns.astype(str)\n",
    "#\n",
    "# # Save to Parquet\n",
    "# os.makedirs('processed', exist_ok=True)\n",
    "# df.to_parquet('processed/grainger_products.parquet', index=False)\n",
    "# print(\"Product details have been saved to 'processed/grainger_products.parquet'\")\n",
    "# print(\"\\nHead of DataFrame:\")\n",
    "# print(df.head(), \"\\n\")\n",
    "# print(\"Tail of DataFrame:\")\n",
    "# print(df.tail(), \"\\n\")\n",
    "# print(\"Size of DataFrame:\", df.size, \"\\n\")\n",
    "# print(\"Values in DataFrame:\")\n",
    "# print(df.values)\n"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f46f5676aa9f97a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:58:09.518380Z",
     "start_time": "2024-07-04T08:21:25.582565Z"
    }
   },
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import html\n",
    "\n",
    "# Base URL and headers for the API\n",
    "base_url = \"https://mobile-rest-qa.nonprod.graingercloud.com/v1/product/detail\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Function to fetch and process data\n",
    "def fetch_product_details(skus):\n",
    "    params = {\n",
    "        \"partNumbers\": skus,\n",
    "        \"extraInfo\": \"false\"\n",
    "    }\n",
    "    print(f\"Fetching details for SKUs: {skus}\")  # Debugging print statement\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            results = []\n",
    "            for item in data:\n",
    "                brand = item.get(\"brand\", {}).get(\"name\", \"N/A\")\n",
    "                code = item.get(\"code\", \"N/A\")\n",
    "                name = item.get(\"name\", \"N/A\")\n",
    "                picture_url = item.get(\"pictureUrl600\", \"N/A\")\n",
    "                price = item.get(\"priceData\", {}).get(\"formattedPrice\", \"N/A\")\n",
    "                description = item.get(\"productDetailsDescription\", \"N/A\")\n",
    "\n",
    "                results.append({\n",
    "                    \"Brand\": brand,\n",
    "                    \"Code\": code,\n",
    "                    \"Name\": name,\n",
    "                    \"PictureUrl600\": picture_url,\n",
    "                    \"Price\": price,\n",
    "                    \"Description\": description\n",
    "                })\n",
    "\n",
    "            print(f\"Successfully processed {len(results)} items.\")  # Debugging print statement\n",
    "            return pd.DataFrame(results) if results else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response for {skus}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to fetch details for {skus}: Status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Load the product codes from the JSON file\n",
    "try:\n",
    "    with open('all_product_codes.json', 'r') as f:\n",
    "        product_codes = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"JSON file 'all_product_codes.json' not found.\")\n",
    "else:\n",
    "    print(f\"Total product codes found: {len(product_codes)}\")\n",
    "\n",
    "# Product codes in chunks of 100\n",
    "chunk_size = 1\n",
    "chunks = [product_codes[i:i + chunk_size] for i in range(0, len(product_codes), chunk_size)]\n",
    "\n",
    "# Iterate over each chunk for API requests\n",
    "df = pd.DataFrame(columns=[\"Brand\", \"Code\", \"Name\", \"PictureUrl600\", \"Price\", \"Description\"])\n",
    "failed_chunks = []\n",
    "for chunk in chunks:\n",
    "    try:\n",
    "        details = fetch_product_details(chunk)\n",
    "        if details is not None:\n",
    "            df = pd.concat([df, details], ignore_index=True)\n",
    "            print(f\"Successfully appended {len(details)} rows to DataFrame.\")  # Debugging print statement\n",
    "        else:\n",
    "            print(f\"No details fetched for chunk: {chunk}. Removing from source.\")\n",
    "            failed_chunks.extend(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch details for chunk: {chunk}, Error: {e}\")\n",
    "        failed_chunks.extend(chunk)\n",
    "\n",
    "# Ensure all column names are strings\n",
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "# Complete data cleaning\n",
    "df.fillna(\"\", inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "def clean_code(code):\n",
    "    # Extract the part before any space or other characters\n",
    "    return re.split(r'\\s|[-_()]+', code, 1)[0]\n",
    "\n",
    "df['Code'] = df['Code'].apply(clean_code)\n",
    "# Remove HTML characters from all columns\n",
    "df = df.applymap(lambda x: html.unescape(x) if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "# Save to Parquet\n",
    "os.makedirs('processed', exist_ok=True)\n",
    "df.to_parquet('processed/grainger_products.parquet', index=False)\n",
    "print(\"Product details have been saved to 'processed/grainger_products.parquet'\")\n",
    "print(\"\\nHead of DataFrame:\")\n",
    "print(df.head(), \"\\n\")\n",
    "print(\"Tail of DataFrame:\")\n",
    "print(df.tail(), \"\\n\")\n",
    "print(\"Size of DataFrame:\", df.size, \"\\n\")\n",
    "print(\"Values in DataFrame:\")\n",
    "print(df.values)\n"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T11:43:35.490500Z",
     "start_time": "2024-07-04T11:43:35.461612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Complete data cleaning\n",
    "df.fillna(\"\", inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "def clean_code(code):\n",
    "    # Extract the part before any space (if exists)\n",
    "    return re.split(r'\\s+', code, 1)[0]\n",
    "\n",
    "df['Code'] = df['Code'].apply(clean_code)\n",
    "# Remove HTML characters from all columns\n",
    "df = df.applymap(lambda x: html.unescape(x) if isinstance(x, str) else x)\n",
    "\n"
   ],
   "id": "f48e8b68c8c91b6a",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T11:44:41.142307Z",
     "start_time": "2024-07-04T11:44:41.039322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Save to Parquet\n",
    "os.makedirs('processed', exist_ok=True)\n",
    "df.to_parquet('processed/grainger_products.parquet', index=False)\n",
    "print(\"Product details have been saved to 'processed/grainger_products.parquet'\")\n",
    "print(\"\\nHead of DataFrame:\")\n",
    "print(df.head(), \"\\n\")\n",
    "print(\"Tail of DataFrame:\")\n",
    "print(df.tail(), \"\\n\")\n",
    "print(\"Size of DataFrame:\", df.size, \"\\n\")\n",
    "print(\"Values in DataFrame:\")\n",
    "print(df.values)\n"
   ],
   "id": "83da416308130c5f",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T11:44:44.895712Z",
     "start_time": "2024-07-04T11:44:44.714725Z"
    }
   },
   "cell_type": "code",
   "source": "!bash ../start_local.sh",
   "id": "3f2c7abf94c1cf0f",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "d0931a03f4e24a89",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
