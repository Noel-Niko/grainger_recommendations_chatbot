{
 "cells": [
  {
   "cell_type": "code",
   "id": "68cd7228-5049-4f1e-ac7e-04696aaea0bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T01:04:05.354604Z",
     "start_time": "2024-06-30T01:03:59.323809Z"
    }
   },
   "source": [
    "!pip install -r requirements.txt"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "873f657c",
   "metadata": {},
   "source": [
    "### <font color='red'>Setup</font> \n",
    "---\n",
    "<font color='red'>⚠️ ⚠️ ⚠️</font> \n",
    "Before running this notebook, ensure you've run the [Set-Up Bedrock notebook](./set-up_bedrock.ipynb) notebook. <font color='red'>⚠️ ⚠️ ⚠️</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06fbee0-9db9-4a9f-8792-b71a670d82f8",
   "metadata": {},
   "source": [
    "## Configure Bedrock\n",
    "\n",
    "Create the necessary clients to invoke Bedrock models. If you need to pass in a certain role then set those values by uncommenting the section below.\n",
    "\n",
    "First we instantiate using Anthropic Claude V2 for text generation, and Titan Embeddings G1 - Text for text embeddings.\n",
    "\n",
    "Note: Many different models are available with Bedrock. Replace the `model_id` to change the model.\n",
    "\n",
    "`llm = Bedrock(model_id=\"anthropic.claude-v2\")`\n",
    "\n",
    "Information on available model IDs [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html)"
   ]
  },
  {
   "cell_type": "code",
   "id": "5f5873db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T01:04:05.378874Z",
     "start_time": "2024-06-30T01:04:05.363624Z"
    }
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2628ef76-b545-4bf9-ade6-5b9f5c2c62b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T01:04:06.937719Z",
     "start_time": "2024-06-30T01:04:05.382958Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from IPython.display import Image\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=False)\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n",
    "\n",
    "model_parameter = {\n",
    "    \"temperature\": 0.0, \n",
    "    \"top_p\": .5, \n",
    "    \"top_k\": 250, \n",
    "    \"max_tokens_to_sample\": 2000, \n",
    "    \"stop_sequences\": [\"\\n\\n Human: bye\"]\n",
    "}\n",
    "llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    model_kwargs=model_parameter, \n",
    "    client=bedrock_runtime\n",
    ")"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1473baff",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "This notebook is using the LangChain framework where it has integrations with different services and the following tools:\n",
    "\n",
    "- **LLM (Large Language Model)**: Anthropic Claude V2 available through Amazon Bedrock\n",
    "\n",
    "  This model will be used to understand the document chunks and provide an answer in human friendly manner.\n",
    "- **Embeddings Model**: Amazon Titan Embeddings available through Amazon Bedrock\n",
    "\n",
    "  This model will be used to generate a numerical representation of the textual documents\n",
    "- **Document Loader**: [S3FileLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.s3_file.S3FileLoader.html) and PDF Loader available through LangChain\n",
    "\n",
    "  This is the loader that can load the documents from a source, for the sake of this notebook we are loading the sample files from a local path. This could easily be replaced with a loader to load documents from enterprise internal systems.\n",
    "\n",
    "- **Vector Store**: In-Memory store FAISS\n",
    "\n",
    "  The index helps to compare the input embedding and the document embeddings to find relevant document\n",
    "- **Wrapper**: wraps index, vector store, embeddings model and the LLM to abstract away the logic from the user."
   ]
  },
  {
   "cell_type": "code",
   "id": "83ad4159-24aa-486d-8872-328109f0f642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T01:04:06.946744Z",
     "start_time": "2024-06-30T01:04:06.942906Z"
    }
   },
   "source": [
    "#TEST\n",
    "customer_input = \"I am  looking for a fan for our warehouses that will be energy efficient and assist with air circulation for the large open indoor warehouse. I need to be able to purchase in bulk for several locations in Indiana.\"\n",
    "\n",
    "\n",
    "# Customer id to infuse order history and delivery address\n",
    "customer_id = \"2\""
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "raw",
   "id": "c0c499ed-f5a2-4b26-8ef6-991c39fb2f63",
   "metadata": {},
   "source": [
    "## Extract `product relevant` information\n",
    "\n",
    "Product data is stored in the grainger_product.parquet.\n",
    "\n",
    "# Without market research the assumption was made that Grainger Customers are meaningfully segmented by Find industry, size, Sustainability Focus, Inventory Manager, and the location\n",
    "\n",
    "TODO: Add to initial data icons and provide user with gov approved (JWOD, GSA, TAA) or environmentally conscious status.\n",
    "\n",
    "\n",
    "The prompt instructs the LLM to fetch the relevant information from the user prompt based on the above to facilitate a query of the source product data. \n",
    "\n",
    "Note the specific prompt template for the `entity extraction`"
   ]
  },
  {
   "cell_type": "code",
   "id": "a6c4210f-f08b-49d2-b9b7-45da05657fc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T01:04:08.840676Z",
     "start_time": "2024-06-30T01:04:06.949492Z"
    }
   },
   "source": [
    "# Identify product attributes from customer prompt to generate better results\n",
    "ner_prompt = \"\"\"Human: Find industry, size, Sustainability Focus, Inventory Manager, and the location in the customer input.\n",
    "Instructions:\n",
    "The industry can be one of the following: Manufacturing, Warehousing, Government and Public Safety, Education, Food and Beverage Distribution, Hospitality, Property Management, Retail, or Other\n",
    "The size can be one of the following: Small Businesses (Smaller companies might prioritize cost-effective solutions and fast shipping options), or Large Enterprises (Larger organizations may require more comprehensive solutions, including strategic services like inventory management and safety consulting), Womens, Other\n",
    "The Sustainability Focused true or false meaning Environmentally Conscious Buyers: Customers interested in sustainability solutions, looking for products that focus on energy management, water conservation, waste reduction, and air quality improvement, or NOT Environmentally Conscious Buyers,\n",
    "The Inventory Manager true or false meaning a purchaser in large amounts to supply an organizational group, versus an individual user purchasing for personal use, \n",
    "The output must be in JSON format inside the tags <attributes></attributes>\n",
    "\n",
    "If the information of an entity is not available in the input then don't include that entity in the JSON output\n",
    "\n",
    "Begin!\n",
    "\n",
    "Customer input: {customer_input}\n",
    "Assistant:\"\"\"\n",
    "entity_extraction_result = llm(ner_prompt.format(customer_input=customer_input)).strip()\n",
    "print(entity_extraction_result)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eaddb1cd",
   "metadata": {},
   "source": [
    "#### Extract values into JSON"
   ]
  },
  {
   "cell_type": "code",
   "id": "80ce47b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T01:04:08.855266Z",
     "start_time": "2024-06-30T01:04:08.842908Z"
    }
   },
   "source": [
    "import re\n",
    "import json\n",
    "result = re.search('<attributes>(.*)</attributes>', entity_extraction_result, re.DOTALL)\n",
    "attributes = json.loads(result.group(1))\n",
    "attributes"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4c556d2c-d133-4bce-a4a2-27721191f8bb",
   "metadata": {},
   "source": [
    "## Use Retrieval Augmented Generation (RAG) \n",
    "\n",
    "Note: documents loaded with [S3FileLoader available under LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/) can be split into smaller chunks. The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limit of the length of input tokens limited to 8k tokens, which roughly translates to ~32000 characters. For the sake of this use-case we are creating chunks of roughly 1000 characters with an overlap of 100 characters using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html).\n",
    "\n",
    "Below: fetching our productdata and creating the embeddings for \n",
    "1. Product catalog description\n",
    "2. Customer reviews\n",
    "\n",
    "# TODO: ADD order history for authenticated user\n",
    "3. Order History "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a9f1b13483921f58"
  },
  {
   "cell_type": "code",
   "id": "a93668d2-22c2-424f-9e92-05d1f5a0194c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T01:04:09.849353Z",
     "start_time": "2024-06-30T01:04:08.857549Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "parquet_file_path = \"processed/grainger_products.parquet\"\n",
    "print(\"Attempting to load file from:\", parquet_file_path)\n",
    "\n",
    "# Now attempt to load the file\n",
    "try:\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "    print(\"File loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Error loading file:\", e)\n",
    "\n",
    "print(df.head())"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fc692daa-6f4d-41bf-a724-1a1448aff55c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T01:04:09.858776Z",
     "start_time": "2024-06-30T01:04:09.851434Z"
    }
   },
   "source": [
    "# ## Approach 1: Using Textual Columns Directly\n",
    "# # To focus on text-based similarity (e.g., based on 'Name' and 'Description'), columns are concatenated\n",
    "# # FAISS.from_texts is used. This allows more granular control.\n",
    "# from langchain.embeddings import BedrockEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# \n",
    "# # Initialize the Titan Embeddings Model\n",
    "# print(\"Initializing Titan Embeddings Model...\")\n",
    "# bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_runtime)\n",
    "# print(\"Titan Embeddings Model initialized.\")\n",
    "# \n",
    "# # Load the Grainger products data from parquet file at path relative to the notebook\n",
    "# parquet_file_path = \"processed/grainger_products.parquet\"\n",
    "# print(f\"Loading data from parquet file: {parquet_file_path}...\")\n",
    "# products_df = pd.read_parquet(parquet_file_path)\n",
    "# print(\"Data loaded successfully.\")\n",
    "# \n",
    "# # Concatenate 'Name', 'Description', and 'Code' columns\n",
    "# print(\"Concatenating 'Name', 'Description', and 'Code' columns...\")\n",
    "# products_df['text_content'] = products_df['Name'].fillna('') + ' ' + products_df['Description'].fillna('') + ' ' + products_df['Code'].fillna('')\n",
    "# \n",
    "# # Create FAISS vector store from concatenated text content\n",
    "# print(\"Creating FAISS vector store based on concatenated text content...\")\n",
    "# vectorstore_faiss = FAISS.from_texts(products_df['text_content'], bedrock_embeddings)\n",
    "# print(\"FAISS vector store created.\")\n",
    "# \n",
    "# # TEST: Process a query based on a product code\n",
    "# customer_code = \"1CXK5\"  # Example product code to search\n",
    "# print(f\"Processing customer query for product with code: {customer_code}...\")\n",
    "# query_result = products_df[products_df['Code'] == customer_code]\n",
    "# \n",
    "# if not query_result.empty:\n",
    "#     print(\"Product details found:\")\n",
    "#     print(query_result)\n",
    "# else:\n",
    "#     print(f\"No product found with code: {customer_code}\")\n",
    "# \n",
    "# # TEST: Process a query based on customer input\n",
    "# customer_input = \"Men's insulated boots\"\n",
    "# print(f\"Processing customer input: {customer_input}...\")\n",
    "# query_embedding = bedrock_embeddings.embed_query(customer_input)\n",
    "# print(\"Customer input processed.\")\n",
    "# \n",
    "# # Convert query embedding to numpy array\n",
    "# np_array_query_embedding = np.array(query_embedding)\n",
    "# print(\"Query embedding converted to numpy array.\")\n",
    "# \n",
    "# # Print the resulting query embedding\n",
    "# print(\"Resulting query embedding:\")\n",
    "# print(np_array_query_embedding)\n"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d47dcae9-07c3-4cee-87d7-371e7f9f18ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T01:05:06.590119Z",
     "start_time": "2024-06-30T01:04:09.861792Z"
    }
   },
   "source": [
    "#VERSION 2: AS A DOCUMENT\n",
    "# Automates the process and optimizes for large and changing data sets.\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "        \n",
    "# Initialize the Titan Embeddings Model\n",
    "print(\"Initializing Titan Embeddings Model...\")\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_runtime)\n",
    "print(\"Titan Embeddings Model initialized.\")\n",
    "\n",
    "documents = []\n",
    "for _, row in df.iterrows():\n",
    "    page_content = f\"{row['Code']} {row['Name']} {row['Brand']} {row['Description'] if pd.notna(row['Description']) else ''}\"\n",
    "    metadata = {\n",
    "        'Brand': row['Brand'],\n",
    "        'Code': row['Code'],\n",
    "        'Name': row['Name'],\n",
    "        'Description': row['Description'],\n",
    "        'Price': row['Price']\n",
    "    }\n",
    "    documents.append(Document(page_content, metadata))\n",
    "\n",
    "\n",
    "# Print the structured documents\n",
    "print(\"Structured documents created:\")\n",
    "for idx, doc in enumerate(documents[:5], 1):  \n",
    "    print(f\"Document {idx} of {len(documents)}:\")\n",
    "    print(doc.page_content[:200])\n",
    "    print()\n",
    "\n",
    "# Create FAISS vector store from structured documents\n",
    "print(\"Creating FAISS vector store from structured documents...\")\n",
    "vectorstore_faiss_doc = FAISS.from_documents(documents, bedrock_embeddings)\n",
    "print(\"FAISS vector store created.\")\n",
    "\n",
    "\n",
    "customer_input = \"Men's insulted boots.\"\n",
    "query_embedding_doc = bedrock_embeddings.embed_query(customer_input)\n",
    "print(\"Customer input processed.\")\n",
    "\n",
    "# Convert query embedding to numpy array\n",
    "np_array_query_embedding_doc = np.array(query_embedding_doc)\n",
    "print(\"Query embedding converted to numpy array.\")\n",
    "\n",
    "# Print the resulting query embedding\n",
    "print(\"Resulting query embedding:\")\n",
    "print(np_array_query_embedding_doc)\n"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T01:05:06.723175Z",
     "start_time": "2024-06-30T01:05:06.721842Z"
    }
   },
   "cell_type": "code",
   "source": "## Going forward the FAISS.from_document version is used",
   "id": "a85270a8bc737992",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1923edcb-2dd4-4df2-ae56-462935eae428",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T01:05:06.757068Z",
     "start_time": "2024-06-30T01:05:06.755648Z"
    }
   },
   "source": [
    "# ## USING SELENIUM\n",
    "# \n",
    "# # import asyncio\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# \n",
    "# # df_path = 'processed/grainger_products.parquet'\n",
    "# # df = pd.read_parquet(df_path)\n",
    "# \n",
    "# \n",
    "# async def get_images(recommendations_list):\n",
    "#     options = Options()\n",
    "#     options.add_argument(\"--headless\")\n",
    "#     options.add_argument(\"--disable-gpu\")\n",
    "#     service = Service(ChromeDriverManager().install())\n",
    "#     driver = webdriver.Chrome(service=service, options=options)\n",
    "# \n",
    "#     image_tasks = []\n",
    "#     image_urls = []\n",
    "#     total_image_time = 0.0\n",
    "# \n",
    "#     for item in recommendations_list:\n",
    "#         # Split the recommendation string to get text and code\n",
    "#         parts = item.split(', ')\n",
    "#         code = parts[-1]  # Code is the last element\n",
    "#         text = ', '.join(parts[:-1])  # Text is everything except the code\n",
    "# \n",
    "#         if code in df['Code'].values:\n",
    "#             start_time = time.time()\n",
    "#             image_url = df.loc[df['Code'] == code, 'PictureUrl600'].iloc[0]\n",
    "#             end_time = time.time()\n",
    "#             total_image_time += end_time - start_time\n",
    "#             image_urls.append((code, image_url))\n",
    "#             print(f\"Fetched image URL for {code} in {end_time - start_time:.2f} seconds\")\n",
    "# \n",
    "#             # Add image fetching task\n",
    "#             image_tasks.append(asyncio.create_task(fetch_image(code, image_url)))\n",
    "#         else:\n",
    "#             print(f\"Code {code} not found in the dataframe.\")\n",
    "# \n",
    "#     # Gather all image tasks concurrently\n",
    "#     image_results = await asyncio.gather(*image_tasks)\n",
    "# \n",
    "#     driver.quit()  # Close the webdriver instance\n",
    "# \n",
    "#     return image_urls, total_image_time\n",
    "# \n",
    "# async def fetch_image(code, image_url):\n",
    "#     # Simulate fetching image (replace with actual fetching logic)\n",
    "#     await asyncio.sleep(1)\n",
    "#     return f\"Image URL for {code}: {image_url}\"\n",
    "# \n",
    "# # Run asyncio event loop with the async function for fetching images\n",
    "# image_urls, total_image_time = await get_images(recommendations)\n",
    "# \n",
    "# # Print or process the image results as needed\n",
    "# print(\"Image URLs:\", image_urls)\n",
    "# print(\"Total Image Time:\", total_image_time)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# USING REQUESTS\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "# df_path = 'processed/grainger_products.parquet'\n",
    "# df = pd.read_parquet(df_path)\n",
    "\n",
    "async def get_images(recommendations_list):\n",
    "    image_tasks = []\n",
    "    image_urls = []\n",
    "    total_image_time = 0.0\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for item in recommendations_list:\n",
    "            # Split the recommendation string to get text and code\n",
    "            parts = item.split(', ')\n",
    "            code = parts[-1]  # Code is the last element\n",
    "            text = ', '.join(parts[:-1])  # Text is everything except the code\n",
    "\n",
    "            if code in df['Code'].values:\n",
    "                start_time = time.time()\n",
    "                image_url = df.loc[df['Code'] == code, 'PictureUrl600'].iloc[0]\n",
    "                end_time = time.time()\n",
    "                total_image_time += end_time - start_time\n",
    "                image_urls.append((code, image_url))\n",
    "                print(f\"Fetched image URL for {code} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "                # Add image fetching task\n",
    "                image_tasks.append(fetch_image(session, code, image_url))\n",
    "            else:\n",
    "                print(f\"Code {code} not found in the dataframe.\")\n",
    "\n",
    "        # Gather all image tasks concurrently\n",
    "        image_results = await asyncio.gather(*image_tasks)\n",
    "\n",
    "    return image_urls, total_image_time\n",
    "\n",
    "async def fetch_image(session, code, image_url):\n",
    "    async with session.get(image_url) as response:\n",
    "        if response.status == 200:\n",
    "            await response.read()  # Simulate fetching image (you can save or process the image here)\n",
    "            return f\"Image URL for {code}: {image_url}\"\n",
    "        else:\n",
    "            return f\"Failed to fetch image for {code}: {image_url}\"\n",
    "\n",
    "# Run asyncio event loop with the async function for fetching images\n",
    "image_urls, total_image_time = await get_images(recommendations)\n",
    "\n",
    "# Print or process the image results as needed\n",
    "print(\"Image URLs:\", image_urls)\n",
    "print(\"Total Image Time:\", total_image_time)\n"
   ],
   "id": "bd985d020c7cd1b0",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9d1ae5aa7940f4d4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f1fd6c9d-a512-4f1e-ba8f-486d8b88f71f",
   "metadata": {},
   "source": [
    "## Generate *`n`* style recommendations\n",
    "\n",
    "Make a query to embed the LLM using customer input. Using LangChain for orchestration of RAG. It also provides a framework for orchestrating RAG flows with what purpose built \"chains\". In this section, we will see how to be a [retrieval chain](https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa) which is more comprehensive and robust than the original retrieval system we built above.\n",
    "\n",
    "The workflow we used above follows the following process:\n",
    "1. User input is received.\n",
    "2. User input is queried against the vector database to retrieve the relevant products\n",
    "3. Product description and chat memory are inserted into a new prompt to respond to the user input.\n",
    "4. This output is fed into the stable diffusion model to return the relevant images\n",
    "\n",
    "However, more complex methods of interacting with the user input can generate more accurate results in RAG architectures. One of the popular mechanisms which can increase accuracy of these retrieval systems is utilizing more than one call to an LLM in order to reformat the user input for more effective search to your vector database. A better workflow is described below compared to the one we already built...\n",
    "\n",
    "1. User input is received.\n",
    "2. An LLM is used to reword the user input to be a better search query for the vector database based on the chat history and product description. \n",
    "3. This could include things like condensing, rewording, addition of chat context, or stylistic changes.\n",
    "4. Reformatted user input is queried against the vector database to retrieve relevant products.The reformatted user input and relevant documents are inserted into a new prompt in order to generate the new style. \n",
    "5. This is then fed into the stable diffusion model to generate the images. \n",
    "\n",
    "In your application the images can come from a pre canned images \n",
    "\n",
    "We will now build out this second workflow using LangChain below. First we need to make a prompt which will reformat the user input to be more compatible for searching of the vector database. The way we do this is by providing the chat history as well as the some basic instructions to Claude and asking it to condense the input into a single output. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# More complex customer Input\n",
    "customer_input = \"I am looking for waterproof insulated boots for my men working on my commercial deep sea fishing boat in the arctic. Must have large sizes.\"\n",
    "print(f\"Processing customer input: {customer_input}...\")\n",
    "query_embedding_doc = bedrock_embeddings.embed_query(customer_input)\n",
    "print(\"Customer input processed.\")\n",
    "\n",
    "# Convert query embedding to numpy array\n",
    "np_array_query_embedding_doc = np.array(query_embedding_doc)\n",
    "print(\"Query embedding converted to numpy array.\")\n",
    "\n",
    "# Print the resulting query embedding\n",
    "print(\"Resulting query embedding:\")\n",
    "print(np_array_query_embedding_doc)\n"
   ],
   "id": "6d9429ff50622b28",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "# import time\n",
    "# from PIL import Image\n",
    "# from IPython import display\n",
    "# from base64 import b64decode\n",
    "# import base64\n",
    "# import io\n",
    "# import json\n",
    "# import os\n",
    "# import sys\n",
    "# import ipywidgets as widgets\n",
    "# import re\n",
    "# \n",
    "# # Function to extract attributes using NER prompt\n",
    "# def extract_attributes(customer_input):\n",
    "#     ner_prompt = \"\"\"Human: Find location, environment, and gender in the customer input.\n",
    "#     The output must be in JSON format inside the tags <attributes></attributes>\n",
    "#     \n",
    "#     If the information of an entity is not available in the input then don't include that entity in the JSON output\n",
    "#     \n",
    "#     Begin!\n",
    "#     \n",
    "#     Customer input: {customer_input}\n",
    "#     Assistant:\"\"\"\n",
    "#     print(\"Extracting attributes...\")\n",
    "#     start_time = time.time()\n",
    "#     entity_extraction_result = llm(ner_prompt.format(customer_input=customer_input)).strip()\n",
    "#     print(\"Raw entity extraction result:\", entity_extraction_result)\n",
    "#     print(f\"Attribute extraction completed in {time.time() - start_time:.2f} seconds.\")\n",
    "#     \n",
    "#     try:\n",
    "#         attributes_json = re.search(r'<attributes>(.*?)</attributes>', entity_extraction_result, re.DOTALL).group(1)\n",
    "#         attributes = json.loads(attributes_json)\n",
    "#         return attributes\n",
    "#     except (AttributeError, json.JSONDecodeError) as e:\n",
    "#         print(\"Error parsing attributes:\", str(e))\n",
    "#         return {}\n",
    "# \n",
    "# attributes = extract_attributes(customer_input)\n",
    "# \n",
    "# gender_map = {\n",
    "#     'Womens': 'of a female ',\n",
    "#     'Mens': 'of a male '\n",
    "# } \n",
    "# \n",
    "# print(\"customer_input: \", customer_input)\n",
    "# print(\"attributes: \", attributes)\n"
   ],
   "id": "f276373d3c35cfc1",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from langchain.chains import RetrievalQA\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# \n",
    "# prompt_template = \"\"\"Human: Use the following pieces of context to generate 5 recommendations for the customer input at the end.\n",
    "# <context>\n",
    "# {context}\n",
    "# </context>\n",
    "# \n",
    "# Customer Input: {question}\n",
    "# Each style recommendation must be inside the tags <product></product>.\n",
    "# \n",
    "# Skip the preamble.\n",
    "# Assistant: \"\"\"\n",
    "# PROMPT = PromptTemplate(\n",
    "#     template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "# )\n",
    "# \n",
    "# # Use RetrievalQA customizations for improving Q&A experience\n",
    "# qa = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=vectorstore_faiss_doc.as_retriever(\n",
    "#         search_type=\"similarity\", search_kwargs={\"k\": 6}\n",
    "#     ),\n",
    "#     return_source_documents=False,\n",
    "#     chain_type_kwargs={\"prompt\": PROMPT},\n",
    "# )\n",
    "# recs_response = qa({\"query\": customer_input})['result']\n",
    "# recs_response"
   ],
   "id": "1f2810aadebfd37d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## GET LIST OF PRODUCTS AND CODES\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template2 = \"\"\"Human: Extract list of 5 products and their respective physical IDs from catalog that matches the style given below. \n",
    "The catalog of products is provided under <catalog></catalog> tags below.\n",
    "<catalog>\n",
    "{context}\n",
    "</catalog>\n",
    "Style: {question}\n",
    "\n",
    "The output should be a json of the form <products>[{{\"product\": <description of the product from the catalog>, \"code\":<code of the product from the catalog>}}, ...]</products>\n",
    "Skip the preamble and always return valid json.\n",
    "Assistant: \"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template2, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Use RetrievalQA customizations for improving Q&A experience\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss_doc.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 6}\n",
    "    ),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")\n",
    "\n",
    "recs_response = qa({\"query\": customer_input})['result']\n",
    "recs_response"
   ],
   "id": "8f8f54df57597629",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PROCESS THE RESPONSE TO JSON\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "# Ensure recs_response is handled correctly\n",
    "recs_response = recs_response.strip()  # Remove leading/trailing whitespace\n",
    "response_json = \"\"\n",
    "\n",
    "# Check if the response starts and ends with expected JSON markers\n",
    "if recs_response.startswith(\"<products>\") and recs_response.endswith(\"</products>\"):\n",
    "    json_content = recs_response[len(\"<products>\") : -len(\"</products>\")].strip()\n",
    "    \n",
    "    try:\n",
    "        parsed_response = json.loads(json_content)\n",
    "        \n",
    "        if isinstance(parsed_response, list):\n",
    "            products_list = []\n",
    "            for product_info in parsed_response:\n",
    "                # Assuming product_info is a dictionary with 'product' and 'code' keys\n",
    "                product_data = {\n",
    "                    \"product\": product_info.get(\"product\", \"\"),\n",
    "                    \"code\": product_info.get(\"code\", \"\")\n",
    "                }\n",
    "                products_list.append(product_data)\n",
    "            \n",
    "            response_json = {\"products\": products_list}\n",
    "            print(response_json)\n",
    "        else:\n",
    "            print(\"Error: Unexpected format of parsed response\")\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {str(e)}\")\n",
    "else:\n",
    "    print(\"Error: Unexpected format of recs_response\")\n",
    "\n"
   ],
   "id": "c6fb0f4168ef8e37",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare input to fetch images for each \n",
    "recommendations = re.findall('<product>(.*?)</product>', recommendations)\n",
    "recommendations"
   ],
   "id": "3729e89773f58a10",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # FOR FAIS.text_input only for vectorstore_faiss not vectorstore_faiss_doc\n",
    "# # Extract products and codes using regex\n",
    "# def response_to_json(recs_response):\n",
    "#     pattern = r'\\{\"product\": \"(.*?)\", \"code\": \"(.*?)\"}'\n",
    "#     return re.findall(pattern, recs_response, re.DOTALL)\n",
    "#     \n",
    "#     \n",
    "# print(response_to_json(recs_response))\n"
   ],
   "id": "f6f63f8c1fd11e61",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import xmltodict\n",
    "# import json\n",
    "# \n",
    "# def parse_products_to_json(data_string):\n",
    "#     # Strip unnecessary newlines and spaces to ensure valid XML\n",
    "#     data_string = data_string.strip()\n",
    "# \n",
    "#     # Parse XML-like data into a Python dictionary\n",
    "#     parsed_data = xmltodict.parse(data_string)\n",
    "# \n",
    "#     # Extract the list of products from the parsed dictionary\n",
    "#     products_list = parsed_data['products']\n",
    "# \n",
    "#     # Convert the products list to JSON format\n",
    "#     json_data = json.dumps(products_list, indent=2)\n",
    "# \n",
    "#     return json_data\n",
    "# \n",
    "# \n",
    "# # Call the function to parse to JSON\n",
    "# parsed_json = parse_products_to_json(recs_response)\n",
    "# print(parsed_json)\n"
   ],
   "id": "5451c374d21de7b8",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import json\n",
    "# import re\n",
    "# \n",
    "# def response_to_json(recs_response):\n",
    "#     pattern = r'\\{\"product\": \"(.*?)\", \"code\": \"(.*?)\"}'\n",
    "#     matches = re.findall(pattern, recs_response, re.DOTALL)\n",
    "#     \n",
    "#     # Convert matches to list of dictionaries\n",
    "#     products_list = [{\"product\": match[0], \"code\": match[1]} for match in matches]\n",
    "#     \n",
    "#     return products_list\n",
    "# \n",
    "# # Convert to JSON\n",
    "# products_json = response_to_json(recs_response)\n",
    "# json_data = json.dumps(products_json, indent=2)  # Convert Python list to JSON string\n",
    "# \n",
    "# print(json_data)\n"
   ],
   "id": "82cf8a822317d836",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import re\n",
    "# import json\n",
    "# \n",
    "# def parse_products_from_string(recs_response1):\n",
    "#     # Strip unnecessary newlines, spaces, and XML tags\n",
    "#     recs_response = re.sub(r'<[^>]+>', '', recs_response1)\n",
    "#     \n",
    "#     # Extract the substring between '[' and ']' to isolate the JSON array\n",
    "#     match = re.search(r'\\[(.*?)\\]', recs_response1, re.DOTALL)\n",
    "#     if match:\n",
    "#         json_data = match.group(1)\n",
    "#         \n",
    "#         # Attempt to load the JSON data\n",
    "#         try:\n",
    "#             products_list = json.loads(json_data)\n",
    "#             return products_list\n",
    "#         except json.JSONDecodeError as e:\n",
    "#             print(f\"Error decoding JSON: {e}\")\n",
    "#             return []\n",
    "#     else:\n",
    "#         print(\"No JSON data found in the input.\")\n",
    "#         return []\n",
    "# \n",
    "# # Example usage:\n",
    "# recs_response1 = '''\n",
    "# <products>\n",
    "# [\n",
    "# {\"product\": \"Rubber Boot: Cold-Insulated/Electrical Hazard (EH)/Oil-Resistant Sole/Plain Toe/Waterproof, 13, 1 PR THE ORIGINAL MUCK BOOT CO.\", \"code\":\"21A652\"},\n",
    "# {\"product\": \"Rubber Boot: Cold-Insulated/Electrical Hazard (EH)/Oil-Resistant Sole/Plain Toe/Waterproof, 11, 1 PR THE ORIGINAL MUCK BOOT CO.\", \"code\":\"21A650\"},\n",
    "# {\"product\": \"Rubber Boot: Cold-Insulated/Electrical Hazard (EH)/Oil-Resistant Sole/Plain Toe/Waterproof, 10, 1 PR THE ORIGINAL MUCK BOOT CO.\", \"code\":\"21A649\"}\n",
    "# ]\n",
    "# </products>\n",
    "# '''\n",
    "# \n",
    "# # Call the function to parse the products from the response\n",
    "# parsed_products = parse_products_from_string(recs_response1)\n",
    "# \n",
    "# # Print the parsed products list\n",
    "# print(parsed_products)\n"
   ],
   "id": "6a6c45329f79dae4",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # SYNCHRONOUS CALLS\n",
    "# \n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "# import time\n",
    "# from PIL import Image, ImageDraw, ImageFont\n",
    "# from IPython import display\n",
    "# from base64 import b64decode\n",
    "# import base64\n",
    "# import io\n",
    "# import json\n",
    "# import os\n",
    "# import sys\n",
    "# import ipywidgets as widgets\n",
    "# import re\n",
    "# \n",
    "# # Function to extract attributes using NER prompt\n",
    "# def extract_attributes(customer_input):\n",
    "#     ner_prompt = \"\"\"Human: Find location, environment, and gender in the customer input.\n",
    "#     The output must be in JSON format inside the tags <attributes></attributes>\n",
    "#     \n",
    "#     If the information of an entity is not available in the input then don't include that entity in the JSON output\n",
    "#     \n",
    "#     Begin!\n",
    "#     \n",
    "#     Customer input: {customer_input}\n",
    "#     Assistant:\"\"\"\n",
    "#     print(\"Extracting attributes...\")\n",
    "#     start_time = time.time()\n",
    "#     entity_extraction_result = llm(ner_prompt.format(customer_input=customer_input)).strip()\n",
    "#     print(\"Raw entity extraction result:\", entity_extraction_result)\n",
    "#     print(f\"Attribute extraction completed in {time.time() - start_time:.2f} seconds.\")\n",
    "#     \n",
    "#     try:\n",
    "#         attributes_json = re.search(r'<attributes>(.*?)</attributes>', entity_extraction_result, re.DOTALL).group(1)\n",
    "#         attributes = json.loads(attributes_json)\n",
    "#         return attributes\n",
    "#     except (AttributeError, json.JSONDecodeError) as e:\n",
    "#         print(\"Error parsing attributes:\", str(e))\n",
    "#         return {}\n",
    "# \n",
    "# # Function to generate thumbnails with recommendation overlay based on recommendations and attributes\n",
    "# def generate_thumbnails_with_overlay(recommendations, attributes):\n",
    "#     gender_map = {\n",
    "#         'Womens': 'of a female ',\n",
    "#         'Mens': 'of a male '\n",
    "#     }\n",
    "# \n",
    "#     os.makedirs(\"data\", exist_ok=True)\n",
    "#     image_strip = \"\"\n",
    "#     \n",
    "#     for i, rec in enumerate(recommendations):\n",
    "#         print(f\"Generating thumbnail {i+1}/{len(recommendations)} for recommendation: {rec}\")\n",
    "#         \n",
    "#         prompt_parts = [\n",
    "#             f\"Product view {gender_map.get(attributes.get('gender', ''))}in {rec}, dslr, ultra quality, dof, film grain, Fujifilm XT3, crystal clear, 8K UHD\"\n",
    "#         ]\n",
    "#         \n",
    "#         if attributes.get('environment'):\n",
    "#             prompt_parts.append(f\", in a {attributes['environment']} setting\")\n",
    "#         \n",
    "#         prompt_text = \"\".join(prompt_parts)\n",
    "#         \n",
    "#         request = json.dumps({\n",
    "#             \"text_prompts\": [\n",
    "#                 {\"text\": prompt_text, \"weight\": 1.0},\n",
    "#                 {\"text\": \"poorly rendered\", \"weight\": -1.0}\n",
    "#             ],\n",
    "#             \"cfg_scale\": 9,\n",
    "#             \"seed\": 4000,\n",
    "#             \"steps\": 50,\n",
    "#             \"style_preset\": \"photographic\",\n",
    "#         })\n",
    "#         modelId = \"stability.stable-diffusion-xl-v1\"\n",
    "# \n",
    "#         start_time = time.time()\n",
    "#         response = bedrock_runtime.invoke_model(body=request, modelId=modelId)\n",
    "#         response_body = json.loads(response.get(\"body\").read())\n",
    "#         print(f\"Thumbnail {i+1} generation completed in {time.time() - start_time:.2f} seconds.\")\n",
    "# \n",
    "#         base_64_img_str = response_body[\"artifacts\"][0].get(\"base64\")\n",
    "#         # Convert base64 image to a thumbnail (reduce size)\n",
    "#         img_data = b64decode(base_64_img_str)\n",
    "#         img = Image.open(io.BytesIO(img_data))\n",
    "#         img.thumbnail((200, 200))  # Adjust the size as needed for thumbnails\n",
    "# \n",
    "#         # Add recommendation text overlay\n",
    "#         draw = ImageDraw.Draw(img)\n",
    "#         text = rec\n",
    "#         font = ImageFont.load_default()  # Adjust the font and size here\n",
    "#         \n",
    "#         # Calculate wrapped text dimensions\n",
    "#         max_text_width = img.width - 10  # Max width for wrapping text\n",
    "#         lines = []\n",
    "#         words = text.split()\n",
    "#         while words:\n",
    "#             line = ''\n",
    "#             while words and draw.textsize(line + words[0], font=font)[0] <= max_text_width:\n",
    "#                 line += words.pop(0) + ' '\n",
    "#             lines.append(line)\n",
    "#         wrapped_text = '\\n'.join(lines)\n",
    "#         \n",
    "#         # Calculate text size after wrapping\n",
    "#         wrapped_text_width, wrapped_text_height = draw.textsize(wrapped_text, font=font)\n",
    "#         \n",
    "#         # Calculate box dimensions\n",
    "#         box_width = img.width\n",
    "#         box_height = wrapped_text_height + 10  # Adjust padding as needed\n",
    "#         \n",
    "#         # Draw black box dynamically sized for wrapped text\n",
    "#         draw.rectangle([(0, img.height - box_height), (img.width, img.height)], fill='black')\n",
    "#         \n",
    "#         # Calculate text position\n",
    "#         text_x = (img.width - wrapped_text_width) / 2\n",
    "#         text_y = img.height - box_height + (box_height - wrapped_text_height) / 2  # Center vertically\n",
    "#         \n",
    "#         # Draw wrapped text\n",
    "#         draw.text((text_x, text_y), wrapped_text, fill='white', font=font)\n",
    "#         \n",
    "#         buffered = io.BytesIO()\n",
    "#         img.save(buffered, format=\"JPEG\")\n",
    "#         base_64_thumbnail_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "# \n",
    "#         image_strip += f\"<td><img src='data:image/png;base64, {base_64_thumbnail_str}'></td>\"\n",
    "# \n",
    "#     display.display(display.HTML(\"<table><tr>\" + image_strip +\"</tr></table>\"))\n",
    "# \n",
    "# # \n",
    "# attributes = extract_attributes(customer_input)\n",
    "# generate_thumbnails_with_overlay(recommendations, attributes)\n"
   ],
   "id": "80e7e6ff-7e46-403b-9c38-f98de88bdeb9",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ASYNCHRONOUS CALLS\n",
    "import asyncio\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "import time\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython import display\n",
    "from base64 import b64decode\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import ipywidgets as widgets\n",
    "import re\n",
    "\n",
    "# Function to extract attributes using NER prompt\n",
    "async def extract_attributes(customer_input):\n",
    "    ner_prompt = \"\"\"Human: Find location, environment, and gender in the customer input.\n",
    "    The output must be in JSON format inside the tags <attributes></attributes>\n",
    "\n",
    "    If the information of an entity is not available in the input then don't include that entity in the JSON output\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    Customer input: {customer_input}\n",
    "    Assistant:\"\"\"\n",
    "    print(\"Extracting attributes...\")\n",
    "    start_time = time.time()\n",
    "    # Await the coroutine returned by asyncio.to_thread to get the result\n",
    "    entity_extraction_result = await asyncio.to_thread(llm, ner_prompt.format(customer_input=customer_input))\n",
    "    entity_extraction_result = entity_extraction_result.strip()  # Now you can safely use strip()\n",
    "    print(\"Raw entity extraction result:\", entity_extraction_result)\n",
    "    print(f\"Attribute extraction completed in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    try:\n",
    "        attributes_json = re.search(r'<attributes>(.*?)</attributes>', entity_extraction_result, re.DOTALL).group(1)\n",
    "        attributes = json.loads(attributes_json)\n",
    "        return attributes\n",
    "    except (AttributeError, json.JSONDecodeError) as e:\n",
    "        print(\"Error parsing attributes:\", str(e))\n",
    "        return {}\n",
    "# \n",
    "# # Function to generate a single thumbnail with recommendation overlay\n",
    "# async def generate_thumbnail(rec, attributes):\n",
    "#     gender_map = {\n",
    "#         'Womens': 'of a female ',\n",
    "#         'Mens': 'of a male '\n",
    "#     }\n",
    "# \n",
    "#     prompt_parts = [\n",
    "#         f\"Product view {gender_map.get(attributes.get('gender', ''))}in {rec}, dslr, ultra quality, dof, film grain, Fujifilm XT3, crystal clear, 8K UHD\"\n",
    "#     ]\n",
    "# \n",
    "#     if attributes.get('environment'):\n",
    "#         prompt_parts.append(f\", in a {attributes['environment']} setting\")\n",
    "# \n",
    "#     prompt_text = \"\".join(prompt_parts)\n",
    "# \n",
    "#     request = json.dumps({\n",
    "#         \"text_prompts\": [\n",
    "#             {\"text\": prompt_text, \"weight\": 1.0},\n",
    "#             {\"text\": \"poorly rendered\", \"weight\": -1.0}\n",
    "#         ],\n",
    "#         \"cfg_scale\": 9,\n",
    "#         \"seed\": 4000,\n",
    "#         \"steps\": 50,\n",
    "#         \"style_preset\": \"photographic\",\n",
    "#     })\n",
    "#     modelId = \"stability.stable-diffusion-xl-v1\"\n",
    "# \n",
    "#     start_time = time.time()\n",
    "#     # Simulate asynchronous network call with asyncio.sleep\n",
    "#     await asyncio.sleep(1)\n",
    "#     response = await asyncio.to_thread(bedrock_runtime.invoke_model, body=request, modelId=modelId)\n",
    "#     response_body = json.loads(response.get(\"body\").read())\n",
    "#     print(f\"Thumbnail for recommendation '{rec}' generation completed in {time.time() - start_time:.2f} seconds.\")\n",
    "# \n",
    "#     base_64_img_str = response_body[\"artifacts\"][0].get(\"base64\")\n",
    "#     # Convert base64 image to a thumbnail (reduce size)\n",
    "#     img_data = b64decode(base_64_img_str)\n",
    "#     img = Image.open(io.BytesIO(img_data))\n",
    "#     img.thumbnail((200, 200))  # Adjust the size as needed for thumbnails\n",
    "# \n",
    "#     # Add recommendation text overlay\n",
    "#     draw = ImageDraw.Draw(img)\n",
    "#     text = rec\n",
    "#     font = ImageFont.load_default()  # Adjust the font and size here\n",
    "# \n",
    "#     # Calculate wrapped text dimensions\n",
    "#     max_text_width = img.width - 10  # Max width for wrapping text\n",
    "#     lines = []\n",
    "#     words = text.split()\n",
    "#     while words:\n",
    "#         line = ''\n",
    "#         while words and draw.textsize(line + words[0], font=font)[0] <= max_text_width:\n",
    "#             line += words.pop(0) + ' '\n",
    "#         lines.append(line)\n",
    "#     wrapped_text = '\\n'.join(lines)\n",
    "# \n",
    "#     # Calculate text size after wrapping\n",
    "#     wrapped_text_width, wrapped_text_height = draw.textsize(wrapped_text, font=font)\n",
    "# \n",
    "#     # Calculate box dimensions\n",
    "#     box_width = img.width\n",
    "#     box_height = wrapped_text_height + 10  # Adjust padding as needed\n",
    "# \n",
    "#     # Draw black box dynamically sized for wrapped text\n",
    "#     draw.rectangle([(0, img.height - box_height), (img.width, img.height)], fill='black')\n",
    "# \n",
    "#     # Calculate text position\n",
    "#     text_x = (img.width - wrapped_text_width) / 2\n",
    "#     text_y = img.height - box_height + (box_height - wrapped_text_height) / 2  # Center vertically\n",
    "# \n",
    "#     # Draw wrapped text\n",
    "#     draw.text((text_x, text_y), wrapped_text, fill='white', font=font)\n",
    "# \n",
    "#     buffered = io.BytesIO()\n",
    "#     img.save(buffered, format=\"JPEG\")\n",
    "#     base_64_thumbnail_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "# \n",
    "#     return f\"<td><img src='data:image/png;base64, {base_64_thumbnail_str}'></td>\"\n",
    "# \n",
    "# async def main():\n",
    "#     \n",
    "#     customer_input = \"Customer's input\"\n",
    "#     attributes = await extract_attributes(customer_input)\n",
    "# \n",
    "#     # List to hold coroutines for each thumbnail generation\n",
    "#     tasks = [generate_thumbnail(rec, attributes) for rec in recommendations]\n",
    "# \n",
    "#     # Gather results concurrently\n",
    "#     image_strips = await asyncio.gather(*tasks)\n",
    "# \n",
    "#     # Display the HTML table with all thumbnails\n",
    "#     display.display(display.HTML(\"<table><tr>\" + \"\".join(image_strips) + \"</tr></table>\"))\n",
    "# \n",
    "# Run the main coroutine\n",
    "# await main()\n"
   ],
   "id": "76943abec14663be",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "async def generate_thumbnail(rec_tuple, attributes):\n",
    "    rec, code = rec_tuple  # Unpack the tuple into rec (product description) and code\n",
    "    \n",
    "    gender_map = {\n",
    "        'Womens': 'of a female ',\n",
    "        'Mens': 'of a male '\n",
    "    }\n",
    "\n",
    "    prompt_parts = [\n",
    "        f\"Product view {gender_map.get(attributes.get('gender', ''))}in {rec}, dslr, ultra quality, dof, film grain, Fujifilm XT3, crystal clear, 8K UHD\"\n",
    "    ]\n",
    "\n",
    "    if attributes.get('environment'):\n",
    "        prompt_parts.append(f\", in a {attributes['environment']} setting\")\n",
    "\n",
    "    prompt_text = \"\".join(prompt_parts)\n",
    "\n",
    "    request = json.dumps({\n",
    "        \"text_prompts\": [\n",
    "            {\"text\": prompt_text, \"weight\": 1.0},\n",
    "            {\"text\": \"poorly rendered\", \"weight\": -1.0}\n",
    "        ],\n",
    "        \"cfg_scale\": 9,\n",
    "        \"seed\": 4000,\n",
    "        \"steps\": 50,\n",
    "        \"style_preset\": \"photographic\",\n",
    "    })\n",
    "    modelId = \"stability.stable-diffusion-xl-v1\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Simulate asynchronous network call with asyncio.sleep\n",
    "    await asyncio.sleep(1)\n",
    "    response = await asyncio.to_thread(bedrock_runtime.invoke_model, body=request, modelId=modelId)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    print(f\"Thumbnail for recommendation '{rec}' generation completed in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    base_64_img_str = response_body[\"artifacts\"][0].get(\"base64\")\n",
    "    # Convert base64 image to a thumbnail (reduce size)\n",
    "    img_data = b64decode(base_64_img_str)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "    img.thumbnail((200, 200))  # Adjust the size as needed for thumbnails\n",
    "\n",
    "    # Add recommendation text overlay\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    text = rec\n",
    "    font = ImageFont.load_default()  # Adjust the font and size here\n",
    "    \n",
    "    # Calculate wrapped text dimensions\n",
    "    max_text_width = img.width - 10  # Max width for wrapping text\n",
    "    lines = []\n",
    "    words = text.split()\n",
    "    while words:\n",
    "        line = ''\n",
    "        while words and draw.textsize(line + words[0], font=font)[0] <= max_text_width:\n",
    "            line += words.pop(0) + ' '\n",
    "        lines.append(line)\n",
    "    wrapped_text = '\\n'.join(lines)\n",
    "    \n",
    "    # Calculate text size after wrapping\n",
    "    wrapped_text_width, wrapped_text_height = draw.textsize(wrapped_text, font=font)\n",
    "    \n",
    "    # Calculate box dimensions\n",
    "    box_width = img.width\n",
    "    box_height = wrapped_text_height + 10  # Adjust padding as needed\n",
    "    \n",
    "    # Draw black box dynamically sized for wrapped text\n",
    "    draw.rectangle([(0, img.height - box_height), (img.width, img.height)], fill='black')\n",
    "    \n",
    "    # Calculate text position\n",
    "    text_x = (img.width - wrapped_text_width) / 2\n",
    "    text_y = img.height - box_height + (box_height - wrapped_text_height) / 2  # Center vertically\n",
    "    \n",
    "    # Draw wrapped text\n",
    "    draw.text((text_x, text_y), wrapped_text, fill='white', font=font)\n",
    "    \n",
    "    buffered = io.BytesIO()\n",
    "    img.save(buffered, format=\"JPEG\")\n",
    "    base_64_thumbnail_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "    return f\"<td><img src='data:image/png;base64, {base_64_thumbnail_str}'></td>\"\n",
    "\n",
    "async def main():\n",
    "    customer_input = \"Customer's input\"\n",
    "    attributes = await extract_attributes(customer_input)\n",
    "    \n",
    "    # List to hold coroutines for each thumbnail generation\n",
    "    tasks = [generate_thumbnail(rec, attributes) for rec in recommendations]\n",
    "    \n",
    "    # Gather results concurrently\n",
    "    image_strips = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Display the HTML table with all thumbnails\n",
    "    display.display(display.HTML(\"<table><tr>\" + \"\".join(image_strips) + \"</tr></table>\"))\n",
    "\n",
    "# Run the main coroutine\n",
    "await main()\n"
   ],
   "id": "c7f9c89a3a409157",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e436593915ab5191",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# import time\n",
    "# \n",
    "# from call_for_product_reviews import navigate_to_reviews\n",
    "# from call_for_single_product_review_selenium import navigate_to_reviews_selenium\n",
    "# \n",
    "# options = Options()\n",
    "# options.add_argument(\"--headless\")\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "# service = Service(ChromeDriverManager().install())\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "# \n",
    "# print(recommendations)\n",
    "# \n",
    "# def get_recommendations(recommendations_list):\n",
    "#     for rec in recommendations_list:\n",
    "#         text, code = rec\n",
    "#         print(\"Getting review...\")\n",
    "#         \n",
    "#         # Measure time to get review without Selenium\n",
    "#         start_time = time.time()\n",
    "#         print(navigate_to_reviews(code))\n",
    "#         end_time = time.time()\n",
    "#         print(f\"Time taken to get review without Selenium: {end_time - start_time} seconds\")\n",
    "#         \n",
    "#         print(\"Getting review via Selenium...\")\n",
    "#         \n",
    "#         # Measure time to get review with Selenium\n",
    "#         start_time = time.time()\n",
    "#         print(navigate_to_reviews_selenium(code, driver))\n",
    "#         end_time = time.time()\n",
    "#         print(f\"Time taken to get review via Selenium: {end_time - start_time} seconds\")\n",
    "# \n",
    "# # Product known to have review for test\n",
    "# recommendation_tuple = (\"Recommended\", \"1VCE8\")\n",
    "# \n",
    "# # Adjusted to pass a single-element list matching the expected input\n",
    "# print(\"Selenium garnered review: \", get_recommendations([recommendation_tuple]))\n"
   ],
   "id": "a92b077fcc095fb2",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import asyncio\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# \n",
    "# \n",
    "# df_path = 'processed/grainger_products.parquet'\n",
    "# df = pd.read_parquet(df_path)\n",
    "# \n",
    "# \n",
    "# async def get_images(recommendations_list):\n",
    "#     options = Options()\n",
    "#     options.add_argument(\"--headless\")\n",
    "#     options.add_argument(\"--disable-gpu\")\n",
    "#     service = Service(ChromeDriverManager().install())\n",
    "#     driver = webdriver.Chrome(service=service, options=options)\n",
    "# \n",
    "#     image_tasks = []\n",
    "#     image_urls = []\n",
    "#     total_image_time = 0.0\n",
    "# \n",
    "#     for product_info in recommendations_list:\n",
    "#         product = product_info['product']\n",
    "#         code = product_info['code']\n",
    "#         \n",
    "#         start_time = time.time()\n",
    "#         image_url = df.loc[df['Code'] == code, 'PictureUrl600'].iloc[0] \n",
    "#         end_time = time.time()\n",
    "#         total_image_time += end_time - start_time\n",
    "#         image_urls.append((code, image_url))\n",
    "#         print(f\"Fetched image URL for {code} in {end_time - start_time:.2f} seconds\")\n",
    "# \n",
    "#         # Add image fetching task\n",
    "#         image_tasks.append(asyncio.create_task(fetch_image(code, image_url)))\n",
    "# \n",
    "#     # Gather all image tasks concurrently\n",
    "#     image_results = await asyncio.gather(*image_tasks)\n",
    "# \n",
    "#     driver.quit()  # Close the webdriver instance\n",
    "#     \n",
    "#     return image_urls, total_image_time\n",
    "# \n",
    "# async def fetch_image(code, image_url):\n",
    "#     # Simulate fetching image (replace with actual fetching logic)\n",
    "#     await asyncio.sleep(1)\n",
    "#     return f\"Image URL for {code}: {image_url}\"\n",
    "# \n",
    "# \n",
    "# # Run asyncio event loop with the async function for fetching images\n",
    "# image_urls, total_image_time = await get_images(response_json['products'])\n",
    "# \n",
    "# # Print or process the image results as needed\n",
    "# print(\"Image URLs:\", image_urls)\n",
    "# print(\"Total Image Time:\", total_image_time)\n"
   ],
   "id": "da2c2092c02945ad",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import asyncio\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# \n",
    "# \n",
    "# df_path = 'processed/grainger_products.parquet'\n",
    "# df = pd.read_parquet(df_path)\n",
    "# \n",
    "# \n",
    "# async def get_image_urls(recommendations_list):\n",
    "#     options = Options()\n",
    "#     options.add_argument(\"--headless\")\n",
    "#     options.add_argument(\"--disable-gpu\")\n",
    "#     service = Service(ChromeDriverManager().install())\n",
    "#     driver = webdriver.Chrome(service=service, options=options)\n",
    "# \n",
    "#     image_urls = []\n",
    "# \n",
    "#     for product_info in recommendations_list:\n",
    "#         code = product_info['code']\n",
    "#         \n",
    "#         # Fetch image URL from DataFrame\n",
    "#         image_url = df.loc[df['Code'] == code, 'PictureUrl600'].iloc[0]\n",
    "#         image_urls.append(image_url)\n",
    "#         print(f\"Fetched image URL for {code}: {image_url}\")\n",
    "# \n",
    "#     driver.quit()  # Close the webdriver instance\n",
    "#     \n",
    "#     return image_urls\n",
    "# \n",
    "# \n",
    "# \n",
    "# image_urls = await get_image_urls(response_json['products'])\n",
    "# \n",
    "# # Print or process the image URLs as needed\n",
    "# print(\"Image URLs:\", image_urls)\n"
   ],
   "id": "fa33ef995fdec050",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "df_path = 'processed/grainger_products.parquet'\n",
    "df = pd.read_parquet(df_path)\n",
    "\n",
    "async def get_image_url_map(recommendations_list):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    image_url_maps = []\n",
    "\n",
    "    for product_info in recommendations_list:\n",
    "        code = product_info['code']\n",
    "        \n",
    "        # Fetch image URL from DataFrame\n",
    "        image_url = df.loc[df['Code'] == code, 'PictureUrl600'].iloc[0]\n",
    "        \n",
    "        # Create a dictionary for each product with \"Code\" and \"Image URL\"\n",
    "        product_dict = {\"Code\": code, \"Image URL\": image_url}\n",
    "        \n",
    "        # Append the product dictionary to the list\n",
    "        image_url_maps.append(product_dict)\n",
    "        print(f\"Fetched image URL for {code}: {image_url}\")\n",
    "\n",
    "    driver.quit()  # Close the webdriver instance\n",
    "    \n",
    "    return image_url_maps\n",
    "\n",
    "\n",
    "image_url_maps = await get_image_url_map(response_json['products'])\n",
    "\n",
    "# Print or process the image URL maps as needed\n",
    "for image_url_map in image_url_maps:\n",
    "    print(f\"Product Code: {image_url_map['Code']} - Image URL: {image_url_map['Image URL']}\")\n"
   ],
   "id": "6c591e8afb9eb726",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import time\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import io\n",
    "import base64\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "async def generate_thumbnail(image_url, code, name):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(image_url) as resp:\n",
    "            img_data = await resp.read()\n",
    "            \n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "    img.thumbnail((200, 200))  # Adjust the size as needed for thumbnails\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    text = f\"{code}: {name}\"\n",
    "    font = ImageFont.load_default()  # Adjust the font and size here\n",
    "    \n",
    "    max_text_width = img.width - 10  # Max width for wrapping text\n",
    "    lines = []\n",
    "    words = text.split()\n",
    "    while words:\n",
    "        line = ''\n",
    "        while words and draw.textsize(line + words[0], font=font)[0] <= max_text_width:\n",
    "            line += words.pop(0) + ' '\n",
    "        lines.append(line)\n",
    "    wrapped_text = '\\n'.join(lines)\n",
    "    \n",
    "    wrapped_text_width, wrapped_text_height = draw.textsize(wrapped_text, font=font)\n",
    "    \n",
    "    box_width = img.width\n",
    "    box_height = wrapped_text_height + 10  # Adjust padding as needed\n",
    "    \n",
    "    draw.rectangle([(0, img.height - box_height), (img.width, img.height)], fill='black')\n",
    "    \n",
    "    text_x = (img.width - wrapped_text_width) / 2\n",
    "    text_y = img.height - box_height + (box_height - wrapped_text_height) / 2  # Center vertically\n",
    "    \n",
    "    draw.text((text_x, text_y), wrapped_text, fill='white', font=font)\n",
    "    \n",
    "    buffered = io.BytesIO()\n",
    "    img.save(buffered, format=\"JPEG\")\n",
    "    base_64_thumbnail_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "    return f\"<td><img src='data:image/jpeg;base64,{base_64_thumbnail_str}'></td>\"\n",
    "\n",
    "async def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    image_strips = [\n",
    "        await generate_thumbnail(item[\"Image URL\"], item[\"Code\"], df.loc[df['Code'] == item[\"Code\"], 'Name'].iloc[0])\n",
    "        for item in image_url_maps if item\n",
    "    ]\n",
    "    \n",
    "    display(HTML(\"<table><tr>\" + \"\".join(image_strips) + \"</tr></table>\"))\n",
    "    print(\"Total Image Time:\", time.time() - start_time)\n",
    "\n",
    "\n",
    "# Execute the main coroutine directly in a Jupyter notebook cell\n",
    "await main()\n"
   ],
   "id": "159b1435ab12f47",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bd652f581c6c07e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import time\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from urllib.parse import urljoin\n",
    "# from selenium import webdriver\n",
    "# from bs4 import BeautifulSoup\n",
    "# \n",
    "# def extract_reviews(html_content):\n",
    "#     \"\"\"\n",
    "#     Extracts reviews from the given HTML content.\n",
    "# \n",
    "#     Parameters:\n",
    "#     - html_content (str): The HTML content to parse.\n",
    "# \n",
    "#     Returns:\n",
    "#     - List[dict]: A list of dictionaries, each containing the extracted review details.\n",
    "#     \"\"\"\n",
    "#     if not html_content:\n",
    "#         print(\"No HTML content to process.\")\n",
    "#         return []\n",
    "# \n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "#     \n",
    "#     # Extracting star rating\n",
    "#     star_rating_container = soup.find('section', class_='pr-review-snapshot-block-snippet')\n",
    "#     if star_rating_container:\n",
    "#         star_rating = star_rating_container.find('div', class_='pr-snippet-stars')\n",
    "#         star_rating_text = star_rating_container.find('div', class_='pr-snippet-rating-decimal').text.strip()\n",
    "#         if star_rating:\n",
    "#             star_rating_label = star_rating['aria-label']\n",
    "#             print(f\"Star Rating: {star_rating_label}, {star_rating_text}\")\n",
    "#         else:\n",
    "#             print(\"Star Rating not found.\")\n",
    "#     else:\n",
    "#         print(\"Star Rating container not found.\")\n",
    "# \n",
    "#     # Extracting recommendation percentage\n",
    "#     recommendation_section = soup.find('section', class_='pr-review-snapshot-block-recommend')\n",
    "#     if recommendation_section:\n",
    "#         recommendation_percent = recommendation_section.find('span', class_='pr-reco-value').text.strip()\n",
    "#         print(f\"Recommendation Percentage: {recommendation_percent}\")\n",
    "#     else:\n",
    "#         print(\"Recommendation section not found.\")\n",
    "# \n",
    "#     # Extracting reviews\n",
    "#     reviews = soup.find_all('section', class_='pr-rd-content-block')\n",
    "#     reviews_data = []\n",
    "# \n",
    "#     for idx, review in enumerate(reviews, start=1):\n",
    "#         print(f\"\\nProcessing review {idx}:\")\n",
    "# \n",
    "#         # Extracting review text\n",
    "#         review_text = review.find('p', class_='pr-rd-description-text')\n",
    "#         if review_text:\n",
    "#             print(\"Review Text:\", review_text.text.strip())\n",
    "#         else:\n",
    "#             print(\"Review Text not found.\")\n",
    "# \n",
    "#         # Adding review data\n",
    "#         reviews_data.append({\n",
    "#             'Star Rating': star_rating_label if star_rating else None,\n",
    "#             'Rating Text': star_rating_text if star_rating_text else None,\n",
    "#             'Review Text': review_text.text.strip() if review_text else None\n",
    "#         })\n",
    "# \n",
    "#     return reviews_data\n",
    "# \n",
    "# \n",
    "# # Selenium setup\n",
    "# options = Options()\n",
    "# options.add_argument(\"--headless\")\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "# service = Service(ChromeDriverManager().install())\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "# \n",
    "# def get_page_soup(url):\n",
    "#     print(f\"Getting page soup for URL: {url}\")\n",
    "#     driver.get(url)\n",
    "#     time.sleep(1)  # Wait for the page to load\n",
    "#     html = driver.page_source\n",
    "#     return BeautifulSoup(html, 'html.parser')\n",
    "# \n",
    "# def search_product(product_id):\n",
    "#     print(f\"Searching for product ID: {product_id}\")\n",
    "#     search_url = f'https://www.zoro.com/search?q={product_id}'\n",
    "#     soup = get_page_soup(search_url)\n",
    "# \n",
    "#     # Find the specific product link using a precise CSS selector\n",
    "#     for a in soup.select('a.product-card-image__link'):\n",
    "#         print(f\"Checking link: {a['href']}\")\n",
    "#         if product_id.lower() in a['href'].lower():\n",
    "#             product_url = urljoin(search_url, a['href'])\n",
    "#             print(f\"Found product URL: {product_url}\")\n",
    "#             return product_url\n",
    "# \n",
    "#     print(\"Product URL not found.\")\n",
    "#     return None\n",
    "# \n",
    "# def find_reviews_link(product_url):\n",
    "#     print(f\"Finding reviews link on product URL: {product_url}\")\n",
    "#     soup = get_page_soup(product_url)\n",
    "# \n",
    "#     # Find the reviews link using a precise CSS selector\n",
    "#     for a in soup.select('a'):\n",
    "#         print(f\"Checking link: {a['href']}\")\n",
    "#         if 'reviews' in a['href']:\n",
    "#             reviews_url = urljoin(product_url, a['href'])\n",
    "#             print(f\"Found reviews URL: {reviews_url}\")\n",
    "#             return reviews_url\n",
    "# \n",
    "#     print(\"Reviews URL not found.\")\n",
    "#     return None\n",
    "# \n",
    "# def navigate_to_reviews(product_id):\n",
    "#     start_time = time.time()\n",
    "#     print(f\"Navigating to reviews for product ID: {product_id}\")\n",
    "#     product_url = search_product(product_id)\n",
    "#     if product_url:\n",
    "#         reviews_url = find_reviews_link(product_url)\n",
    "#         if reviews_url:\n",
    "#             print(f\"Navigating to reviews URL: {reviews_url}\")\n",
    "#             driver.get(reviews_url)\n",
    "#             time.sleep(1)  # Wait for the reviews page to load\n",
    "#             # Extract page source after page load\n",
    "#             html_content = driver.page_source\n",
    "# \n",
    "#             # Pass HTML content to extract_reviews function\n",
    "#             reviews_data = extract_reviews(html_content)\n",
    "# \n",
    "#             # Print extracted reviews data\n",
    "#             print(\"\\nExtracted Reviews:\")\n",
    "#             for review in reviews_data:\n",
    "#                 print(review)\n",
    "#         else:\n",
    "#             print(\"Reviews URL not found.\")\n",
    "#     else:\n",
    "#         print(\"Product URL not found.\")\n",
    "#     end_time = time.time()  # Record the end time\n",
    "#     total_time_taken = end_time - start_time  # Calculate the total time taken\n",
    "#     print(f\"Total Time Taken: {total_time_taken:.2f} seconds\")  # Display the total time taken\n",
    "# \n",
    "# \n",
    "# \n",
    "# product_id = '1VCE8'\n",
    "# reviews_received = navigate_to_reviews(product_id)\n",
    "# \n",
    "# driver.quit()\n"
   ],
   "id": "fef15e65cb195ede",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from urllib.parse import urljoin\n",
    "# import time\n",
    "# \n",
    "# def extract_reviews(html_content):\n",
    "#     \"\"\"\n",
    "#     Extracts reviews from the given HTML content.\n",
    "# \n",
    "#     Parameters:\n",
    "#     - html_content (str): The HTML content to parse.\n",
    "# \n",
    "#     Returns:\n",
    "#     - List[dict]: A list of dictionaries, each containing the extracted review details.\n",
    "#     \"\"\"\n",
    "#     if not html_content:\n",
    "#         print(\"No HTML content to process.\")\n",
    "#         return []\n",
    "# \n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "#     \n",
    "#     # Initialize an empty list to hold the extracted reviews\n",
    "#     reviews_data = []\n",
    "#     \n",
    "#     # Attempt to extract star rating and recommendation percentage\n",
    "#     try:\n",
    "#         # Star Rating\n",
    "#         star_rating_container = soup.find('section', class_='pr-review-snapshot-block-snippet')\n",
    "#         if star_rating_container:\n",
    "#             star_rating = star_rating_container.find('div', class_='pr-snippet-stars')\n",
    "#             star_rating_text = star_rating_container.find('div', class_='pr-snippet-rating-decimal').text.strip()\n",
    "#             if star_rating:\n",
    "#                 star_rating_label = star_rating['aria-label']\n",
    "#                 print(f\"Star Rating: {star_rating_label}, {star_rating_text}\")\n",
    "#             else:\n",
    "#                 print(\"Star Rating not found.\")\n",
    "#         else:\n",
    "#             print(\"Star Rating container not found.\")\n",
    "#         \n",
    "#         # Recommendation Percentage\n",
    "#         recommendation_section = soup.find('section', class_='pr-review-snapshot-block-recommend')\n",
    "#         if recommendation_section:\n",
    "#             recommendation_percent = recommendation_section.find('span', class_='pr-reco-value').text.strip()\n",
    "#             print(f\"Recommendation Percentage: {recommendation_percent}\")\n",
    "#         else:\n",
    "#             print(\"Recommendation section not found.\")\n",
    "#     except AttributeError:\n",
    "#         print(\"Error parsing review details.\")\n",
    "#     \n",
    "#     # Extracting reviews\n",
    "#     reviews = soup.find_all('section', class_='pr-rd-content-block')\n",
    "#     for idx, review in enumerate(reviews, start=1):\n",
    "#         print(f\"\\nProcessing review {idx}:\")\n",
    "#         \n",
    "#         # Extracting review text\n",
    "#         review_text = review.find('p', class_='pr-rd-description-text')\n",
    "#         if review_text:\n",
    "#             print(\"Review Text:\", review_text.text.strip())\n",
    "#         else:\n",
    "#             print(\"Review Text not found.\")\n",
    "#         \n",
    "#         # Constructing the review data dictionary\n",
    "#         review_data = {\n",
    "#             'Star Rating': star_rating_label if star_rating else None,\n",
    "#             'Rating Text': star_rating_text if star_rating_text else None,\n",
    "#             'Review Text': review_text.text.strip() if review_text else None\n",
    "#         }\n",
    "#         \n",
    "#         # Add the constructed review data to the list\n",
    "#         reviews_data.append(review_data)\n",
    "#     \n",
    "#     return reviews_data\n",
    "# \n",
    "# # Selenium setup\n",
    "# options = Options()\n",
    "# options.add_argument(\"--headless\")\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "# service = Service(ChromeDriverManager().install())\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "# \n",
    "# def get_page_soup(url):\n",
    "#     \"\"\"\n",
    "#     Retrieves the page soup for the given URL using Selenium.\n",
    "# \n",
    "#     Parameters:\n",
    "#     - url (str): The URL to fetch.\n",
    "# \n",
    "#     Returns:\n",
    "#     - BeautifulSoup: The BeautifulSoup object of the page content.\n",
    "#     \"\"\"\n",
    "#     print(f\"Getting page soup for URL: {url}\")\n",
    "#     driver.get(url)\n",
    "#     time.sleep(1)  # Wait for the page to load\n",
    "#     html = driver.page_source\n",
    "#     return BeautifulSoup(html, 'html.parser')\n",
    "# \n",
    "# def search_product(product_id):\n",
    "#     \"\"\"\n",
    "#     Searches for the product using the product ID and returns the product URL.\n",
    "# \n",
    "#     Parameters:\n",
    "#     - product_id (str): The product ID to search for.\n",
    "# \n",
    "#     Returns:\n",
    "#     - str: The URL of the product.\n",
    "#     \"\"\"\n",
    "#     print(f\"Searching for product ID: {product_id}\")\n",
    "#     search_url = f'https://www.zoro.com/search?q={product_id}'\n",
    "#     soup = get_page_soup(search_url)\n",
    "# \n",
    "#     # Find the specific product link using a precise CSS selector\n",
    "#     for a in soup.select('a.product-card-image__link'):\n",
    "#         print(f\"Checking link: {a['href']}\")\n",
    "#         if product_id.lower() in a['href'].lower():\n",
    "#             product_url = urljoin(search_url, a['href'])\n",
    "#             print(f\"Found product URL: {product_url}\")\n",
    "#             return product_url\n",
    "# \n",
    "#     print(\"Product URL not found.\")\n",
    "#     return None\n",
    "# \n",
    "# def find_reviews_link(product_url):\n",
    "#     \"\"\"\n",
    "#     Finds the reviews link on the product URL.\n",
    "# \n",
    "#     Parameters:\n",
    "#     - product_url (str): The URL of the product.\n",
    "# \n",
    "#     Returns:\n",
    "#     - str: The URL of the reviews page.\n",
    "#     \"\"\"\n",
    "#     print(f\"Finding reviews link on product URL: {product_url}\")\n",
    "#     soup = get_page_soup(product_url)\n",
    "# \n",
    "#     # Find the reviews link using a precise CSS selector\n",
    "#     for a in soup.select('a'):\n",
    "#         print(f\"Checking link: {a['href']}\")\n",
    "#         if 'reviews' in a['href']:\n",
    "#             reviews_url = urljoin(product_url, a['href'])\n",
    "#             print(f\"Found reviews URL: {reviews_url}\")\n",
    "#             return reviews_url\n",
    "# \n",
    "#     print(\"Reviews URL not found.\")\n",
    "#     return None\n",
    "# \n",
    "# def navigate_to_reviews(product_id):\n",
    "#     \"\"\"\n",
    "#     Navigates to the reviews page of the product and extracts reviews.\n",
    "# \n",
    "#     Parameters:\n",
    "#     - product_id (str): The product ID to search for.\n",
    "# \n",
    "#     Returns:\n",
    "#     - List[dict]: A list of dictionaries containing the reviews.\n",
    "#     \"\"\"\n",
    "#     start_time = time.time()\n",
    "#     print(f\"Navigating to reviews for product ID: {product_id}\")\n",
    "#     product_url = search_product(product_id)\n",
    "#     if product_url:\n",
    "#         reviews_url = find_reviews_link(product_url)\n",
    "#         if reviews_url:\n",
    "#             print(f\"Navigating to reviews URL: {reviews_url}\")\n",
    "#             driver.get(reviews_url)\n",
    "#             time.sleep(1)  # Wait for the reviews page to load\n",
    "#             # Extract page source after page load\n",
    "#             html_content = driver.page_source\n",
    "# \n",
    "#             # Pass HTML content to extract_reviews function\n",
    "#             reviews_data = extract_reviews(html_content)\n",
    "#             print(\"reviews_data = \", reviews_data)    \n",
    "#             # Print extracted reviews data\n",
    "#             print(\"\\nExtracted Reviews:\")\n",
    "#             for review in reviews_data:\n",
    "#                 print(review)\n",
    "#             end_time = time.time()  # Record the end time\n",
    "#             total_time_taken = end_time - start_time  # Calculate the total time taken\n",
    "#             print(f\"Total Time Taken: {total_time_taken:.2f} seconds\")     \n",
    "#             return reviews_data    \n",
    "#         else:\n",
    "#             print(\"Reviews URL not found.\")\n",
    "#     else:\n",
    "#         print(\"Product URL not found.\")\n",
    "#     end_time = time.time()  # Record the end time\n",
    "#     total_time_taken = end_time - start_time  # Calculate the total time taken\n",
    "#     print(f\"Total Time Taken: {total_time_taken:.2f} seconds\")  # Display the total time taken\n",
    "# \n",
    "# # Example product ID to fetch reviews for\n",
    "# product_id = '1VCE8'\n",
    "# reviews_received = navigate_to_reviews(product_id)\n",
    "# driver.quit()\n"
   ],
   "id": "be94a2790d47ebe0",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import aiohttp\n",
    "# import asyncio\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urljoin\n",
    "# \n",
    "# async def fetch_html(session, url):\n",
    "#     \"\"\"\n",
    "#     Fetches the HTML content from the given URL asynchronously.\n",
    "# \n",
    "#     Parameters:\n",
    "#     - session (ClientSession): The aiohttp client session.\n",
    "#     - url (str): The URL to fetch.\n",
    "# \n",
    "#     Returns:\n",
    "#     - str: The HTML content of the page.\n",
    "#     \"\"\"\n",
    "#     async with session.get(url) as response:\n",
    "#         return await response.text()\n",
    "# \n",
    "# async def get_page_soup(session, url):\n",
    "#     \"\"\"\n",
    "#     Retrieves the page soup for the given URL using aiohttp.\n",
    "# \n",
    "#     Parameters:\n",
    "#     - session (ClientSession): The aiohttp client session.\n",
    "#     - url (str): The URL to fetch.\n",
    "# \n",
    "#     Returns:\n",
    "#     - BeautifulSoup: The BeautifulSoup object of the page content.\n",
    "#     \"\"\"\n",
    "#     print(f\"Getting page soup for URL: {url}\")\n",
    "#     html = await fetch_html(session, url)\n",
    "#     return BeautifulSoup(html, 'html.parser')\n",
    "# \n",
    "# async def search_product(session, product_id):\n",
    "#     \"\"\"\n",
    "#     Searches for the product using the product ID and returns the product URL.\n",
    "# \n",
    "#     Parameters:\n",
    "#     - session (ClientSession): The aiohttp client session.\n",
    "#     - product_id (str): The product ID to search for.\n",
    "# \n",
    "#     Returns:\n",
    "#     - str: The URL of the product.\n",
    "#     \"\"\"\n",
    "#     print(f\"Searching for product ID: {product_id}\")\n",
    "#     search_url = f'https://www.zoro.com/search?q={product_id}'\n",
    "#     soup = await get_page_soup(session, search_url)\n",
    "# \n",
    "#     for a in soup.select('a.product-card-image__link'):\n",
    "#         print(f\"Checking link: {a['href']}\")\n",
    "#         if product_id.lower() in a['href'].lower():\n",
    "#             product_url = urljoin(search_url, a['href'])\n",
    "#             print(f\"Found product URL: {product_url}\")\n",
    "#             return product_url\n",
    "# \n",
    "#     print(\"Product URL not found.\")\n",
    "#     return None\n",
    "# \n",
    "# async def find_reviews_link(session, product_url):\n",
    "#     \"\"\"\n",
    "#     Finds the reviews link on the product URL.\n",
    "# \n",
    "#     Parameters:\n",
    "#     - session (ClientSession): The aiohttp client session.\n",
    "#     - product_url (str): The URL of the product.\n",
    "# \n",
    "#     Returns:\n",
    "#     - str: The URL of the reviews page.\n",
    "#     \"\"\"\n",
    "#     print(f\"Finding reviews link on product URL: {product_url}\")\n",
    "#     soup = await get_page_soup(session, product_url)\n",
    "# \n",
    "#     for a in soup.select('a'):\n",
    "#         print(f\"Checking link: {a['href']}\")\n",
    "#         if 'reviews' in a['href']:\n",
    "#             reviews_url = urljoin(product_url, a['href'])\n",
    "#             print(f\"Found reviews URL: {reviews_url}\")\n",
    "#             return reviews_url\n",
    "# \n",
    "#     print(\"Reviews URL not found.\")\n",
    "#     return None\n",
    "# \n",
    "# async def extract_reviews(html_content):\n",
    "#     \"\"\"\n",
    "#     Extracts reviews from the given HTML content.\n",
    "# \n",
    "#     Parameters:\n",
    "#     - html_content (str): The HTML content to parse.\n",
    "# \n",
    "#     Returns:\n",
    "#     - List[dict]: A list of dictionaries, each containing the extracted review details.\n",
    "#     \"\"\"\n",
    "#     if not html_content:\n",
    "#         print(\"No HTML content to process.\")\n",
    "#         return []\n",
    "# \n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "# \n",
    "#     reviews_data = []\n",
    "# \n",
    "#     try:\n",
    "#         star_rating_container = soup.find('section', class_='pr-review-snapshot-block-snippet')\n",
    "#         if star_rating_container:\n",
    "#             star_rating = star_rating_container.find('div', class_='pr-snippet-stars')\n",
    "#             star_rating_text = star_rating_container.find('div', class_='pr-snippet-rating-decimal').text.strip()\n",
    "#             if star_rating:\n",
    "#                 star_rating_label = star_rating['aria-label']\n",
    "#                 print(f\"Star Rating: {star_rating_label}, {star_rating_text}\")\n",
    "#             else:\n",
    "#                 print(\"Star Rating not found.\")\n",
    "#         else:\n",
    "#             print(\"Star Rating container not found.\")\n",
    "# \n",
    "#         recommendation_section = soup.find('section', class_='pr-review-snapshot-block-recommend')\n",
    "#         if recommendation_section:\n",
    "#             recommendation_percent = recommendation_section.find('span', class_='pr-reco-value').text.strip()\n",
    "#             print(f\"Recommendation Percentage: {recommendation_percent}\")\n",
    "#         else:\n",
    "#             print(\"Recommendation section not found.\")\n",
    "#     except AttributeError:\n",
    "#         print(\"Error parsing review details.\")\n",
    "# \n",
    "#     reviews = soup.find_all('section', class_='pr-rd-content-block')\n",
    "#     for idx, review in enumerate(reviews, start=1):\n",
    "#         print(f\"\\nProcessing review {idx}:\")\n",
    "# \n",
    "#         review_text = review.find('p', class_='pr-rd-description-text')\n",
    "#         if review_text:\n",
    "#             print(\"Review Text:\", review_text.text.strip())\n",
    "#         else:\n",
    "#             print(\"Review Text not found.\")\n",
    "# \n",
    "#         review_data = {\n",
    "#             'Star Rating': star_rating_label if star_rating else None,\n",
    "#             'Rating Text': star_rating_text if star_rating_text else None,\n",
    "#             'Review Text': review_text.text.strip() if review_text else None\n",
    "#         }\n",
    "# \n",
    "#         reviews_data.append(review_data)\n",
    "# \n",
    "#     return reviews_data\n",
    "# \n",
    "# async def navigate_to_reviews(product_id):\n",
    "#     \"\"\"\n",
    "#     Navigates to the reviews page of the product and extracts reviews.\n",
    "# \n",
    "#     Parameters:\n",
    "#     - product_id (str): The product ID to search for.\n",
    "# \n",
    "#     Returns:\n",
    "#     - List[dict]: A list of dictionaries containing the reviews.\n",
    "#     \"\"\"\n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         start_time = time.time()\n",
    "#         print(f\"Navigating to reviews for product ID: {product_id}\")\n",
    "#         product_url = await search_product(session, product_id)\n",
    "#         if product_url:\n",
    "#             reviews_url = await find_reviews_link(session, product_url)\n",
    "#             if reviews_url:\n",
    "#                 print(f\"Navigating to reviews URL: {reviews_url}\")\n",
    "#                 html_content = await fetch_html(session, reviews_url)\n",
    "#                 reviews_data = await extract_reviews(html_content)\n",
    "#                 \n",
    "#                 print(\"\\nExtracted Reviews:\")\n",
    "#                 for review in reviews_data:\n",
    "#                     print(review)\n",
    "#                 \n",
    "#                 return reviews_data\n",
    "#             else:\n",
    "#                 print(\"Reviews URL not found.\")\n",
    "#         else:\n",
    "#             print(\"Product URL not found.\")\n",
    "# \n",
    "#         end_time = time.time()\n",
    "#         total_time_taken = end_time - start_time\n",
    "#         print(f\"Total Time Taken: {total_time_taken:.2f} seconds\")\n",
    "# \n",
    "# # Example product ID to fetch reviews for\n",
    "# product_id = '1VCE8'\n",
    "# \n",
    "# # Run the asynchronous function\n",
    "# await navigate_to_reviews(product_id)\n"
   ],
   "id": "3fb5f8e05c0fbe5d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # To enable nested event loops for Jupyter notebooks\n",
    "\n",
    "def extract_reviews(html_content):\n",
    "    \"\"\"\n",
    "    Extracts reviews from the given HTML content.\n",
    "\n",
    "    Parameters:\n",
    "    - html_content (str): The HTML content to parse.\n",
    "\n",
    "    Returns:\n",
    "    - List[dict]: A list of dictionaries, each containing the extracted review details.\n",
    "    \"\"\"\n",
    "    if not html_content:\n",
    "        print(\"No HTML content to process.\")\n",
    "        return None  # Return None when no HTML content is provided\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Initialize an empty list to hold the extracted reviews\n",
    "    reviews_data = []\n",
    "\n",
    "    # Attempt to extract star rating and recommendation percentage\n",
    "    try:\n",
    "        # Star Rating\n",
    "        star_rating_container = soup.find('section', class_='pr-review-snapshot-block-snippet')\n",
    "        if star_rating_container:\n",
    "            star_rating = star_rating_container.find('div', class_='pr-snippet-stars')\n",
    "            star_rating_text = star_rating_container.find('div', class_='pr-snippet-rating-decimal').text.strip()\n",
    "            if star_rating:\n",
    "                star_rating_label = star_rating['aria-label']\n",
    "                print(f\"Star Rating: {star_rating_label}, {star_rating_text}\")\n",
    "            else:\n",
    "                print(\"Star Rating not found.\")\n",
    "        else:\n",
    "            print(\"Star Rating container not found.\")\n",
    "\n",
    "        # Recommendation Percentage\n",
    "        recommendation_section = soup.find('section', class_='pr-review-snapshot-block-recommend')\n",
    "        if recommendation_section:\n",
    "            recommendation_percent = recommendation_section.find('span', class_='pr-reco-value').text.strip()\n",
    "            print(f\"Recommendation Percentage: {recommendation_percent}\")\n",
    "        else:\n",
    "            print(\"Recommendation section not found.\")\n",
    "    except AttributeError:\n",
    "        print(\"Error parsing review details.\")\n",
    "\n",
    "    # Extracting reviews\n",
    "    reviews = soup.find_all('section', class_='pr-rd-content-block')\n",
    "    if reviews:\n",
    "        for idx, review in enumerate(reviews, start=1):\n",
    "            print(f\"\\nProcessing review {idx}:\")\n",
    "            # Extracting review text\n",
    "            review_text = review.find('p', class_='pr-rd-description-text')\n",
    "            if review_text:\n",
    "                print(\"Review Text:\", review_text.text.strip())\n",
    "            else:\n",
    "                print(\"Review Text not found.\")\n",
    "\n",
    "            # Constructing the review data dictionary\n",
    "            review_data = {\n",
    "                'Star Rating': star_rating_label if star_rating else None,\n",
    "                'Rating Text': star_rating_text if star_rating_text else None,\n",
    "                'Review Text': review_text.text.strip() if review_text else None\n",
    "            }\n",
    "\n",
    "            # Add the constructed review data to the list\n",
    "            reviews_data.append(review_data)\n",
    "\n",
    "    else:\n",
    "        print(\"No reviews found on the page.\")\n",
    "\n",
    "    return reviews_data if reviews_data else None  # Return None if no reviews were extracted\n",
    "\n",
    "\n",
    "# Define the synchronous functions\n",
    "def get_page_soup(url, driver):\n",
    "    print(f\"Getting page soup for URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Wait for the page to load\n",
    "    html = driver.page_source\n",
    "    return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "def search_product(product_id, driver):\n",
    "    print(f\"Searching for product ID: {product_id}\")\n",
    "    search_url = f'https://www.zoro.com/search?q={product_id}'\n",
    "    soup = get_page_soup(search_url, driver)\n",
    "\n",
    "    for a in soup.select('a.product-card-image__link'):\n",
    "        print(f\"Checking link: {a['href']}\")\n",
    "        if product_id.lower() in a['href'].lower():\n",
    "            product_url = urljoin(search_url, a['href'])\n",
    "            print(f\"Found product URL: {product_url}\")\n",
    "            return product_url\n",
    "\n",
    "    print(\"Product URL not found.\")\n",
    "    return None\n",
    "\n",
    "def find_reviews_link(product_url, driver):\n",
    "    print(f\"Finding reviews link on product URL: {product_url}\")\n",
    "    soup = get_page_soup(product_url, driver)\n",
    "\n",
    "    for a in soup.select('a'):\n",
    "        print(f\"Checking link: {a['href']}\")\n",
    "        if 'reviews' in a['href']:\n",
    "            reviews_url = urljoin(product_url, a['href'])\n",
    "            print(f\"Found reviews URL: {reviews_url}\")\n",
    "            return reviews_url\n",
    "\n",
    "    print(\"Reviews URL not found.\")\n",
    "    return None\n",
    "\n",
    "# Define the asynchronous function\n",
    "async def navigate_to_reviews(product_id):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(f\"Navigating to reviews for product ID: {product_id}\")\n",
    "    product_url = search_product(product_id, driver)\n",
    "    if product_url:\n",
    "        reviews_url = find_reviews_link(product_url, driver)\n",
    "        if reviews_url:\n",
    "            print(f\"Navigating to reviews URL: {reviews_url}\")\n",
    "            driver.get(reviews_url)\n",
    "            await asyncio.sleep(1)  # Wait for the reviews page to load\n",
    "            html_content = driver.page_source\n",
    "            reviews_data = extract_reviews(html_content)\n",
    "\n",
    "            print(\"\\nExtracted Reviews:\")\n",
    "            for review in reviews_data:\n",
    "                print(review)\n",
    "\n",
    "            driver.quit()\n",
    "            end_time = time.time()\n",
    "            total_time_taken = end_time - start_time\n",
    "            print(f\"Total Time Taken: {total_time_taken:.2f} seconds\")\n",
    "            return reviews_data\n",
    "        else:\n",
    "            print(\"Reviews URL not found.\")\n",
    "    else:\n",
    "        print(\"Product URL not found.\")\n",
    "\n",
    "    driver.quit()\n",
    "    end_time = time.time()\n",
    "    total_time_taken = end_time - start_time\n",
    "    print(f\"Total Time Taken: {total_time_taken:.2f} seconds\")\n",
    "    return None\n",
    "\n",
    "async def fetch_reviews_for_product_ids(product_ids):\n",
    "    tasks = [navigate_to_reviews(product_id) for product_id in product_ids]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    for product_id, result in zip(product_ids, results):\n",
    "        print(f\"Reviews for {product_id}: {result}\")\n",
    "\n",
    "# # Example list of product IDs\n",
    "# product_ids = ['1VCE8', '2ABC9', '3XYZ0', '4LMN1', '5PQR2']\n",
    "# \n",
    "# # Fetch reviews concurrently\n",
    "# await fetch_reviews_for_product_ids(product_ids)\n"
   ],
   "id": "7af7cd55515d58c2",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "async def extract_product_codes(image_url_maps):\n",
    "    \"\"\"\n",
    "    Extracts product codes from a list of image URL maps.\n",
    "\n",
    "    Parameters:\n",
    "    - image_url_maps (list): A list of dictionaries containing 'Code' and 'Image URL' keys.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of product codes.\n",
    "    \"\"\"\n",
    "    product_codes = []\n",
    "    for image_url_map in image_url_maps:\n",
    "        product_code = image_url_map['Code']\n",
    "        product_codes.append(product_code)\n",
    "    return product_codes\n"
   ],
   "id": "3c6b0cebc6dfdf5c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import concurrent.futures\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "async def fetch_reviews_for_product_ids(product_ids):\n",
    "    tasks = [navigate_to_reviews(product_id) for product_id in product_ids]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    for product_id, result in zip(product_ids, results):\n",
    "        print(f\"Reviews for {product_id}: {result}\")\n",
    "\n",
    "# # Example list of product IDs\n",
    "# product_ids = ['1VCE8', '2ABC9', '3XYZ0', '4LMN1', '5PQR2']\n",
    "# \n",
    "# # Fetch reviews concurrently\n",
    "# await fetch_reviews_for_product_ids(product_ids)"
   ],
   "id": "68bd5a9d75f091ac",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "async def fetch_reviews_for_product_id(product_id: str) -> Optional[dict]:\n",
    "    # Your existing code to fetch reviews for a single product ID\n",
    "    # Assuming it returns a dictionary with review details or None if no reviews found\n",
    "    reviews = await navigate_to_reviews(product_id)\n",
    "    return reviews\n",
    "\n",
    "async def generate_product_reviews(image_url_maps: List[dict]) -> Optional[dict]:\n",
    "    reviews_dict = {}\n",
    "\n",
    "    # Loop through each item in image_url_maps\n",
    "    for item in image_url_maps:\n",
    "        product_id = item['Code']\n",
    "        reviews = await fetch_reviews_for_product_id(product_id)\n",
    "        reviews_dict[product_id] = reviews\n",
    "\n",
    "    return reviews_dict\n",
    "\n"
   ],
   "id": "2dabe8ca05a2b8a0",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ## GET LIST OF PRODUCTS AND CODES\n",
    "# from langchain.chains import RetrievalQA\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# prompt_template_with_attributes = \"\"\"\n",
    "# Human: Extract list of 5 products and their respective physical IDs from catalog that answer the user question. \n",
    "# The catalog of products is provided under <catalog></catalog> tags below.\n",
    "# <catalog>\n",
    "# {context}\n",
    "# </catalog>\n",
    "# Customer Attributes: {customer_attributes}\n",
    "# Question: {question}\n",
    "# \n",
    "# The output should be a json of the form <products>[{{\"product\": <description of the product from the catalog>, \"code\":<code of the product from the catalog>}},...]</products>\n",
    "# Skip the preamble and always return valid json.\n",
    "# Assistant: \"\"\"\n",
    "# \n",
    "# PROMPT_WITH_ATTRIBUTES = PromptTemplate(\n",
    "#     template=prompt_template_with_attributes, input_variables=[\"context\", \"question\", \"customer_attributes\"]\n",
    "# )\n",
    "# \n",
    "# # Use RetrievalQA customizations for improving Q&A experience\n",
    "# qa = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=vectorstore_faiss_doc.as_retriever(\n",
    "#         search_type=\"similarity\", search_kwargs={\"k\": 6}\n",
    "#     ),\n",
    "#     return_source_documents=False,\n",
    "#     chain_type_kwargs={\"prompt\": PROMPT},\n",
    "# )\n",
    "# \n",
    "# recs_response = qa({\"query\": customer_input})['result']\n",
    "# recs_response"
   ],
   "id": "d4995700cd46a337",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# image_url_maps[0]['Code'] = '1VCE8'\n",
    "reviews_dict = await generate_product_reviews(image_url_maps)\n",
    "print(\"THE REVIEW DICT: \", reviews_dict)"
   ],
   "id": "7c4351b858d3917e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Enhance user experience with Chatbot\n",
    "\n",
    "#### Generating detailed overview based on customer reviews of products sister company Zoro when the same product is sold.\n",
    "LangChain's [ConversationBufferMemory](https://python.langchain.com/docs/use_cases/question_answering/chat_vector_db) class provides a way to capture conversational memory for LLM chat applications. We will have Claude being able to retrieve context through conversational memory using the prompt template. Note that this time our prompt template includes a {chat_history} variable where our chat history will be included to the prompt.\n",
    "\n",
    "The prompt template has both conversation memory as well as chat history as inputs along with the human input. Notice how the prompt also instructs Claude to not answer questions which it does not have the context for. This helps reduce hallucinations which is extremely important when creating end user facing applications which need to be factual.\n",
    "\n",
    "\n",
    "![Architecture](./images/chatbot_products.png)"
   ],
   "id": "07e0443f-487f-4c49-87c2-ab581be8535f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "581a0ca80ccaaf85",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # chat_prompt1 = \"Show me specific reviews that talk about the quality of the products.\"\n",
    "# # chat_prompt2 = \"What do people like about the products?\"\n",
    "# \n",
    "# from langchain.chains import ConversationalRetrievalChain\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "# from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "# \n",
    "# chat_history = [\" \"]\n",
    "# memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "# conversation = ConversationalRetrievalChain.from_llm(\n",
    "#     llm=llm, \n",
    "#     retriever=vectorstore_faiss_doc.as_retriever(), \n",
    "#     memory=memory_chain,\n",
    "#     condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "#     #verbose=True, \n",
    "#     chain_type='stuff', # 'refine',\n",
    "#     #max_tokens_limit=300\n",
    "# )\n",
    "# \n",
    "# # Generate detailed reviews based on customer reviews of specific clothing in product catalog\n",
    "# # \n",
    "# # try:\n",
    "# #     chat_history = []\n",
    "# #     print(\"********************chat_history\", chat_history)\n",
    "# #     chat_res1 = conversation.run({'question': chat_prompt1, 'chat_history': chat_history })\n",
    "# #     print_ww(chat_res1)\n",
    "# #     chat_history.append([chat_prompt1, chat_res1])\n",
    "# #     if response_json:\n",
    "# #         # print(\"**********response_json['products']\", response_json['products'])\n",
    "# #         chat_history.append(response_json['products'])\n",
    "# #         # print(\"********************chat_history\", chat_history)\n",
    "# #     if reviews_dict:\n",
    "# #         # print(\"**********reviews_dict\", reviews_dict)\n",
    "# #         chat_history.append(reviews_dict)\n",
    "# #         # print(\"********************chat_history\", chat_history)\n",
    "# # except ValueError as error:\n",
    "# #     if  \"AccessDeniedException\" in str(error):\n",
    "# #         class StopExecution(ValueError):\n",
    "# #             def _render_traceback_(self):\n",
    "# #                 pass\n",
    "# #         raise StopExecution        \n",
    "# #     else:\n",
    "# #         raise error"
   ],
   "id": "255e05cd-da0e-4649-95e9-59e2cc9358dc",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from langchain.chains.chat_vector_db.prompts import QA_PROMPT\n",
    "# \n",
    "# \n",
    "# async def process_chat_question(question, clear_history=False):\n",
    "#     # llm=llm\n",
    "#     # retriever=vectorstore_faiss_doc.as_retriever()\n",
    "#     # memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "#     try:\n",
    "#         if clear_history:\n",
    "#             chat_history.clear()  # Clear chat history if specified\n",
    "# \n",
    "#         context = {\n",
    "#             'question': question,\n",
    "#             'chat_history': chat_history\n",
    "#         }\n",
    "# \n",
    "#         # Run conversation with provided context\n",
    "#         chat_res = await conversation.run(**context)\n",
    "#         print_ww(chat_res)  # Assuming print_ww is defined elsewhere for specific logging/printing\n",
    "# \n",
    "#         # Append the chat prompt and result to history\n",
    "#         chat_history.append([question, chat_res])\n",
    "# \n",
    "#         # Optionally add response_json['products'] and reviews_dict to chat history\n",
    "#         if response_json:\n",
    "#             chat_history.append(response_json['products'])\n",
    "# \n",
    "#         if reviews_dict:\n",
    "#             chat_history.append(reviews_dict)\n",
    "# \n",
    "#         return str(chat_res)  # Return chat response as a string\n",
    "# \n",
    "#     except ValueError as error:\n",
    "#         if \"AccessDeniedException\" in str(error):\n",
    "#             class StopExecution(ValueError):\n",
    "#                 def _render_traceback_(self):\n",
    "#                     pass\n",
    "#             raise StopExecution\n",
    "#         else:\n",
    "#             raise error\n"
   ],
   "id": "305db2d93dfe0089",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# question=\"what are the products you have recommended so far?\"\n",
    "# process_chat_question(question)"
   ],
   "id": "2b34da3f08fd6a6f",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# HERE IS THE CONVERSATION\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "chat_history = [\" \"]\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    retriever=vectorstore_faiss_doc.as_retriever(), \n",
    "    memory=memory_chain,\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    chain_type='stuff',  # 'refine',\n",
    ")\n",
    "\n",
    "# Define a function to process the chat question\n",
    "def process_chat_question(question, clear_history=False):\n",
    "    try:\n",
    "        if clear_history:\n",
    "            chat_history.clear()  # Clear chat history if specified\n",
    "\n",
    "        context = {\n",
    "            'question': question,\n",
    "            'chat_history': chat_history\n",
    "        }\n",
    "\n",
    "        # Run conversation with provided context synchronously\n",
    "        chat_res = conversation.run(**context)\n",
    "\n",
    "        # Append the chat prompt and result to history\n",
    "        chat_history.append([question, chat_res])\n",
    "\n",
    "        # Optionally add response_json['products'] and reviews_dict to chat history\n",
    "        if response_json:\n",
    "            chat_history.append(response_json['products'])\n",
    "\n",
    "        if reviews_dict:\n",
    "            chat_history.append(reviews_dict)\n",
    "\n",
    "        return str(chat_res)  # Return chat response as a string\n",
    "\n",
    "    except ValueError as error:\n",
    "        if \"AccessDeniedException\" in str(error):\n",
    "            class StopExecution(ValueError):\n",
    "                def _render_traceback_(self):\n",
    "                    pass\n",
    "            raise StopExecution\n",
    "        else:\n",
    "            raise error\n",
    "\n",
    "# Example usage:\n",
    "question = (\"I am looking for information about product 1VCE8\")\n",
    "response = process_chat_question(question, clear_history=True)  # Specify clear_history as needed\n",
    "print(response) \n"
   ],
   "id": "8012f28e803a70f5",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " \n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "chat_history = [\" \"]\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    retriever=vectorstore_faiss_doc.as_retriever(), \n",
    "    memory=memory_chain,\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    #verbose=True, \n",
    "    chain_type='stuff', # 'refine',\n",
    "    #max_tokens_limit=300\n",
    ")"
   ],
   "id": "41ec5c10f5adc53b",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "28cf8288-acbf-464e-b202-174755e7568d",
   "metadata": {},
   "source": [
    "chat_prompt2 = \"What do people like about those product?\"\n",
    "try:\n",
    "    print(\"Chat History: \" , chat_history, \"\\n\")\n",
    "    chat_res2 = conversation.run({'question': chat_prompt2 + \"Do Not Answer if embeddings does not return anything.\", 'chat_history': chat_history }) # + \" Answer even if embeddings does not return anything.\"\n",
    "    print_ww(\"chat_res2: \", chat_res2)\n",
    "    chat_history.append([chat_prompt2, chat_res2])\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "18c0a1cb-72aa-4d26-bc17-d5846a1e4188",
   "metadata": {},
   "source": [
    "#### Customer order history semantic searches\n",
    "\n",
    "Generating size and color recommendations based on customer's order history. This will help to provide curated content to the customer"
   ]
  },
  {
   "cell_type": "code",
   "id": "45f4710f-c185-4356-a850-f2c051cf1837",
   "metadata": {},
   "source": [
    "#THIS IS NOT SOMETHING THAT CAN BE EMULATED UNTIL THE USER LOGIN IS SET UP AND THE FUNCTION IS NOT YET DEFINED AS THIS IS NOT NECESSARILY RELEVANT TO GRIANGER ORDERS.\n",
    "\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss_doc)\n",
    "query_embedding = bedrock_embeddings.embed_query(customer_input)\n",
    "np.array(query_embedding)\n",
    "\n",
    "chat_prompt3 = \"What size and color should I wear?\"\n",
    "chat_res3 = wrapper_store_faiss.query(question= chat_prompt3 + \" based on order history for customer with id \" + customer_id, llm=llm)\n",
    "print_ww(chat_res3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "13b358e8-fa6b-4cbf-b05b-ea985dcfa260",
   "metadata": {},
   "source": [
    "## Showing final products based on customer style selection \n",
    "\n",
    "Continuing on our architectural pattern we will change the prompt template and leverage the LLM to generate the `recommended` products based on the user selection and weather and other details. The key extraction entities will be \n",
    "\n",
    "1. Leverage the customer initial prompt to generate the relevant ids\n",
    "2. Extract the relevant products from the vector store\n",
    "3. Physical ID for the products needed\n",
    "\n",
    "\n",
    "\n",
    "![Architecture](./images/other_products.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "2bd18a21-5519-4543-8d8f-98ddab109b78",
   "metadata": {},
   "source": [
    "selected_style = recommendations[3]\n",
    "print(selected_style)\n",
    "\n",
    "selected_style = recommendations[0]\n",
    "print(selected_style)\n",
    "\n",
    "# Fetch the response from qa2\n",
    "try:\n",
    "    print(\" GETTING THE RESPONSE: \"),\n",
    "    response = qa({chat_res2: selected_style})\n",
    "    print(\" THIS IS THE RESPONSE: \", response)\n",
    "except AttributeError as e:\n",
    "    print(\"AttributeError:\", e)\n",
    "    print(\"qa2 configuration:\", chat_res2)\n",
    "    print(\"qa2 retriever:\", chat_res2.retriever)\n",
    "    print(\"Input type:\", type(selected_style))\n",
    "    \n",
    "# Print the entire response to see its structure\n",
    "print(response)\n",
    "\n",
    "# Check if 'result' is in the response\n",
    "if 'result' in response:\n",
    "    cart_items = response['result']\n",
    "    print_ww(cart_items)\n",
    "else:\n",
    "    print(\"Error: 'result' key not found in the response. Response content:\")\n",
    "    print(response)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e2343e50",
   "metadata": {},
   "source": [
    "products = json.loads(re.findall('<products>(.*?)</products>', cart_items, re.DOTALL)[0])\n",
    "products"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "793cd743-d2d6-4084-8113-68aa2fba8759",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "from IPython import display\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "cart_item_strip = \"\"\n",
    "for product in products:\n",
    "    url = \"https://sagemaker-example-files-prod-us-east-1.s3.us-east-1.amazonaws.com/datasets/image/howser-bedrock/data/aistylist/images/products/\" + urllib.parse.quote(product['physical_id'].strip(), safe='', encoding=None, errors=None) + \".jpg\"\n",
    "    # im = Image.open(requests.get(url, stream=True).raw)\n",
    "    cart_item_strip += \"<td><img src='\"+ url + \"'></td>\"\n",
    "display.display(display.HTML(\"<table><tr>\" + cart_item_strip +\"</tr></table>\"))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4a073284-c491-4265-82a5-c48db9ea7a93",
   "metadata": {},
   "source": [
    "## Integrating DIY Agents to associate external APIs and databases\n",
    "### Using ReAct: Synergizing Reasoning and Acting in Language Models Framework\n",
    "Large language models can generate both explanations for their reasoning and task-specific responses in an alternating fashion. \n",
    "\n",
    "Producing reasoning explanations enables the model to infer, monitor, and revise action plans, and even handle unexpected scenarios. The action step allows the model to interface with and obtain information from external sources such as knowledge bases or environments.\n",
    "\n",
    "The ReAct framework could enable large language models to interact with external tools to obtain additional information that results in more accurate and fact-based responses. Here we will leverage the user prompt and perform the following actions\n",
    "1. Extract the city \n",
    "2. Get weather information\n",
    "3. Search our product catalog using semantic search to find relevant products\n",
    "4. Display the products for user to add to cart\n",
    "\n",
    "![Architecture](./images/weather.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f33d6499-8a26-446f-9c6d-b518dc7edcfd",
   "metadata": {},
   "source": [
    "import os\n",
    "import python_weather\n",
    "\n",
    "async def getweather(city):\n",
    "  # declare the client. the measuring unit used defaults to the metric system (celcius, km/h, etc.)\n",
    "  async with python_weather.Client(unit=python_weather.IMPERIAL) as client:\n",
    "    # fetch a weather forecast from a city\n",
    "    weather = await client.get(city)\n",
    "    \n",
    "    # returns the current day's forecast temperature (int)\n",
    "    return weather.current"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "122aacf9-84e2-4aba-968a-6a07f6f98966",
   "metadata": {},
   "source": [
    "## Accessory recommendations \n",
    "\n",
    "We will provide  accessory recommendations based on location provided in customer input"
   ]
  },
  {
   "cell_type": "code",
   "id": "004c8b61",
   "metadata": {},
   "source": [
    "await getweather(entity_extraction_result[2])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ad2eae6f",
   "metadata": {},
   "source": [
    "if accessory_response:\n",
    "    accessories = re.findall('<style>(.*?)</style>', accessory_response)\n",
    "    accessories_items = qa2({\"query\": ', '.join(accessories)})['result']\n",
    "    accessories_items = json.loads(re.findall('<products>(.*?)</products>', accessories_items, re.DOTALL)[0])\n",
    "    accessory_strip = \"\"\n",
    "    for accessory in accessories_items:\n",
    "        url = \"https://sagemaker-example-files-prod-us-east-1.s3.us-east-1.amazonaws.com/datasets/image/howser-bedrock/data/aistylist/images/products/\" + urllib.parse.quote(accessory['physical_id'].strip(), safe='', encoding=None, errors=None) + \".jpg\"\n",
    "        accessory_strip += \"<td><img src='\"+ url + \"'></td>\"\n",
    "    display.display(display.HTML(\"<table><tr>\" + accessory_strip +\"</tr></table>\"))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "accessory_response = None\n",
    "if attributes[\"location\"]:\n",
    "    current_weather = await getweather(entity_extraction_result[2])\n",
    "    accessory_input = \"Suggest list of accessories based on the weather and the selected style. It is \" + current_weather.description + \" with temperature at \" + str(current_weather.temperature) +\" degrees fahrenheit.\\n Selected Style: \" + recommendations[0]\n",
    "    accessory_response = qa({\"query\": accessory_input})['result']\n",
    "    print_ww(accessory_response)"
   ],
   "id": "204770ed-f303-4ee0-a258-21f87596e7a6",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c4e689c0-8862-41a1-8867-e2bb10c2f64f",
   "metadata": {},
   "source": [
    "## Simulate the order check out\n",
    "\n",
    "Add a customer data table to complete the order transaction. This information provides the shipping address for the outfit order."
   ]
  },
  {
   "cell_type": "code",
   "id": "e79f2e20-0c64-44fa-ab3b-8d2401a00c4f",
   "metadata": {},
   "source": [
    "customer_table=[{\"id\": 1, \"first_name\": \"John\", \"last_name\": \"Doe\", \"age\": 35, \"address\": \"123 Bedrock st, California 90210\"},\n",
    "  {\"id\": 2, \"first_name\": \"Jane\", \"last_name\": \"Smith\", \"age\": 27, \"address\": \"234 Sagemaker drive, Texas 12345\"},\n",
    "  {\"id\": 3, \"first_name\": \"Bob\", \"last_name\": \"Jones\", \"age\": 42, \"address\": \"111 DeepRacer ct, Virginia 55555\"},\n",
    "  {\"id\": 4, \"first_name\": \"Sara\", \"last_name\": \"Miller\", \"age\": 29, \"address\": \"222 Robomaker ave, New Yotk 13579\"},\n",
    "  {\"id\": 5, \"first_name\": \"Mark\", \"last_name\": \"Davis\", \"age\": 31, \"address\": \"444 Transcribe blvd, Florida 02468\"},\n",
    "  {\"id\": 6, \"first_name\": \"Laura\", \"last_name\": \"Wilson\", \"age\": 24, \"address\": \"555 CodeGuru st, California 98765\" },\n",
    "  {\"id\": 7, \"first_name\": \"Steve\", \"last_name\": \"Moore\", \"age\": 36, \"address\": \"456 DeepLens st, Texas 11223\"},\n",
    "  {\"id\": 8, \"first_name\": \"Michelle\", \"last_name\": \"Chen\", \"age\": 22, \"address\": \"642 DeepCompose st, Colorado 33215\"},\n",
    "  {\"id\": 9, \"first_name\": \"David\", \"last_name\": \"Lee\", \"age\": 29, \"address\": \"777 S3 st, California 99567\"},\n",
    "  {\"id\": 10, \"first_name\": \"Jessica\", \"last_name\": \"Brown\", \"age\": 18, \"address\": \"909 Ec st, Utah 43210\"}]\n",
    "\n",
    "def address_lookup(id):\n",
    "    for customer in customer_table:\n",
    "        if customer[\"id\"] == int(id):\n",
    "            return customer\n",
    "        \n",
    "    return None\n",
    "\n",
    "print(address_lookup(customer_id)[\"address\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c22dcfd0",
   "metadata": {},
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
