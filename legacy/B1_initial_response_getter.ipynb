{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:06:03.998659Z",
     "start_time": "2024-07-01T20:05:59.993840Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -r requirements.txt",
   "id": "initial_id",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:06:04.004111Z",
     "start_time": "2024-07-01T20:06:04.001077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "ba5dc8375cee7baa",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:06:05.107662Z",
     "start_time": "2024-07-01T20:06:04.005793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from modules.vector_index.utils import bedrock\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=False)\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n",
    "\n",
    "model_parameter = {\n",
    "    \"temperature\": 0.0, \n",
    "    \"top_p\": .5, \n",
    "    \"top_k\": 250, \n",
    "    \"max_tokens_to_sample\": 2000, \n",
    "    \"stop_sequences\": [\"\\n\\n Human: bye\"]\n",
    "}\n",
    "llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    model_kwargs=model_parameter, \n",
    "    client=bedrock_runtime\n",
    ")"
   ],
   "id": "917e3339e7e26528",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:06:05.724785Z",
     "start_time": "2024-07-01T20:06:05.111738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create Vector Index\n",
    "import pandas as pd\n",
    "parquet_file_path = \"processed/grainger_products.parquet\"\n",
    "print(\"Attempting to load file from:\", parquet_file_path)\n",
    "\n",
    "# Now attempt to load the file\n",
    "try:\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "    print(\"File loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Error loading file:\", e)\n",
    "\n",
    "print(df.head())\n"
   ],
   "id": "8eb2e0061c74496e",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:09:42.422102Z",
     "start_time": "2024-07-01T20:06:05.726217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# AS A DOCUMENT\n",
    "# Automates the process and optimizes for large and changing data sets.\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "        \n",
    "# Initialize the Titan Embeddings Model\n",
    "print(\"Initializing Titan Embeddings Model...\")\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_runtime)\n",
    "print(\"Titan Embeddings Model initialized.\")\n",
    "\n",
    "documents = []\n",
    "for _, row in df.iterrows():\n",
    "    page_content = f\"{row['Code']} {row['Name']} {row['Brand']} {row['Description'] if pd.notna(row['Description']) else ''}\"\n",
    "    metadata = {\n",
    "        'Brand': row['Brand'],\n",
    "        'Code': row['Code'],\n",
    "        'Name': row['Name'],\n",
    "        'Description': row['Description'],\n",
    "        'Price': row['Price']\n",
    "    }\n",
    "    documents.append(Document(page_content, metadata))\n",
    "\n",
    "\n",
    "# Print the structured documents\n",
    "print(\"Structured documents created:\")\n",
    "for idx, doc in enumerate(documents[:5], 1):  \n",
    "    print(f\"Document {idx} of {len(documents)}:\")\n",
    "    print(doc.page_content[:200])\n",
    "    print()\n",
    "\n",
    "# Create FAISS vector store from structured documents\n",
    "print(\"Creating FAISS vector store from structured documents...\")\n",
    "vectorstore_faiss_doc = FAISS.from_documents(documents, bedrock_embeddings)\n",
    "print(\"FAISS vector store created.\")\n",
    "\n"
   ],
   "id": "8ee19c6318ea86ed",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:09:42.427732Z",
     "start_time": "2024-07-01T20:09:42.424293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ENTER INITIAL INPUT HERE\n",
    "\n",
    "customer_input = \"I am looking for waterproof insulated boots for my men working on my commercial deep sea fishing boat in the arctic. Must have large sizes\"\n"
   ],
   "id": "953bd0b82f1b2bc3",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:09:42.542505Z",
     "start_time": "2024-07-01T20:09:42.429978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "query_embedding_doc = bedrock_embeddings.embed_query(customer_input)\n",
    "print(\"Customer input processed.\")\n",
    "\n",
    "# Convert query embedding to numpy array\n",
    "np_array_query_embedding_doc = np.array(query_embedding_doc)\n",
    "print(\"Query embedding converted to numpy array.\")\n",
    "\n",
    "# Print the resulting query embedding\n",
    "print(\"Resulting query embedding:\")\n",
    "print(np_array_query_embedding_doc)\n"
   ],
   "id": "2d777e20a73a3ce2",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:09:42.548351Z",
     "start_time": "2024-07-01T20:09:42.544399Z"
    }
   },
   "cell_type": "code",
   "source": "customer_input = \"I am looking for hats to protect my men from the sun while working out in road construction in Arizona heat. I have a large company and need a solution that I can buy in bulk.\"",
   "id": "751fd5041072c2fa",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:13:58.078803Z",
     "start_time": "2024-07-01T20:13:58.073653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "def extract_customer_attributes(customer_input):\n",
    "    # Define the NER prompt with placeholders for the customer input\n",
    "    ner_prompt = \"\"\"Human: Find industry, size, Sustainability Focus, Inventory Manager, and the location in the customer input.\n",
    "    Instructions:\n",
    "    The industry can be one of the following: Manufacturing, Warehousing, Government and Public Safety, Education, Food and Beverage Distribution, Hospitality, Property Management, Retail, or Other\n",
    "    The size can be one of the following: Small Businesses (Smaller companies might prioritize cost-effective solutions and fast shipping options), or Large Enterprises (Larger organizations may require more comprehensive solutions, including strategic services like inventory management and safety consulting), Womens, Other\n",
    "    The Sustainability Focused true or false meaning Environmentally Conscious Buyers: Customers interested in sustainability solutions, looking for products that focus on energy management, water conservation, waste reduction, and air quality improvement, or NOT Environmentally Conscious Buyers,\n",
    "    The Inventory Manager true or false meaning a purchaser in large amounts to supply an organizational group, versus an individual user purchasing for personal use,\n",
    "    The output must be in JSON format inside the tags <attributes></attributes>\n",
    "\n",
    "    If the information of an entity is not available in the input then don't include that entity in the JSON output\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    Customer input: {customer_input}\n",
    "    Assistant:\"\"\".format(customer_input=customer_input)\n",
    "\n",
    "    # Process the customer input with the NER model\n",
    "    entity_extraction_result = llm(ner_prompt).strip()\n",
    "\n",
    "    # Extract the attributes from the processed result\n",
    "    result = re.search('<attributes>(.*?)</attributes>', entity_extraction_result, re.DOTALL)\n",
    "    if result:\n",
    "        attributes_str = result.group(1)\n",
    "        # Convert the attributes string to JSON\n",
    "        attributes = json.loads(attributes_str)\n",
    "        return attributes\n",
    "    else:\n",
    "        return {}\n"
   ],
   "id": "d000a7d2a0039369",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:14:09.314075Z",
     "start_time": "2024-07-01T20:14:09.308817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## GET LIST OF PRODUCTS AND CODES\n",
    "# from langchain.chains import RetrievalQA\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# prompt_template2 = \"\"\"Human: Extract list of upto 5 products and their respective physical IDs from catalog that answer the user question. \n",
    "# The catalog of products is provided under <catalog></catalog> tags below.\n",
    "# <catalog>\n",
    "# {context}\n",
    "# </catalog>\n",
    "# Question: {question}\n",
    "# \n",
    "# The output should be a json of the form <products>[{{\"product\": <description of the product from the catalog>, \"code\":<code of the product from the catalog>}}, ...]</products>\n",
    "# Skip the preamble and always return valid json.\n",
    "# Assistant: \"\"\"\n",
    "# PROMPT = PromptTemplate(\n",
    "#     template=prompt_template2, input_variables=[\"context\", \"question\"]\n",
    "# )\n",
    "# \n",
    "# # Use RetrievalQA customizations for improving Q&A experience\n",
    "# search_index_get_answer_from_llm = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=vectorstore_faiss_doc.as_retriever(\n",
    "#         search_type=\"similarity\", search_kwargs={\"k\": 6}\n",
    "#     ),\n",
    "#     return_source_documents=False,\n",
    "#     chain_type_kwargs={\"prompt\": PROMPT},\n",
    "# )\n"
   ],
   "id": "7e64cd972402e335",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T21:45:30.651091Z",
     "start_time": "2024-07-01T21:45:30.646942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template3 = \"\"\"Human: Extract list of upto 5 products and their respective physical IDs from catalog that answer the user question. \n",
    "The catalog of products is provided under <catalog></catalog> tags below.\n",
    "<catalog>\n",
    "{context}\n",
    "</catalog>\n",
    "Question: {question}\n",
    "\n",
    "The output should be a json of the form <products>[{{\"product\": <description of the product from the catalog>, \"code\":<code of the product from the catalog>}}, ...]</products> for me to process.\n",
    "Also, provide a user-readable message responding in full to the question with all the of the information to display to the user in the form <response>{{message}}</response>.\n",
    "Skip the preamble and always return valid json.\n",
    "Assistant: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template3, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Use RetrievalQA customizations for improving Q&A experience\n",
    "search_index_get_answer_from_llm = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss_doc.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 6}\n",
    "    ),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")"
   ],
   "id": "7906a0bc271fcd78",
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:14:12.849071Z",
     "start_time": "2024-07-01T20:14:12.846403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Call for reviews:\n",
    "# TODO\n",
    "reviews_dict = None"
   ],
   "id": "73fcf382c57fe0a",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:14:15.455049Z",
     "start_time": "2024-07-01T20:14:15.448635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def response_to_json(recs_response):\n",
    "    # Ensure recs_response is handled correctly\n",
    "    recs_response = recs_response.strip()  # Remove leading/trailing whitespace\n",
    "    response_json = \"\"\n",
    "\n",
    "    # Check if the response starts and ends with expected JSON markers\n",
    "    if recs_response.startswith(\"<products>\") and recs_response.endswith(\"</products>\"):\n",
    "        json_content = recs_response[len(\"<products>\") : -len(\"</products>\")].strip()\n",
    "\n",
    "        try:\n",
    "            parsed_response = json.loads(json_content)\n",
    "\n",
    "            if isinstance(parsed_response, list):\n",
    "                products_list = []\n",
    "                for product_info in parsed_response:\n",
    "                    # Assuming product_info is a dictionary with 'product' and 'code' keys\n",
    "                    product_data = {\n",
    "                        \"product\": product_info.get(\"product\", \"\"),\n",
    "                        \"code\": product_info.get(\"code\", \"\")\n",
    "                    }\n",
    "                    products_list.append(product_data)\n",
    "\n",
    "                response_json = {\"products\": products_list}\n",
    "                return response_json\n",
    "            else:\n",
    "                print(\"Error: Unexpected format of parsed response\")\n",
    "                return None\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {str(e)}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Error: Unexpected format of recs_response\")\n",
    "        return None"
   ],
   "id": "3fde7a81320657a3",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T21:48:22.312938Z",
     "start_time": "2024-07-01T21:48:22.306832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def split_process_and_message_from_response(recs_response):\n",
    "    recs_response = recs_response.strip()  # Remove leading/trailing whitespace\n",
    "\n",
    "    # Extract the message\n",
    "    message_match = re.search('<response>(.*?)</response>', recs_response, re.DOTALL)\n",
    "    message = message_match.group(1).strip() if message_match else None\n",
    "\n",
    "    # Extract the products\n",
    "    if \"<products>\" in recs_response and \"</products>\" in recs_response:\n",
    "        json_content = recs_response[recs_response.index(\"<products>\") + len(\"<products>\"): recs_response.index(\"</products>\")].strip()\n",
    "\n",
    "        try:\n",
    "            parsed_response = json.loads(json_content)\n",
    "\n",
    "            if isinstance(parsed_response, list):\n",
    "                products_list = []\n",
    "                for product_info in parsed_response:\n",
    "                    product_data = {\n",
    "                        \"product\": product_info.get(\"product\", \"\"),\n",
    "                        \"code\": product_info.get(\"code\", \"\")\n",
    "                    }\n",
    "                    products_list.append(product_data)\n",
    "\n",
    "                response_json = {\"products\": products_list}\n",
    "                return message, response_json\n",
    "            else:\n",
    "                print(\"Error: Unexpected format of parsed response\")\n",
    "                return None, None\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {str(e)}\")\n",
    "            return None, None\n",
    "    else:\n",
    "        print(\"Error: Unexpected format of recs_response\")\n",
    "        return None, None"
   ],
   "id": "9999cba94019bce5",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:14:17.217875Z",
     "start_time": "2024-07-01T20:14:17.214785Z"
    }
   },
   "cell_type": "code",
   "source": "chat_history = [\" \"]",
   "id": "6e554a761b3dcfd6",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T21:53:28.286237Z",
     "start_time": "2024-07-01T21:53:16.235631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "def process_chat_question(question, clear_history=False):\n",
    "    try:\n",
    "        if clear_history:\n",
    "            chat_history.clear()  # Clear chat history if specified\n",
    "        \n",
    "        chat_history.append([question])\n",
    "        \n",
    "        # Extract product attributes from the question\n",
    "        customer_attributes_retrieved = extract_customer_attributes(question)\n",
    "\n",
    "        # Format the customer input with the extracted attributes\n",
    "        customer_input_with_attributes = \"{} {}\".format(question, str(customer_attributes_retrieved))\n",
    "\n",
    "        # # Retrieve data based on the formatted customer input\n",
    "        # retrieved_data_from_index = search_index_get_answer_from_llm({\"query\": customer_input_with_attributes})['result'] < -- the exact same thing as search_index_get_answer_from_llm.run(**context)\n",
    "        # \n",
    "        # # Append the retrieved data to the chat history\n",
    "        # chat_history.append(retrieved_data_from_index)\n",
    "\n",
    "        # Prepare the context with the formatted customer input and chat history\n",
    "        context = {\n",
    "            'query': customer_input_with_attributes,\n",
    "            'chat_history': chat_history\n",
    "        }\n",
    "\n",
    "        # OBTAIN RESPONSE\n",
    "        # Run conversation with provided context synchronously\n",
    "        llm_retrieval_augmented_response = search_index_get_answer_from_llm.run(**context)\n",
    "        print(llm_retrieval_augmented_response)\n",
    "        message, product_list_as_json = split_process_and_message_from_response(llm_retrieval_augmented_response)   \n",
    "        \n",
    "        \n",
    "        # UPDATE HISTORY\n",
    "        if product_list_as_json is not None:\n",
    "            chat_history.append(product_list_as_json['products'])\n",
    "        \n",
    "        if reviews_dict is not None:\n",
    "            chat_history.append(reviews_dict)\n",
    "\n",
    "\n",
    "        return message, product_list_as_json  # Return chat response as a string\n",
    "\n",
    "\n",
    "    except ValueError as error:\n",
    "        if \"AccessDeniedException\" in str(error):\n",
    "            class StopExecution(ValueError):\n",
    "                def _render_traceback_(self):\n",
    "                    pass\n",
    "            raise StopExecution\n",
    "        else:\n",
    "            raise error\n",
    "        \n",
    "customer_input2 = \"I am looking for waterproof insulated boots for my men working for me on a commercial fishing boat in the Arctic cold.\"\n",
    "message, response_as_json = process_chat_question(customer_input2)\n",
    "print(str(response_as_json))\n",
    "print (message)\n"
   ],
   "id": "594a5758c2c52428",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:14:20.157705Z",
     "start_time": "2024-07-01T20:14:20.155539Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9264490c78f6eeb1",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:14:20.918995Z",
     "start_time": "2024-07-01T20:14:20.915482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # HERE IS THE CONVERSATION\n",
    "# from langchain.chains import ConversationalRetrievalChain\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "# from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "# \n",
    "# chat_history = [\" \"]\n",
    "# memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "# conversation = ConversationalRetrievalChain.from_llm(\n",
    "#     llm=llm, \n",
    "#     retriever=vectorstore_faiss_doc.as_retriever(), \n",
    "#     memory=memory_chain,\n",
    "#     condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "#     chain_type='stuff',  # 'refine',\n",
    "# )\n",
    "# \n",
    "# # Define a function to process the chat question\n",
    "# def process_chat_question(question, clear_history=False):\n",
    "#     try:\n",
    "#         if clear_history:\n",
    "#             chat_history.clear()  # Clear chat history if specified\n",
    "# \n",
    "#         context = {\n",
    "#             'question': question,\n",
    "#             'chat_history': chat_history\n",
    "#         }\n",
    "# \n",
    "#         # Run conversation with provided context synchronously\n",
    "#         chat_res = search_index_get_answer_from_llm.run(**context)\n",
    "# \n",
    "#         # Append the chat prompt and result to history\n",
    "#         chat_history.append([question, chat_res])\n",
    "# \n",
    "#         # Optionally add response_json['products'] and reviews_dict to chat history\n",
    "#         if response_json:\n",
    "#             chat_history.append(response_json['products'])\n",
    "# \n",
    "#         if reviews_dict:\n",
    "#             chat_history.append(reviews_dict)\n",
    "# \n",
    "#         return str(chat_res)  # Return chat response as a string\n",
    "# \n",
    "#     except ValueError as error:\n",
    "#         if \"AccessDeniedException\" in str(error):\n",
    "#             class StopExecution(ValueError):\n",
    "#                 def _render_traceback_(self):\n",
    "#                     pass\n",
    "#             raise StopExecution\n",
    "#         else:\n",
    "#             raise error\n"
   ],
   "id": "d55fc043bbeb9822",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:24:33.571140Z",
     "start_time": "2024-07-01T20:24:05.484347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "customer_input2 = \"I am looking for waterproof insulated boots for my men working for me on a commercial fishing boat in the Arctic cold.\"\n",
    "customer_attributes = extract_customer_attributes(customer_input2)\n",
    "customer_input = \"{} {}\".format(customer_input, str(customer_attributes))\n",
    "retrieved_data = search_index_get_answer_from_llm({\"query\": customer_input2})['result']\n",
    "chat_history.append(retrieved_data)\n",
    "response = process_chat_question(question=customer_input2, clear_history=False)\n",
    "print(response)"
   ],
   "id": "5c8c575299631d60",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:09:58.886135Z",
     "start_time": "2024-07-01T20:09:58.885979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage:\n",
    "question = (\"What boots do you have in size 14 that are water proof?\")\n",
    "response = process_chat_question(question,clear_history=False)  # Specify clear_history as needed\n",
    "print(response)"
   ],
   "id": "cfc47ecd751830af",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T21:27:44.403180Z",
     "start_time": "2024-07-01T21:27:18.088307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Example usage:\n",
    "question = (\"I am looking for hats to protect my men from the sun while working out in road construction in Arizona heat. \")\n",
    "response = process_chat_question(question,  clear_history=True)  # Specify clear_history as needed\n",
    "print(response)"
   ],
   "id": "9f8ebe782830cb83",
   "execution_count": 35,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "56e1aea0cf3bd416",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
