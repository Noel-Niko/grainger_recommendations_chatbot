{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T01:11:00.393140Z",
     "start_time": "2024-06-25T01:11:00.389844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "domain = \"https://www.grainger.com\"\n",
    "start_url = \"https://www.grainger.com/category\""
   ],
   "id": "b622fd09e9296127",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T01:11:14.933512Z",
     "start_time": "2024-06-25T01:11:00.401478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import os\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from collections import deque\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium setup\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Function to get the hyperlinks from a URL using Selenium\n",
    "def get_hyperlinks(url):\n",
    "    print(f\"Getting hyperlinks for URL: {url}\")\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Adjust wait time as needed based on page load speed\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        hyperlinks = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "        return hyperlinks\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting hyperlinks: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to crawl the website\n",
    "def crawl(url):\n",
    "    local_domain = urlparse(domain).netloc\n",
    "    queue = deque([url])\n",
    "    seen = {url}\n",
    "\n",
    "    # Create necessary directories if they don't exist\n",
    "    if not os.path.exists(\"text/\"):\n",
    "        os.mkdir(\"text/\")\n",
    "    if not os.path.exists(f\"text/{local_domain}/\"):\n",
    "        os.mkdir(f\"text/{local_domain}/\")\n",
    "    if not os.path.exists(\"processed\"):\n",
    "        os.mkdir(\"processed\")\n",
    "\n",
    "    while queue:\n",
    "        url = queue.popleft()  # Use popleft to ensure we process URLs in a breadth-first manner\n",
    "        print(f\"Crawling URL: {url}\")\n",
    "        try:\n",
    "            # Construct file path for saving content\n",
    "            file_path = f\"text/{local_domain}/{url[8:].replace('/', '_')}.txt\"\n",
    "            with open(file_path, \"w\") as f:\n",
    "                driver.get(url)\n",
    "                time.sleep(3)  # Adjust wait time as needed based on page load speed\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                text = soup.get_text()\n",
    "                if \"You need to enable JavaScript to run this app.\" in text:\n",
    "                    print(f\"Unable to parse page {url} due to JavaScript being required\")\n",
    "                f.write(text)\n",
    "                # print(f\"Writing: {text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling URL: {e}\")\n",
    "\n",
    "        try:\n",
    "            for link in get_domain_hyperlinks(local_domain, url):\n",
    "                if link not in seen:\n",
    "                    queue.append(link)\n",
    "                    seen.add(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing links: {e}\")\n",
    "\n",
    "# Function to get the hyperlinks from a URL that are within the same domain and base URL\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    hyperlinks = get_hyperlinks(url)\n",
    "    print(f\"Found {len(hyperlinks)} hyperlinks on {url}\")\n",
    "    for link in set(hyperlinks):\n",
    "        clean_link = None\n",
    "        print(f\"Checking link: {link}\")\n",
    "        \n",
    "        # Handle absolute URLs\n",
    "        if link.startswith(\"http\"):\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain and link.startswith(domain):\n",
    "                clean_link = link\n",
    "        else:\n",
    "            # Handle relative URLs\n",
    "            clean_link = urljoin(url, link)\n",
    "\n",
    "        if clean_link is not None and '@' not in clean_link and clean_link not in clean_links:\n",
    "            print(f\"Adding clean link: {clean_link}\")\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    print(f\"Clean links: {clean_links}\")\n",
    "    return list(set(clean_links))\n",
    "\n",
    "# Start crawling from the base URL\n",
    "crawl(start_url)\n",
    "\n",
    "driver.quit()\n"
   ],
   "id": "8e4a4817eef2c0f1",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T01:11:14.940741Z",
     "start_time": "2024-06-25T01:11:14.936032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_newlines(serie):\n",
    "    serie = serie.str.replace('\\n', ' ')\n",
    "    serie = serie.str.replace('\\\\n', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    return serie"
   ],
   "id": "c90a548c527fb797",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T01:11:14.961940Z",
     "start_time": "2024-06-25T01:11:14.943053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create a list to store the text files\n",
    "texts=[]\n",
    "\n",
    "domain_dir = os.path.join(\"text\", domain)\n",
    "abs_domain_dir = os.path.abspath(domain_dir)\n",
    "\n",
    "if os.path.exists(abs_domain_dir):\n",
    "    # Get all the text files in the text directory\n",
    "    for file in os.listdir(\"text/\" + domain + \"/\"):\n",
    "        \n",
    "        try:\n",
    "            # Open the file and read the text\n",
    "            with open(\"text/\" + domain + \"/\" + file, \"r\") as f:\n",
    "                text = f.read()\n",
    "        \n",
    "                # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n",
    "                texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred during reading file '{text}': {e}\")       \n",
    "else:\n",
    "    print(f\"Directory '{abs_domain_dir}' does not exist.\")\n",
    "# Create a dataframe from the list of texts\n",
    "df = pd.DataFrame(texts, columns = ['fname', 'text'])\n",
    "\n",
    "# Set the text column to be the raw text with the newlines removed\n",
    "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
    "df.to_csv('processed/scraped.csv')\n",
    "df.head()"
   ],
   "id": "bcf56d37ebc4382e",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T01:11:15.448375Z",
     "start_time": "2024-06-25T01:11:14.964809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'text': [\"This is a sample text.\", \"Another example text.\", \"Yet another text sample.\"]\n",
    "})\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Calculate the number of tokens for each text\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "# Visualize the distribution of the number of tokens per row using a histogram\n",
    "df.n_tokens.hist()\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Tokens per Text')\n",
    "plt.show()\n"
   ],
   "id": "6a6250b69dfcc8c0",
   "execution_count": 33,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
