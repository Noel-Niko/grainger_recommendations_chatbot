{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9dd792053e8726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T21:25:40.838954Z",
     "start_time": "2024-06-26T21:25:40.835278Z"
    }
   },
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "domain = \"https://www.grainger.com\"\n",
    "start_url = \"https://www.grainger.com/category/tools/power-tools/cordless-tool-combination-kits?categoryIndex=4\"\n",
    "local_domain = urlparse(domain).netloc"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629746066fbb4484",
   "metadata": {},
   "source": [
    "### WEB CRAWLING THE GRAINGER SITE REQUIRES GRAINGER VPN ACCESS"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29ca48b13f9b17da",
   "metadata": {},
   "source": [
    "#DEPTH-FIRST CRAWLER\n",
    "import time\n",
    "import os\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from collections import deque\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Selenium setup\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Function to get the hyperlinks from a URL using Selenium\n",
    "def get_hyperlinks(url):\n",
    "    print(f\"Getting hyperlinks for URL: {url}\")\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(1)  # Adjust wait time\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        hyperlinks = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "        return hyperlinks\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting hyperlinks: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to crawl the website using depth-first strategy\n",
    "def crawl(url):\n",
    "    local_domain = urlparse(domain).netloc\n",
    "    stack = [url]  # Use a stack instead of a queue for depth-first\n",
    "    seen = {url}\n",
    "    clean_links = []\n",
    "\n",
    "    # Create necessary directories if they don't exist\n",
    "    if not os.path.exists(\"text/\"):\n",
    "        os.mkdir(\"text/\")\n",
    "    if not os.path.exists(f\"text/{local_domain}/\"):\n",
    "        os.mkdir(f\"text/{local_domain}/\")\n",
    "    if not os.path.exists(\"processed\"):\n",
    "        os.mkdir(\"processed\")\n",
    "\n",
    "    while stack and len(clean_links) < 100000:\n",
    "        url = stack.pop()  # Use pop() to ensure we process URLs in a depth-first manner\n",
    "        print(f\"Crawling URL: {url}\")\n",
    "        try:     \n",
    "               # Handle absolute URLs\n",
    "            if url.startswith(\"http://\") or url.startswith(\"https://\"):\n",
    "                url_domain = urlparse(url).netloc\n",
    "                if url_domain != local_domain:\n",
    "                    print(f\"Skipping URL: {url} (Not part of {domain})\")\n",
    "                    continue\n",
    "                \n",
    "            # Construct file path for saving content\n",
    "            file_path = f\"text/{local_domain}/{url[8:].replace('/', '_')}.txt\"\n",
    "            with open(file_path, \"w\") as f:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)  # Adjust wait time as needed based on page load speed\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                text = soup.get_text()\n",
    "                if \"You need to enable JavaScript to run this app.\" in text:\n",
    "                    print(f\"Unable to parse page {url} due to JavaScript being required\")\n",
    "                f.write(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling URL: {e}\")\n",
    "\n",
    "        try:\n",
    "            new_links = get_domain_hyperlinks(local_domain, url, clean_links)\n",
    "            for link in new_links:\n",
    "                if link not in seen:\n",
    "                    stack.append(link)\n",
    "                    seen.add(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing links: {e}\")\n",
    "\n",
    "    print(f\"Collected {len(clean_links)} clean links\")\n",
    "\n",
    "# Function to get the hyperlinks from a URL that are within the same domain and base URL\n",
    "def get_domain_hyperlinks(local_domain, url, clean_links):\n",
    "    regex_pattern = re.compile(r'/product/.*-[A-Z0-9]{5,7}(?=.*\\d.*\\d.*\\d)\\?.*')\n",
    "\n",
    "    hyperlinks = get_hyperlinks(url)\n",
    "    print(f\"Found {len(hyperlinks)} hyperlinks on {url}\")\n",
    "    for link in set(hyperlinks):\n",
    "        clean_link = None\n",
    "        print(f\"Checking link: {link}\")\n",
    "\n",
    "        # Handle absolute URLs\n",
    "        if link.startswith(\"http\"):\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain and link.startswith(domain):\n",
    "                clean_link = link\n",
    "        else:\n",
    "            # Handle relative URLs\n",
    "            clean_link = urljoin(url, link)\n",
    "\n",
    "        if clean_link is not None and '@' not in clean_link and regex_pattern.search(clean_link):\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            if clean_link not in clean_links:\n",
    "                print(f\"Adding clean link: {clean_link}\")\n",
    "                clean_links.append(clean_link)\n",
    "\n",
    "        # Stop collecting if we reach n links\n",
    "        if len(clean_links) >= 100000:\n",
    "            break\n",
    "\n",
    "    print(f\"Clean links: {clean_links}\")\n",
    "    return list(set(hyperlinks))\n",
    "\n",
    "crawl(start_url)\n",
    "\n",
    "driver.quit()\n",
    "#DEPTH-FIRST"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "254eb99084874ec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T11:43:29.723770Z",
     "start_time": "2024-06-25T11:43:29.721390Z"
    }
   },
   "source": [
    "def remove_newlines(serie):\n",
    "    serie = serie.str.replace('\\n', ' ')\n",
    "    serie = serie.str.replace('\\\\n', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    return serie"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920f8defadbf841",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-26T21:25:52.952031Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "\"\"\"\n",
    "Uncomment: df.to_csv('processed/scraped.csv')\n",
    "to save changes and OVERWRITE current file.\n",
    "CAUTION: this will overwrite the existing,\n",
    "and require VPN access in the Grainger System \n",
    "to rerun the exhaustive web crawl above.\n",
    "\"\"\" \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create a list to store the text files\n",
    "texts=[]\n",
    "\n",
    "domain_dir = os.path.join(\"text\", local_domain)\n",
    "abs_domain_dir = os.path.abspath(domain_dir)\n",
    "\n",
    "if os.path.exists(abs_domain_dir):\n",
    "    # Get all the text files in the text directory\n",
    "    for file in os.listdir(abs_domain_dir):\n",
    "        \n",
    "        try:\n",
    "            # Open the file and read the text\n",
    "            with open(\"text/\" + local_domain + \"/\" + file, \"r\") as f:\n",
    "                text = f.read()\n",
    "        \n",
    "                # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n",
    "                texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred during reading file '{text}': {e}\")       \n",
    "else:\n",
    "    print(f\"Directory '{abs_domain_dir}' does not exist.\")\n",
    "# Create a dataframe from the list of texts\n",
    "df = pd.DataFrame(texts, columns = ['fname', 'text'])\n",
    "\n",
    "# Set the text column to be the raw text with the newlines removed\n",
    "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
    "\"\"\"\n",
    "Uncomment: df.to_csv('processed/scraped.csv')\n",
    "to save changes and OVERWRITE current file.\n",
    "CAUTION: this will overwrite the existing,\n",
    "and require VPN access in the Grainger System \n",
    "to rerun the exhaustive web crawl above.\n",
    "\"\"\" \n",
    "# df.to_csv('processed/scraped.csv')\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b49919d40052d4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:12:33.461051Z",
     "start_time": "2024-06-25T15:12:24.738209Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the dataframe from the CSV file\n",
    "df = pd.read_csv('processed/scraped.csv')\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Calculate the number of tokens for each text\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "# Visualize the distribution of the number of tokens per row using a histogram\n",
    "df.n_tokens.hist()\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Tokens per Text')\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64bf4dd30f8615eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T05:45:35.179217Z",
     "start_time": "2024-06-25T05:45:35.142029Z"
    }
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
