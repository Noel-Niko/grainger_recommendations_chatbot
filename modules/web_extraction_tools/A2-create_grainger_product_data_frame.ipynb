{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a11e5f910934738",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T14:01:41.190143Z",
     "start_time": "2024-06-26T14:01:41.185289Z"
    }
   },
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "domain = \"https://www.grainger.com\"\n",
    "local_domain = urlparse(domain).netloc\n",
    "# Create necessary directories if they don't exist\n",
    "if not os.path.exists(\"text/\"):\n",
    "    os.mkdir(\"text/\")\n",
    "if not os.path.exists(f\"text/{local_domain}/\"):\n",
    "    os.mkdir(f\"text/{local_domain}/\")\n",
    "if not os.path.exists(\"processed\"):\n",
    "    os.mkdir(\"processed\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a61fd1fe2b624523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T14:01:41.988839Z",
     "start_time": "2024-06-26T14:01:41.986386Z"
    }
   },
   "source": [
    "# TEST \n",
    "import re\n",
    "\n",
    "# Pattern to match product skus/codes\n",
    "regex_pattern = re.compile(r'[A-Z0-9]{5,7}')\n",
    "# Test strings\n",
    "test_strings = [\n",
    "    \"1DKW3_1.pdf\",\n",
    "    \"3VE59C-Operating-Instructions-and-Parts-Manual.pdf\",\n",
    "    \"_3M-Disposable-Respirator-Dual-4JF99?opr=PDPBRDSP&analytics=dsbrItems_5ZZZ6.txt\"\n",
    "]\n",
    "\n",
    "# Extract product codes from test strings\n",
    "for test in test_strings:\n",
    "    matches = regex_pattern.findall(test)\n",
    "    print(f\"Matches in '{test}': {matches}\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d225d8e17be55ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T14:01:42.816356Z",
     "start_time": "2024-06-26T14:01:41.990122Z"
    }
   },
   "source": [
    "# # PULL product codes from web scraped data and save as json\n",
    "# import os\n",
    "# import re\n",
    "# import json\n",
    "# \n",
    "# # Define the regex pattern for product codes\n",
    "# regex_pattern = re.compile(r'[A-Z0-9]{5,7}')\n",
    "# \n",
    "# # Directory containing the files\n",
    "# directory = 'GraingerWebScrape/www.grainger.com'\n",
    "# \n",
    "# # List to store found product codes\n",
    "# product_codes = []\n",
    "# \n",
    "# # Function to extract product codes from text\n",
    "# def extract_product_codes(text):\n",
    "#     return regex_pattern.findall(text)\n",
    "# \n",
    "# # Iterate through all files in the directory\n",
    "# for root, dirs, files in os.walk(directory):\n",
    "#     for file in files:\n",
    "#         file_path = os.path.join(root, file)\n",
    "# \n",
    "#         # Check for product codes in the file name\n",
    "#         codes_in_filename = extract_product_codes(file)\n",
    "#         product_codes.extend(codes_in_filename)\n",
    "# \n",
    "#         # Check for product codes in the file content\n",
    "#         try:\n",
    "#             with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                 content = f.read()\n",
    "#                 codes_in_content = extract_product_codes(content)\n",
    "#                 product_codes.extend(codes_in_content)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Could not read file {file_path}: {e}\")\n",
    "# \n",
    "# # Remove duplicates by converting the list to a set and back to a list\n",
    "# product_codes = list(set(product_codes))\n",
    "# \n",
    "# # Save all product codes to a single JSON file\n",
    "# with open('all_product_codes.json', 'w') as f:\n",
    "#     json.dump(product_codes, f, indent=4)\n",
    "# \n",
    "# print(f\"Total product codes found: {len(product_codes)}\")\n",
    "# print(f\"Product codes saved in 'all_product_codes.json'\")\n"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # PULL product codes from web scraped data and save as json\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Define the regex pattern for product codes\n",
    "regex_pattern = re.compile(r'[A-Z0-9]{5,7}')\n",
    "\n",
    "# Directory containing the files\n",
    "directory = 'GraingerWebScrape/www.grainger.com'\n",
    "\n",
    "# Load existing product codes from the JSON file\n",
    "existing_product_codes = []\n",
    "if os.path.exists('all_product_codes.json'):\n",
    "    with open('all_product_codes.json', 'r') as f:\n",
    "        existing_product_codes = json.load(f)\n",
    "\n",
    "# List to store found product codes\n",
    "product_codes = []\n",
    "\n",
    "# Function to extract product codes from text\n",
    "def extract_product_codes(text):\n",
    "    return regex_pattern.findall(text)\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        # Check for product codes in the file name\n",
    "        codes_in_filename = extract_product_codes(file)\n",
    "        product_codes.extend(codes_in_filename)\n",
    "\n",
    "        # Check for product codes in the file content\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                codes_in_content = extract_product_codes(content)\n",
    "                product_codes.extend(codes_in_content)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read file {file_path}: {e}\")\n",
    "\n",
    "# Remove duplicates by converting the list to a set and back to a list\n",
    "product_codes = list(set(product_codes))\n",
    "\n",
    "# Combine existing and new product codes\n",
    "product_codes = list(set(existing_product_codes + product_codes))\n",
    "\n",
    "# Save all product codes to the JSON file\n",
    "with open('all_product_codes.json', 'w') as f:\n",
    "    json.dump(product_codes, f, indent=4)\n",
    "\n",
    "print(f\"Total product codes found: {len(product_codes)}\")\n",
    "print(f\"Product codes saved in 'all_product_codes.json'\")"
   ],
   "id": "e75e472252e9edd8"
  },
  {
   "cell_type": "code",
   "id": "a23af246e9826876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T01:42:47.283516Z",
     "start_time": "2024-06-29T01:42:30.363990Z"
    }
   },
   "source": [
    "# FETCH DATA ON PRODUCT CODES FROM URL AND SAVE AS DATA FRAME\n",
    "# UPDATE THE JSON FILE WITH VALID PRODUCT CODES\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Base URL and headers for the API\n",
    "base_url = \"https://mobile-rest-qa.nonprod.graingercloud.com/v1/product/detail\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Function to fetch and process data\n",
    "def fetch_product_details(skus):\n",
    "    params = {\n",
    "        \"partNumbers\": skus,\n",
    "        \"extraInfo\": \"false\"\n",
    "    }\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            results = []\n",
    "            for item in data:\n",
    "                brand = item.get(\"brand\", {}).get(\"name\", \"N/A\")\n",
    "                code = item.get(\"code\", \"N/A\")\n",
    "                name = item.get(\"name\", \"N/A\")\n",
    "                picture_url = item.get(\"pictureUrl600\", \"N/A\")\n",
    "                price = item.get(\"priceData\", {}).get(\"formattedPrice\", \"N/A\")\n",
    "                description = item.get(\"productDetailsDescription\", \"N/A\")\n",
    "\n",
    "                results.append({\n",
    "                    \"Brand\": brand,\n",
    "                    \"Code\": code,\n",
    "                    \"Name\": name,\n",
    "                    \"PictureUrl600\": picture_url,\n",
    "                    \"Price\": price,\n",
    "                    \"Description\": description\n",
    "                })\n",
    "\n",
    "            return pd.DataFrame(results) if results else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response for {skus}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to fetch details for {skus}: Status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Load the product codes from the JSON file\n",
    "with open('all_product_codes.json', 'r') as f:\n",
    "    product_codes = json.load(f)\n",
    "\n",
    "print(f\"Total product codes found: {len(product_codes)}\")\n",
    "\n",
    "# Product codes in chunks of 100\n",
    "chunk_size = 1\n",
    "chunks = [product_codes[i:i + chunk_size] for i in range(0, len(product_codes), chunk_size)]\n",
    "\n",
    "# Iterate over each chunk for API requests\n",
    "df = pd.DataFrame(columns=[\"Brand\", \"Code\", \"Name\", \"PictureUrl600\", \"Price\", \"Description\"])\n",
    "failed_chunks = []\n",
    "for chunk in chunks:\n",
    "    try:\n",
    "        details = fetch_product_details(chunk)\n",
    "        if details is not None:\n",
    "            df = pd.concat([df, details], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No details fetched for chunk: {chunk}. Removing from source.\")\n",
    "            failed_chunks.extend(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch details for chunk: {chunk}, Error: {e}\")\n",
    "        failed_chunks.extend(chunk)\n",
    "\n",
    "# # Remove failed product codes from the source list\n",
    "# product_codes = [code for code in product_codes if code not in failed_chunks]\n",
    "\n",
    "# # Save the updated product codes to the JSON file\n",
    "# with open('all_product_codes.json', 'w') as f:\n",
    "#     json.dump(product_codes, f, indent=4)\n",
    "\n",
    "# # Remove rows where all columns are NaN\n",
    "# df = df.dropna(how='all')\n",
    "\n",
    "# Ensure all column names are strings\n",
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "# Save to Parquet\n",
    "os.makedirs('processed', exist_ok=True)\n",
    "df.to_parquet('processed/grainger_products.parquet', index=False)\n",
    "print(\"Product details have been saved to 'processed/grainger_products.parquet'\")\n",
    "print(\"\\nHead of DataFrame:\")\n",
    "print(df.head(), \"\\n\")\n",
    "print(\"Tail of DataFrame:\")\n",
    "print(df.tail(), \"\\n\")\n",
    "print(\"Size of DataFrame:\", df.size, \"\\n\")\n",
    "print(\"Values in DataFrame:\")\n",
    "print(df.values)\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f5676aa9f97a6",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
