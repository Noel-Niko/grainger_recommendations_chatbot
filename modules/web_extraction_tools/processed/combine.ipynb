{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-06T14:26:06.532719Z",
     "start_time": "2024-07-06T14:26:00.968029Z"
    }
   },
   "source": "!pip install pandas\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/grainger_recommendations_chatbot_3-11/lib/python3.11/site-packages (1.5.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/envs/grainger_recommendations_chatbot_3-11/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/grainger_recommendations_chatbot_3-11/lib/python3.11/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/envs/grainger_recommendations_chatbot_3-11/lib/python3.11/site-packages (from pandas) (1.26.4)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/grainger_recommendations_chatbot_3-11/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T18:26:37.589850Z",
     "start_time": "2024-07-09T18:26:36.929296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def print_file_size(file_path):\n",
    "    size = os.path.getsize(file_path)\n",
    "    print(f\"Size of {file_path}: {size} bytes\")\n",
    "\n",
    "def combine_csv_files(file1, file2, output_file):\n",
    "    # Print size of each file before combining\n",
    "    print_file_size(file1)\n",
    "    print_file_size(file2)\n",
    "    \n",
    "    # Read the CSV files\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    \n",
    "    # Combine the dataframes\n",
    "    combined_df = pd.concat([df1, df2])\n",
    "    \n",
    "    # Save the combined dataframe to a new CSV file\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print size of the combined file\n",
    "    print_file_size(output_file)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Usage example\n",
    "combined_df = combine_csv_files('scraped.csv', 'scraped1.csv', 'scraped.csv')\n",
    "print(combined_df)\n"
   ],
   "id": "66802a8edf6d06b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of grainger_products.parquet: 228693 bytes\n",
      "Size of grainger_products_newer.parquet: 284889 bytes\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'str'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 29\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m combined_df\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Usage example\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m combined_df \u001B[38;5;241m=\u001B[39m combine_csv_files(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgrainger_products.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgrainger_products_newer.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnewest_grainger_products_combined.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(combined_df\u001B[38;5;241m.\u001B[39mhead())\n",
      "Cell \u001B[0;32mIn[1], line 18\u001B[0m, in \u001B[0;36mcombine_csv_files\u001B[0;34m(file1, file2, output_file)\u001B[0m\n\u001B[1;32m     15\u001B[0m df2 \u001B[38;5;241m=\u001B[39m file2 \u001B[38;5;66;03m#pd.read_csv(file2)\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Combine the dataframes\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m combined_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([df1, df2])\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Save the combined dataframe to a new CSV file\u001B[39;00m\n\u001B[1;32m     21\u001B[0m combined_df\u001B[38;5;241m.\u001B[39mto_csv(output_file, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/grainger_recommendations_chatbot-2/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[1;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[1;32m    330\u001B[0m     )\n\u001B[0;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/grainger_recommendations_chatbot-2/lib/python3.11/site-packages/pandas/core/reshape/concat.py:368\u001B[0m, in \u001B[0;36mconcat\u001B[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[0m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;129m@deprecate_nonkeyword_arguments\u001B[39m(version\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, allowed_args\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjs\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconcat\u001B[39m(\n\u001B[1;32m    148\u001B[0m     objs: Iterable[NDFrame] \u001B[38;5;241m|\u001B[39m Mapping[HashableT, NDFrame],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    157\u001B[0m     copy: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    158\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m    159\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;124;03m    Concatenate pandas objects along a particular axis.\u001B[39;00m\n\u001B[1;32m    161\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    366\u001B[0m \u001B[38;5;124;03m    1   3   4\u001B[39;00m\n\u001B[1;32m    367\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 368\u001B[0m     op \u001B[38;5;241m=\u001B[39m _Concatenator(\n\u001B[1;32m    369\u001B[0m         objs,\n\u001B[1;32m    370\u001B[0m         axis\u001B[38;5;241m=\u001B[39maxis,\n\u001B[1;32m    371\u001B[0m         ignore_index\u001B[38;5;241m=\u001B[39mignore_index,\n\u001B[1;32m    372\u001B[0m         join\u001B[38;5;241m=\u001B[39mjoin,\n\u001B[1;32m    373\u001B[0m         keys\u001B[38;5;241m=\u001B[39mkeys,\n\u001B[1;32m    374\u001B[0m         levels\u001B[38;5;241m=\u001B[39mlevels,\n\u001B[1;32m    375\u001B[0m         names\u001B[38;5;241m=\u001B[39mnames,\n\u001B[1;32m    376\u001B[0m         verify_integrity\u001B[38;5;241m=\u001B[39mverify_integrity,\n\u001B[1;32m    377\u001B[0m         copy\u001B[38;5;241m=\u001B[39mcopy,\n\u001B[1;32m    378\u001B[0m         sort\u001B[38;5;241m=\u001B[39msort,\n\u001B[1;32m    379\u001B[0m     )\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m op\u001B[38;5;241m.\u001B[39mget_result()\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/grainger_recommendations_chatbot-2/lib/python3.11/site-packages/pandas/core/reshape/concat.py:458\u001B[0m, in \u001B[0;36m_Concatenator.__init__\u001B[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001B[0m\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, (ABCSeries, ABCDataFrame)):\n\u001B[1;32m    454\u001B[0m         msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    455\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot concatenate object of type \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(obj)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    456\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124monly Series and DataFrame objs are valid\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    457\u001B[0m         )\n\u001B[0;32m--> 458\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m    460\u001B[0m     ndims\u001B[38;5;241m.\u001B[39madd(obj\u001B[38;5;241m.\u001B[39mndim)\n\u001B[1;32m    462\u001B[0m \u001B[38;5;66;03m# get the sample\u001B[39;00m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;66;03m# want the highest ndim that we have, and must be non-empty\u001B[39;00m\n\u001B[1;32m    464\u001B[0m \u001B[38;5;66;03m# unless all objs are empty\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: cannot concatenate object of type '<class 'str'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T18:48:08.753594Z",
     "start_time": "2024-07-09T18:48:08.726783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def print_file_size(file_path):\n",
    "    size = os.path.getsize(file_path)\n",
    "    print(f\"Size of {file_path}: {size} bytes\")\n",
    "\n",
    "def combine_parquet_files(file1, file2, output_file):\n",
    "    # Print size of each file before combining\n",
    "    print_file_size(file1)\n",
    "    print_file_size(file2)\n",
    "    \n",
    "    # Read the Parquet files\n",
    "    df1 = pd.read_parquet(file1)\n",
    "    df2 = pd.read_parquet(file2)\n",
    "    \n",
    "    # Combine the dataframes\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    \n",
    "    try:\n",
    "        # Save the combined dataframe to a new Parquet file\n",
    "        combined_df.to_parquet(output_file, index=False)\n",
    "        print(f\"Combined dataframe saved to {output_file}\")\n",
    "        \n",
    "        # Print size of the combined file\n",
    "        print_file_size(output_file)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving combined dataframe: {e}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Define the input file paths\n",
    "file1 = 'grainger_products.parquet'\n",
    "file2 = 'grainger_products_newer.parquet'\n",
    "\n",
    "output_file = 'combined_output.parquet'\n",
    "\n",
    "# Combine the Parquet files\n",
    "combined_df = combine_parquet_files(file1, file2, output_file)\n",
    "print(combined_df.head())\n"
   ],
   "id": "effc20ee2190f25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of grainger_products.parquet: 228693 bytes\n",
      "Size of grainger_products_newer.parquet: 284889 bytes\n",
      "Combined dataframe saved to modules/vector_index/v_i_processed/combined_output.parquet\n",
      "Size of modules/vector_index/v_i_processed/combined_output.parquet: 311603 bytes\n",
      "               Brand    Code  \\\n",
      "0       VF IMAGEWEAR   2KVP3   \n",
      "1            DRAEGER  29XM53   \n",
      "2             DAYTON   2TGD5   \n",
      "3            RED KAP  43A886   \n",
      "4  LEATHERHEAD TOOLS  45EG37   \n",
      "\n",
      "                                                Name  \\\n",
      "0  VF IMAGEWEAR Coverall: M ( 40 1/2 in x 42 in )...   \n",
      "1  DRAEGER Detector  tube: CO2, 0.5 to 10% Vol. M...   \n",
      "2  DAYTON Fire Damper: 10 in Nominal Duct Ht, 12 ...   \n",
      "3  RED KAP Mns Ls Cotton Coverall-Red: L ( 42 1/2...   \n",
      "4  LEATHERHEAD TOOLS Pike Pole, Dog Bone: 12 ft H...   \n",
      "\n",
      "                                       PictureUrl600    Price  \\\n",
      "0  https://static.grainger.com/rp/s/is/image/Grai...   $69.01   \n",
      "1  https://static.grainger.com/rp/s/is/image/Grai...  $175.14   \n",
      "2  https://static.grainger.com/rp/s/is/image/Grai...   $61.89   \n",
      "3  https://static.grainger.com/rp/s/is/image/Grai...   $59.92   \n",
      "4  https://static.grainger.com/rp/s/is/image/Grai...  $217.99   \n",
      "\n",
      "                                         Description  \n",
      "0     <p>Coveralls keep personal clothing clean.</p>  \n",
      "1                                                     \n",
      "2  <p>These fire dampers are designed for use in ...  \n",
      "3     <p>Coveralls keep personal clothing clean.</p>  \n",
      "4                                                     \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:01:57.687309Z",
     "start_time": "2024-07-09T19:01:57.678740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def combine_json_files(file1, file2, output_file):\n",
    "    # Read contents from file1\n",
    "    with open(file1, 'r') as f1:\n",
    "        data1 = json.load(f1)\n",
    "    \n",
    "    # Read contents from file2\n",
    "    with open(file2, 'r') as f2:\n",
    "        data2 = json.load(f2)\n",
    "    \n",
    "    # Combine the lists\n",
    "    combined_data = data1 + data2\n",
    "    \n",
    "    try: \n",
    "        # Write combined data to output file\n",
    "        with open(output_file, 'w') as out_f:\n",
    "            json.dump(combined_data, out_f, indent=4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving combined dataframe: {e}\")\n",
    "    \n",
    "    print(f\"Combined JSON data saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "combine_json_files('all_product_codes.json', 'all_product_codesold.json', 'combined_product_codes.json')\n"
   ],
   "id": "2a3fb4e728f1e84f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined JSON data saved to combined_product_codes.json\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b36af0c523020620"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
