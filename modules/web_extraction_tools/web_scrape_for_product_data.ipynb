{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def save_last_visited_url(url):\n",
    "    filename = 'start_url.json'\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump({'start_url': url}, file)\n",
    "        print(f\"Saved last visited url: {url} to {filename}\")\n",
    "\n",
    "# save_last_visited_url(\"https://www.grainger.com/category\")\n"
   ],
   "id": "1982860f03d420b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "domain = \"https://www.grainger.com\"\n",
    "filename = 'start_url.json'\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    start_url = data['start_url']\n",
    "    print(f\"start_url: {start_url}\")\n",
    "    \n",
    "local_domain = urlparse(domain).netloc"
   ],
   "id": "7a9dd792053e8726",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "### WEB CRAWLING THE GRAINGER SITE REQUIRES GRAINGER VPN ACCESS",
   "id": "629746066fbb4484",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # BREADTH-FIRST SEARCH\n",
    "# import time\n",
    "# import os\n",
    "# from urllib.parse import urlparse, urljoin\n",
    "# from collections import deque\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from bs4 import BeautifulSoup\n",
    "# import re\n",
    "# \n",
    "# # Selenium setup\n",
    "# options = Options()\n",
    "# options.add_argument(\"--headless\")\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "# service = Service(ChromeDriverManager().install())\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "# \n",
    "# # Function to get the hyperlinks from a URL using Selenium\n",
    "# def get_hyperlinks(url):\n",
    "#     print(f\"Getting hyperlinks for URL: {url}\")\n",
    "#     try:\n",
    "#         driver.get(url)\n",
    "#         time.sleep(1)  # Adjust wait time as needed based on page load speed\n",
    "#         html = driver.page_source\n",
    "#         soup = BeautifulSoup(html, 'html.parser')\n",
    "#         hyperlinks = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "#         return hyperlinks\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error getting hyperlinks: {e}\")\n",
    "#         return []\n",
    "# \n",
    "# # Function to crawl the website\n",
    "# def crawl(url):\n",
    "#     local_domain = urlparse(domain).netloc\n",
    "#     queue = deque([url])\n",
    "#     seen = {url}\n",
    "#     clean_links = []\n",
    "#     last_visited_url = None  \n",
    "# \n",
    "#     # Create necessary directories if they don't exist\n",
    "#     if not os.path.exists(\"text/\"):\n",
    "#         os.mkdir(\"text/\")\n",
    "#     if not os.path.exists(f\"text/{local_domain}/\"):\n",
    "#         os.mkdir(f\"text/{local_domain}/\")\n",
    "#     if not os.path.exists(\"processed\"):\n",
    "#         os.mkdir(\"processed\")\n",
    "# \n",
    "#     while queue and len(clean_links) < 1000:\n",
    "#         url = queue.popleft()  # Use popleft to ensure we process URLs in a breadth-first manner\n",
    "#         print(f\"Crawling URL: {url}\")\n",
    "#         try:\n",
    "#             # Construct file path for saving content\n",
    "#             file_path = f\"text/{local_domain}/{url[8:].replace('/', '_')}.txt\"\n",
    "#             with open(file_path, \"w\") as f:\n",
    "#                 driver.get(url)\n",
    "#                 time.sleep(3)  # Adjust wait time as needed based on page load speed\n",
    "#                 soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "#                 text = soup.get_text()\n",
    "#                 if \"You need to enable JavaScript to run this app.\" in text:\n",
    "#                     print(f\"Unable to parse page {url} due to JavaScript being required\")\n",
    "#                 f.write(text)\n",
    "#                 # print(f\"Writing: {text}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error crawling URL: {e}\")\n",
    "# \n",
    "#         try:\n",
    "#             new_links = get_domain_hyperlinks(local_domain, url, clean_links)\n",
    "#             for link in new_links:\n",
    "#                 if link not in seen:\n",
    "#                     queue.append(link)\n",
    "#                     seen.add(link)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing links: {e}\")\n",
    "# \n",
    "#     print(f\"Collected {len(clean_links)} clean links\")\n",
    "#     if last_visited_url:\n",
    "#         print(f\"Last visited URL: {last_visited_url}\")\n",
    "#         save_last_visited_url(last_visited_url)\n",
    "# \n",
    "# # Function to get the hyperlinks from a URL that are within the same domain and base URL\n",
    "# def get_domain_hyperlinks(local_domain, url, clean_links):\n",
    "#     # regex_pattern = re.compile(r'-[a-zA-Z0-9]+\\?.*')\n",
    "#     regex_pattern = re.compile(r'-((?=.*\\d.*\\d.*\\d)[A-Z0-9]{5,7})\\?.*')\n",
    "#     \n",
    "#     hyperlinks = get_hyperlinks(url)\n",
    "#     print(f\"Found {len(hyperlinks)} hyperlinks on {url}\")\n",
    "#     for link in set(hyperlinks):\n",
    "#         clean_link = None\n",
    "#         print(f\"Checking link: {link}\")\n",
    "# \n",
    "#         # Handle absolute URLs\n",
    "#         if link.startswith(\"http\"):\n",
    "#             url_obj = urlparse(link)\n",
    "#             if url_obj.netloc == local_domain and link.startswith(domain):\n",
    "#                 clean_link = link\n",
    "#         else:\n",
    "#             # Handle relative URLs\n",
    "#             clean_link = urljoin(url, link)\n",
    "# \n",
    "#         if clean_link is not None and '@' not in clean_link and regex_pattern.search(clean_link):\n",
    "#             if clean_link.endswith(\"/\"):\n",
    "#                 clean_link = clean_link[:-1]\n",
    "#             if clean_link not in clean_links:\n",
    "#                 print(f\"Adding clean link: {clean_link}\")\n",
    "#                 clean_links.append(clean_link)\n",
    "# \n",
    "#         # Stop collecting if we reach 1000 links\n",
    "#         if len(clean_links) >= 1000:\n",
    "#             last_visited_url = url  \n",
    "#             break\n",
    "# \n",
    "#     print(f\"Clean links: {clean_links}\")\n",
    "#     return list(set(hyperlinks))\n",
    "# \n",
    "# crawl(start_url)\n",
    "# \n",
    "# driver.quit()\n"
   ],
   "id": "f3ce3d6197e10e6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DEPTH FIRST SEARCH\n",
    "import time\n",
    "import os\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from collections import deque\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Selenium setup\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Function to get the hyperlinks from a URL using Selenium\n",
    "def get_hyperlinks(url):\n",
    "    print(f\"Getting hyperlinks for URL: {url}\")\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(1)  # Adjust wait time\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        hyperlinks = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "        return hyperlinks\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting hyperlinks: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to crawl the website using depth-first strategy\n",
    "def crawl(url):\n",
    "    local_domain = urlparse(url).netloc\n",
    "    stack = [url]  # Use a stack instead of a queue for depth-first\n",
    "    seen = {url}\n",
    "    clean_links = []\n",
    "    last_visited_url = None  # Variable to store the last visited URL\n",
    "\n",
    "    # Create necessary directories if they don't exist\n",
    "    if not os.path.exists(\"text/\"):\n",
    "        os.mkdir(\"text/\")\n",
    "    if not os.path.exists(f\"text/{local_domain}/\"):\n",
    "        os.mkdir(f\"text/{local_domain}/\")\n",
    "    if not os.path.exists(\"processed\"):\n",
    "        os.mkdir(\"processed\")\n",
    "\n",
    "    while stack and len(clean_links) < 100000:\n",
    "        url = stack.pop()  # Use pop() to ensure we process URLs in a depth-first manner\n",
    "        print(f\"Crawling URL: {url}\")\n",
    "        try:     \n",
    "            # Handle absolute URLs\n",
    "            if url.startswith(\"http://\") or url.startswith(\"https://\"):\n",
    "                url_domain = urlparse(url).netloc\n",
    "                if url_domain != local_domain:\n",
    "                    print(f\"Skipping URL: {url} (Not part of {url})\")\n",
    "                    continue\n",
    "\n",
    "            # Construct file path for saving content\n",
    "            file_path = f\"text/{local_domain}/{url[8:].replace('/', '_')}.txt\"\n",
    "            with open(file_path, \"w\") as f:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)  # Adjust wait time as needed based on page load speed\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                text = soup.get_text()\n",
    "                if \"You need to enable JavaScript to run this app.\" in text:\n",
    "                    print(f\"Unable to parse page {url} due to JavaScript being required\")\n",
    "                f.write(text)\n",
    "            last_visited_url = url  # Update the last visited URL\n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling URL: {e}\")\n",
    "\n",
    "        try:\n",
    "            new_links = get_domain_hyperlinks(local_domain, url, clean_links)\n",
    "            for link in new_links:\n",
    "                if link not in seen:\n",
    "                    stack.append(link)\n",
    "                    seen.add(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing links: {e}\")\n",
    "\n",
    "    print(f\"Collected {len(clean_links)} clean links\")\n",
    "    if last_visited_url:\n",
    "        print(f\"Last visited URL: {last_visited_url}\")\n",
    "        save_last_visited_url(last_visited_url)\n",
    "\n",
    "# Function to get the hyperlinks from a URL that are within the same domain and base URL\n",
    "def get_domain_hyperlinks(local_domain, url, clean_links):\n",
    "    regex_pattern = re.compile(r'/product/.*-[A-Z0-9]{5,7}(?=.*\\d.*\\d.*\\d)\\?.*')\n",
    "\n",
    "    hyperlinks = get_hyperlinks(url)\n",
    "    print(f\"Found {len(hyperlinks)} hyperlinks on {url}\")\n",
    "    for link in set(hyperlinks):\n",
    "        clean_link = None\n",
    "        print(f\"Checking link: {link}\")\n",
    "\n",
    "        # Handle absolute URLs\n",
    "        if link.startswith(\"http\"):\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain and link.startswith(url):\n",
    "                clean_link = link\n",
    "        else:\n",
    "            # Handle relative URLs\n",
    "            clean_link = urljoin(url, link)\n",
    "\n",
    "        if clean_link is not None and '@' not in clean_link and regex_pattern.search(clean_link):\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            if clean_link not in clean_links:\n",
    "                print(f\"Adding clean link: {clean_link}\")\n",
    "                clean_links.append(clean_link)\n",
    "\n",
    "        # Stop collecting if we reach n links\n",
    "        if len(clean_links) >= 100000:\n",
    "            break\n",
    "\n",
    "    print(f\"Clean links: {clean_links}\")\n",
    "    return list(set(hyperlinks))\n",
    "\n",
    "\n",
    "crawl(start_url)\n",
    "\n",
    "driver.quit()\n"
   ],
   "id": "29ca48b13f9b17da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_newlines(serie):\n",
    "    serie = serie.str.replace('\\n', ' ')\n",
    "    serie = serie.str.replace('\\\\n', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    return serie"
   ],
   "id": "254eb99084874ec5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def print_file_size(file_path):\n",
    "    size = os.path.getsize(file_path)\n",
    "    print(f\"Size of {file_path}: {size} bytes\")\n",
    "\n",
    "\n",
    "# Create a list to store the text files\n",
    "texts=[]\n",
    "\n",
    "domain_dir = os.path.join(\"text\", local_domain)\n",
    "abs_domain_dir = os.path.abspath(domain_dir)\n",
    "\n",
    "if os.path.exists(abs_domain_dir):\n",
    "    # Get all the text files in the text directory\n",
    "    for file in os.listdir(abs_domain_dir):\n",
    "\n",
    "        try:\n",
    "            # Open the file and read the text\n",
    "            with open(\"text/\" + local_domain + \"/\" + file, \"r\") as f:\n",
    "                text = f.read()\n",
    "\n",
    "                texts.append((file.replace('-', ' ').replace('_', ' ').replace('#update', ''), text))\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred during reading file '{text}': {e}\")       \n",
    "else:\n",
    "    print(f\"Directory '{abs_domain_dir}' does not exist.\")\n",
    "\n",
    "\n",
    "# Create a dataframe from the list of texts\n",
    "df = pd.DataFrame(texts, columns = ['fname', 'text'])\n",
    "\n",
    "# Set the text column to be the raw text with the newlines removed\n",
    "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
    "\n",
    "# Load the existing scraped.csv if it exists\n",
    "scraped_csv_path = 'processed/scraped.csv'\n",
    "if os.path.exists(scraped_csv_path):\n",
    "    existing_df = pd.read_csv(scraped_csv_path)\n",
    "    print_file_size(scraped_csv_path)\n",
    "    # Append the new data to the existing data\n",
    "    combined_df = pd.concat([existing_df, df])\n",
    "else:\n",
    "    combined_df = df\n",
    "\n",
    "\n",
    "# Remove duplicates\n",
    "combined_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Fill any empty columns with empty strings\n",
    "combined_df.fillna(\"\", inplace=True)\n",
    "\n",
    "# Save the combined dataframe to scraped.csv\n",
    "combined_df.to_csv(scraped_csv_path, index=False)\n",
    "\n",
    "# Print size of the combined file\n",
    "print_file_size(scraped_csv_path)\n",
    "\n",
    "df = combined_df\n",
    "df.to_csv(\"processed/scraped.csv\")\n",
    "df.head()"
   ],
   "id": "f920f8defadbf841",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the dataframe from the CSV file\n",
    "df = pd.read_csv('processed/scraped.csv')\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Calculate the number of tokens for each text\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "# Visualize the distribution of the number of tokens per row using a histogram\n",
    "df.n_tokens.hist()\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Tokens per Text')\n",
    "plt.show()\n"
   ],
   "id": "7b49919d40052d4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "64bf4dd30f8615eb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
