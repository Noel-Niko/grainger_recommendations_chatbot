{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T01:42:40.288948Z",
     "start_time": "2024-06-24T01:42:38.513472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import time\n",
    "# import requests\n",
    "# import re\n",
    "# import urllib.request\n",
    "# from bs4 import BeautifulSoup\n",
    "# from collections import deque\n",
    "# from html.parser import HTMLParser\n",
    "# from urllib.parse import urlparse\n",
    "# import os\n",
    "# \n",
    "# # headers = {\n",
    "# #     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "# #     'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "# #     'Accept-Language': 'en-US,en;q=0.5',\n",
    "# #     'Accept-Encoding': 'gzip, deflate, br',\n",
    "# #     'Connection': 'keep-alive',\n",
    "# #     'Upgrade-Insecure-Requests': '1',\n",
    "# # }\n",
    "# # \n",
    "# # \n",
    "# # # Function to get the hyperlinks from a URL\n",
    "# # def get_hyperlinks(url):\n",
    "# #     try:\n",
    "# #         req = urllib.request.Request(url, headers=headers)\n",
    "# #         with urllib.request.urlopen(req) as response:\n",
    "# #             if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "# #                 return []\n",
    "# #             html = response.read().decode('utf-8')\n",
    "# #     except Exception as e:\n",
    "# #         print(e)\n",
    "# #         return []\n",
    "# # \n",
    "# #     parser = HyperlinkParser()\n",
    "# #     parser.feed(html)\n",
    "# #     return parser.hyperlinks\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# \n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless\")\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "# \n",
    "# # Initialize the Service with the path to the ChromeDriver executable\n",
    "# service = Service(ChromeDriverManager().install())\n",
    "# \n",
    "# # Pass the Service object to the WebDriver constructor\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "# \n",
    "# def get_hyperlinks(url):\n",
    "#     driver.get(url)\n",
    "#     time.sleep(2)  # Adjust based on page load time\n",
    "#     html = driver.page_source\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "#     hyperlinks = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "#     return hyperlinks\n",
    "# \n",
    "# \n",
    "# # Regex pattern to match a URL\n",
    "# HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "# \n",
    "# # Class to parse the HTML and get the hyperlinks\n",
    "# class HyperlinkParser(HTMLParser):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # Create a list to store the hyperlinks\n",
    "#         self.hyperlinks = []\n",
    "# \n",
    "#     # Override the HTMLParser's handle_starttag method to get the hyperlinks\n",
    "#     def handle_starttag(self, tag, attrs):\n",
    "#         attrs = dict(attrs)\n",
    "# \n",
    "#         # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n",
    "#         if tag == \"a\" and \"href\" in attrs:\n",
    "#             self.hyperlinks.append(attrs[\"href\"])\n",
    "# \n",
    "# \n",
    "# # Function to get the hyperlinks from a URL that are within the same domain\n",
    "# def get_domain_hyperlinks(local_domain, url):\n",
    "#     clean_links = []\n",
    "#     for link in set(get_hyperlinks(url)):\n",
    "#         clean_link = None\n",
    "# \n",
    "#     \n",
    "#         # If the link is a URL, check if it is within the same domain\n",
    "#         try: \n",
    "#             if re.search(HTTP_URL_PATTERN, link):\n",
    "#                 # Parse the URL and check if the domain is the same\n",
    "#                 url_obj = urlparse(link)\n",
    "#                 if url_obj.netloc == local_domain:\n",
    "#                     clean_link = link\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#                  \n",
    "# \n",
    "#         # If the link is not a URL, check if it is a relative link\n",
    "#         else:\n",
    "#             try:\n",
    "#                 if link.startswith(\"/\"):\n",
    "#                     link = link[1:]\n",
    "#                 elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "#                     continue\n",
    "#                 clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "# \n",
    "#         if clean_link is not None:\n",
    "#             try:\n",
    "#                 if clean_link.endswith(\"/\"):\n",
    "#                     clean_link = clean_link[:-1]\n",
    "#                 clean_links.append(clean_link)\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "# \n",
    "#     # Return the list of hyperlinks that are within the same domain\n",
    "#     return list(set(clean_links))\n",
    "# \n",
    "# def get_first_five_links(url):\n",
    "#     seen = set()\n",
    "#     queue = []\n",
    "#     link_count = 0 \n",
    "#     \n",
    "#     for link in get_hyperlinks(url): \n",
    "#         try:\n",
    "#             if link not in seen and link_count < 5: \n",
    "#                 queue.append(link)\n",
    "#                 seen.add(link)\n",
    "#                 link_count += 1 \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing link {link}: {e}\")\n",
    "#     \n",
    "#     return queue[:5] \n",
    "# \n",
    "# \n",
    "# def create_directories_for_crawl(url):\n",
    "#     # Parse the URL and get the domain\n",
    "#     local_domain = urlparse(url).netloc\n",
    "# \n",
    "#     # Create a directory to store the text files\n",
    "#     if not os.path.exists(\"text/\"):\n",
    "#         os.mkdir(\"text/\")\n",
    "#     if not os.path.exists(\"text/\" + local_domain + \"/\"):\n",
    "#         os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "# \n",
    "#     # Create a directory to store the csv files\n",
    "#     if not os.path.exists(\"processed\"):\n",
    "#         os.mkdir(\"processed\")\n",
    "# \n",
    "# def crawl(url):\n",
    "#     # Create a queue to store the URLs to crawl\n",
    "#     queue = deque([url])\n",
    "# \n",
    "#     # Create a set to store the URLs that have already been seen (no duplicates)\n",
    "#     seen = {url}\n",
    "# \n",
    "#     # Call create_directories_for_crawl with the initial URL to ensure directories are created\n",
    "#     create_directories_for_crawl(url)\n",
    "# \n",
    "#     # While the queue is not empty, continue crawling\n",
    "#     while queue:\n",
    "# \n",
    "#         # Get the next URL from the queue\n",
    "#         url = queue.popleft()  # Changed from pop() to popleft() to maintain order\n",
    "#         print(url)  # for debugging and to see the progress\n",
    "#         try:\n",
    "#             # Save text from the url to a <url>.txt file\n",
    "#             save_page_content_to_file(url)\n",
    "#         except Exception as e:\n",
    "#             print(e)  \n",
    "# \n",
    "#         try:    \n",
    "#             # Get the first five hyperlinks from the URL and add them to the queue\n",
    "#             first_five_links = get_first_five_links(url)\n",
    "#             for link in first_five_links:\n",
    "#                 if link not in seen:\n",
    "#                     queue.append(link)\n",
    "#                     seen.add(link)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "# \n",
    "# def save_page_content_to_file(url):\n",
    "#     # Parse the URL and get the domain\n",
    "#     local_domain = urlparse(url).netloc\n",
    "# \n",
    "#     # Create directories to store the text files if they don't exist\n",
    "#     create_directories_for_crawl(url)\n",
    "# \n",
    "#     try:\n",
    "#         # Get the text from the URL using BeautifulSoup\n",
    "#         soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "# \n",
    "#         # Get the text but remove the tags\n",
    "#         text = soup.get_text()\n",
    "# \n",
    "#         # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n",
    "#         if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "#             print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "#         \n",
    "#         # Write the text to the file in the text directory\n",
    "#         with open('text/' + local_domain + '/' + url.replace(\"http://\", \"\").replace(\"https://\", \"\").replace(\"/\", \"_\") + \".txt\", \"w\") as f:\n",
    "#             f.write(text)\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "# \n",
    "# \n"
   ],
   "id": "c38229cdb3c30ee4",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T02:41:15.793948Z",
     "start_time": "2024-06-24T02:41:14.379539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# Initialize the Service with the path to the ChromeDriver executable\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# Pass the Service object to the WebDriver constructor\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def get_hyperlinks(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Adjust based on page load time\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    hyperlinks = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    return hyperlinks\n",
    "\n",
    "\n",
    "# Regex pattern to match a URL\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# Define root domain to crawl\n",
    "# domain = \"ci.hartford.wi.us\"\n",
    "domain = \"www.grainger.com\"\n",
    "# Full URL including the specific path to start crawling from\n",
    "# full_url = \"https://ci.hartford.wi.us/440/Experience-Downtown\"\n",
    "full_url = \"https://www.grainger.com/\"\n",
    "\n",
    "# Class to parse the HTML and get the hyperlinks\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a list to store the hyperlinks\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "\n",
    "# Function to get the hyperlinks from a URL that are within the same domain\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "\n",
    "        # If the link is a URL, check if it is within the same domain\n",
    "        try: \n",
    "            if re.search(HTTP_URL_PATTERN, link):\n",
    "                # Parse the URL and check if the domain is the same\n",
    "                url_obj = urlparse(link)\n",
    "                if url_obj.netloc == local_domain:\n",
    "                    clean_link = link\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "        # If the link is not a URL, check if it is a relative link\n",
    "        else:\n",
    "            try:\n",
    "                if link.startswith(\"/\"):\n",
    "                    link = link[1:]\n",
    "                elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                    continue\n",
    "                clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        if clean_link is not None:\n",
    "            try:\n",
    "                if clean_link.endswith(\"/\"):\n",
    "                    clean_link = clean_link[:-1]\n",
    "                clean_links.append(clean_link)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    # Return the list of hyperlinks that are within the same domain\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def crawl(url, product):\n",
    "    # Parse the URL and get the domain\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # Create a queue to store the URLs to crawl\n",
    "    queue = deque([url])\n",
    "\n",
    "    # Create a set to store the URLs that have already been seen (no duplicates)\n",
    "    seen = {url}\n",
    "\n",
    "    # Create a directory to store the text files\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # Create a directory to store the csv files\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # While the queue is not empty, continue crawling\n",
    "    while queue:\n",
    "\n",
    "        # Get the next URL from the queue\n",
    "        url = queue.pop()\n",
    "        print(url) # for debugging and to see the progress\n",
    "        try:\n",
    "            # Save text from the url to a <url>.txt file\n",
    "            with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\") as f:\n",
    "\n",
    "                # Get the text from the URL using BeautifulSoup\n",
    "                soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "                # Get the text but remove the tags\n",
    "                text = soup.get_text()\n",
    "\n",
    "                # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n",
    "                if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                    print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "\n",
    "                # Otherwise, write the text to the file in the text directory\n",
    "                f.write(text)\n",
    "        except Exception as e:\n",
    "            print(e)  \n",
    "\n",
    "        try:    \n",
    "            # Get the hyperlinks from the URL and add them to the queue\n",
    "            for link in get_domain_hyperlinks(local_domain, url):\n",
    "                try:\n",
    "                     if product in link and \"search\" not in link and link not in seen:\n",
    "                        queue.append(link)\n",
    "                        seen.add(link)\n",
    "                except Exception as e:\n",
    "                    print(e)          \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n"
   ],
   "id": "e583cf7a3d537404",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T02:40:56.300201Z",
     "start_time": "2024-06-24T02:40:56.292660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import time\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urlparse, urljoin\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from collections import deque\n",
    "# import os\n",
    "# import re\n",
    "# \n",
    "# # Selenium setup\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless\")\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "# service = Service(ChromeDriverManager().install())\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "# \n",
    "# def get_hyperlinks(url):\n",
    "#     driver.get(url)\n",
    "#     time.sleep(2)  # Adjust based on page load time\n",
    "#     html = driver.page_source\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "#     hyperlinks = [urljoin(url, a['href']) for a in soup.find_all('a', href=True)]\n",
    "#     return hyperlinks\n",
    "# \n",
    "# HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "# \n",
    "# def get_domain_hyperlinks(local_domain, url):\n",
    "#     clean_links = []\n",
    "#     for link in set(get_hyperlinks(url)):\n",
    "#         try:\n",
    "#             if re.search(HTTP_URL_PATTERN, link):\n",
    "#                 url_obj = urlparse\n"
   ],
   "id": "cb239fffc717bc41",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3219616653.py, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[26], line 35\u001B[0;36m\u001B[0m\n\u001B[0;31m    \u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m incomplete input\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T02:41:46.858562Z",
     "start_time": "2024-06-24T02:41:32.254573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import time\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urlparse, urljoin\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from collections import deque\n",
    "# import os\n",
    "# import re\n",
    "# \n",
    "# # Selenium setup\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless\")\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "# service = Service(ChromeDriverManager().install())\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "# \n",
    "# def get_hyperlinks(url):\n",
    "#     driver.get(url)\n",
    "#     time.sleep(2)  # Adjust based on page load time\n",
    "#     html = driver.page_source\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "#     hyperlinks = [urljoin(url, a['href']) for a in soup.find_all('a', href=True)]\n",
    "#     return hyperlinks\n",
    "# \n",
    "# HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "# \n",
    "# def get_domain_hyperlinks(local_domain, url):\n",
    "#     clean_links = []\n",
    "#     for link in set(get_hyperlinks(url)):\n",
    "#         try:\n",
    "#             if re.search(HTTP_URL_PATTERN, link):\n",
    "#                 url_obj = urlparse(link)\n",
    "#                 if local_domain in url_obj.netloc:\n",
    "#                     clean_links.append(link)\n",
    "#             else:\n",
    "#                 if link.startswith(\"/\"):\n",
    "#                     clean_links.append(urljoin(url, link))\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing link {link}: {e}\")\n",
    "# \n",
    "#     return list(set(clean_links))\n",
    "# \n",
    "# def crawl(url, product):\n",
    "#     local_domain = urlparse(url).netloc\n",
    "#     queue = deque([url])\n",
    "#     seen = {url}\n",
    "# \n",
    "#     if not os.path.exists(\"text/\"):\n",
    "#         os.mkdir(\"text/\")\n",
    "#     if not os.path.exists(f\"text/{local_domain}/\"):\n",
    "#         os.mkdir(f\"text/{local_domain}/\")\n",
    "#     if not os.path.exists(\"processed\"):\n",
    "#         os.mkdir(\"processed\")\n",
    "# \n",
    "#     results = []\n",
    "# \n",
    "#     while queue:\n",
    "#         url = queue.pop()\n",
    "#         print(f\"Crawling URL: {url}\")  # Debugging progress\n",
    "#         try:\n",
    "#             response = requests.get(url)\n",
    "#             soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#             text = soup.get_text()\n",
    "# \n",
    "#             if \"You need to enable JavaScript to run this app.\" in text:\n",
    "#                 print(f\"Unable to parse page due to JavaScript requirement: {url}\")\n",
    "#                 continue\n",
    "# \n",
    "#             if product in url:\n",
    "#                 results.append(url)\n",
    "# \n",
    "#             with open(f'text/{local_domain}/{url[8:].replace(\"/\", \"_\")}.txt', \"w\") as f:\n",
    "#                 f.write(text)\n",
    "# \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error fetching {url}: {e}\")\n",
    "# \n",
    "#         try:\n",
    "#             for link in get_domain_hyperlinks(local_domain, url):\n",
    "#                 if link not in seen:\n",
    "#                     queue.append(link)\n",
    "#                     seen.add(link)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing hyperlinks for {url}: {e}\")\n",
    "# \n",
    "#     return results\n",
    "# \n",
    "# product = '1VCE8'\n",
    "# urls = [f'https://www.zoro.com/search?q={product}']\n",
    "# all_reviews = []\n",
    "# \n",
    "# for url in urls:\n",
    "#     reviews = crawl(url, product)\n",
    "#     all_reviews.extend(reviews)\n",
    "# \n",
    "# print(\"Product URLs found:\", all_reviews)\n"
   ],
   "id": "d9cf9a791b79409c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling URL: https://www.zoro.com/search?q=1VCE8\n",
      "Crawling URL: https://www.zoro.com/test-instruments-gauges/c/24/\n",
      "Crawling URL: https://www.zoro.com/borescope/c/7066/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 95\u001B[0m\n\u001B[1;32m     92\u001B[0m all_reviews \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m url \u001B[38;5;129;01min\u001B[39;00m urls:\n\u001B[0;32m---> 95\u001B[0m     reviews \u001B[38;5;241m=\u001B[39m \u001B[43mcrawl\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     all_reviews\u001B[38;5;241m.\u001B[39mextend(reviews)\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProduct URLs found:\u001B[39m\u001B[38;5;124m\"\u001B[39m, all_reviews)\n",
      "Cell \u001B[0;32mIn[28], line 81\u001B[0m, in \u001B[0;36mcrawl\u001B[0;34m(url, product)\u001B[0m\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError fetching \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 81\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m link \u001B[38;5;129;01min\u001B[39;00m \u001B[43mget_domain_hyperlinks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocal_domain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m     82\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m link \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m seen:\n\u001B[1;32m     83\u001B[0m             queue\u001B[38;5;241m.\u001B[39mappend(link)\n",
      "Cell \u001B[0;32mIn[28], line 31\u001B[0m, in \u001B[0;36mget_domain_hyperlinks\u001B[0;34m(local_domain, url)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_domain_hyperlinks\u001B[39m(local_domain, url):\n\u001B[1;32m     30\u001B[0m     clean_links \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 31\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m link \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mset\u001B[39m(\u001B[43mget_hyperlinks\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m):\n\u001B[1;32m     32\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     33\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m re\u001B[38;5;241m.\u001B[39msearch(HTTP_URL_PATTERN, link):\n",
      "Cell \u001B[0;32mIn[28], line 20\u001B[0m, in \u001B[0;36mget_hyperlinks\u001B[0;34m(url)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_hyperlinks\u001B[39m(url):\n\u001B[0;32m---> 20\u001B[0m     \u001B[43mdriver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# Adjust based on page load time\u001B[39;00m\n\u001B[1;32m     22\u001B[0m     html \u001B[38;5;241m=\u001B[39m driver\u001B[38;5;241m.\u001B[39mpage_source\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:363\u001B[0m, in \u001B[0;36mWebDriver.get\u001B[0;34m(self, url)\u001B[0m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, url: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    362\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 363\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCommand\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGET\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43murl\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:352\u001B[0m, in \u001B[0;36mWebDriver.execute\u001B[0;34m(self, driver_command, params)\u001B[0m\n\u001B[1;32m    349\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msessionId\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m params:\n\u001B[1;32m    350\u001B[0m         params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msessionId\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msession_id\n\u001B[0;32m--> 352\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcommand_executor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdriver_command\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response:\n\u001B[1;32m    354\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_handler\u001B[38;5;241m.\u001B[39mcheck_response(response)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/selenium/webdriver/remote/remote_connection.py:302\u001B[0m, in \u001B[0;36mRemoteConnection.execute\u001B[0;34m(self, command, params)\u001B[0m\n\u001B[1;32m    300\u001B[0m trimmed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_trim_large_entries(params)\n\u001B[1;32m    301\u001B[0m LOGGER\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, command_info[\u001B[38;5;241m0\u001B[39m], url, \u001B[38;5;28mstr\u001B[39m(trimmed))\n\u001B[0;32m--> 302\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand_info\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/selenium/webdriver/remote/remote_connection.py:322\u001B[0m, in \u001B[0;36mRemoteConnection._request\u001B[0;34m(self, method, url, body)\u001B[0m\n\u001B[1;32m    319\u001B[0m     body \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeep_alive:\n\u001B[0;32m--> 322\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    323\u001B[0m     statuscode \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mstatus\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/urllib3/_request_methods.py:144\u001B[0m, in \u001B[0;36mRequestMethods.request\u001B[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001B[0m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_encode_url(\n\u001B[1;32m    137\u001B[0m         method,\n\u001B[1;32m    138\u001B[0m         url,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    141\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39murlopen_kw,\n\u001B[1;32m    142\u001B[0m     )\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest_encode_body\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfields\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfields\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43murlopen_kw\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/urllib3/_request_methods.py:279\u001B[0m, in \u001B[0;36mRequestMethods.request_encode_body\u001B[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001B[0m\n\u001B[1;32m    275\u001B[0m     extra_kw[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContent-Type\u001B[39m\u001B[38;5;124m\"\u001B[39m, content_type)\n\u001B[1;32m    277\u001B[0m extra_kw\u001B[38;5;241m.\u001B[39mupdate(urlopen_kw)\n\u001B[0;32m--> 279\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mextra_kw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/urllib3/poolmanager.py:443\u001B[0m, in \u001B[0;36mPoolManager.urlopen\u001B[0;34m(self, method, url, redirect, **kw)\u001B[0m\n\u001B[1;32m    441\u001B[0m     response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(method, url, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    442\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 443\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mu\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest_uri\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    445\u001B[0m redirect_location \u001B[38;5;241m=\u001B[39m redirect \u001B[38;5;129;01mand\u001B[39;00m response\u001B[38;5;241m.\u001B[39mget_redirect_location()\n\u001B[1;32m    446\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m redirect_location:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    786\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[1;32m    805\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    534\u001B[0m \u001B[38;5;66;03m# Receive the response from the server\u001B[39;00m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 536\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    538\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/site-packages/urllib3/connection.py:464\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresponse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPResponse\n\u001B[1;32m    463\u001B[0m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[0;32m--> 464\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    466\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    467\u001B[0m     assert_header_parsing(httplib_response\u001B[38;5;241m.\u001B[39mmsg)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/http/client.py:1375\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1374\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1375\u001B[0m         \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1376\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[1;32m   1377\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/http/client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 318\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[1;32m    320\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/http/client.py:279\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 279\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/python-web-crawler-2/lib/python3.10/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    707\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T02:37:08.790387Z",
     "start_time": "2024-06-24T02:37:01.187427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import time\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from bs4 import BeautifulSoup\n",
    "# \n",
    "# # Selenium setup\n",
    "# options = Options()\n",
    "# options.add_argument(\"--headless\")\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "# service = Service(ChromeDriverManager().install())\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "# \n",
    "# def search_product(product_id):\n",
    "#     # Search URL for the product\n",
    "#     search_url = f'https://www.zoro.com/search?q={product_id}'\n",
    "#     driver.get(search_url)\n",
    "#     time.sleep(2)  # Wait for the page to load\n",
    "# \n",
    "#     html = driver.page_source\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "# \n",
    "#     # Find the specific product link\n",
    "#     product_link = None\n",
    "#     for a in soup.find_all('a', href=True, class_='product-card-image__link'):\n",
    "#         if product_id in a['href']:\n",
    "#             product_link = urljoin(search_url, a['href'])\n",
    "#             break\n",
    "# \n",
    "#     if product_link:\n",
    "#         print(f\"Found product URL: {product_link}\")\n",
    "#     else:\n",
    "#         print(\"Product URL not found.\")\n",
    "# \n",
    "#     return product_link\n",
    "# \n",
    "# product_id = '1VCE8'\n",
    "# product_url = search_product(product_id)\n",
    "# \n",
    "# driver.quit()\n"
   ],
   "id": "c5ce42f8ef5ffe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product URL not found.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T02:39:48.578785Z",
     "start_time": "2024-06-24T02:39:38.363806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import time\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urljoin\n",
    "# \n",
    "# # Selenium setup\n",
    "# options = Options()\n",
    "# options.add_argument(\"--headless\")\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "# service = Service(ChromeDriverManager().install())\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "# \n",
    "# def search_product(product_id):\n",
    "#     # Search URL for the product\n",
    "#     search_url = f'https://www.zoro.com/search?q={product_id}'\n",
    "#     driver.get(search_url)\n",
    "#     time.sleep(5)  # Wait for the page to load\n",
    "# \n",
    "#     # Get the page source after it loads\n",
    "#     html = driver.page_source\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "# \n",
    "#     # Log the HTML content for debugging\n",
    "#     with open('page_source.html', 'w', encoding='utf-8') as file:\n",
    "#         file.write(soup.prettify())\n",
    "# \n",
    "#     # Find the specific product link\n",
    "#     product_link = None\n",
    "#     for a in soup.select('a.product-card-image__link'):\n",
    "#         if product_id in a['href']:\n",
    "#             product_link = urljoin(search_url, a['href'])\n",
    "#             break\n",
    "# \n",
    "#     if product_link:\n",
    "#         print(f\"Found product URL: {product_link}\")\n",
    "#     else:\n",
    "#         print(\"Product URL not found.\")\n",
    "# \n",
    "#     return product_link\n",
    "# \n",
    "# product_id = '1VCE8'\n",
    "# product_url = search_product(product_id)\n",
    "# \n",
    "# driver.quit()\n"
   ],
   "id": "5178f2954a41abc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product URL not found.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T02:43:24.311680Z",
     "start_time": "2024-06-24T02:43:13.915078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import time\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urljoin\n",
    "# \n",
    "# # Selenium setup\n",
    "# options = Options()\n",
    "# options.add_argument(\"--headless\")\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "# service = Service(ChromeDriverManager().install())\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "# \n",
    "# def search_product(product_id):\n",
    "#     # Search URL for the product\n",
    "#     search_url = f'https://www.zoro.com/search?q={product_id}'\n",
    "#     driver.get(search_url)\n",
    "#     time.sleep(5)  # Wait for the page to load\n",
    "# \n",
    "#     # Get the page source after it loads\n",
    "#     html = driver.page_source\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "# \n",
    "#     # Find the specific product link\n",
    "#     product_link = None\n",
    "#     for a in soup.find_all('a', href=True):\n",
    "#         if product_id in a['href']:\n",
    "#             product_link = urljoin(search_url, a['href'])\n",
    "#             break\n",
    "# \n",
    "#     if product_link:\n",
    "#         print(f\"Found product URL: {product_link}\")\n",
    "#     else:\n",
    "#         print(\"Product URL not found.\")\n",
    "# \n",
    "#     return product_link\n",
    "# \n",
    "# product_id = '1VCE8'\n",
    "# product_url = search_product(product_id)\n",
    "# \n",
    "# driver.quit()\n"
   ],
   "id": "57df58624d0e1824",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found product URL: https://www.zoro.com/search?q=1VCE8&fqc%3Acategory=30#search-container\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T02:50:24.209671Z",
     "start_time": "2024-06-24T02:50:12.479029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Selenium setup\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def search_product(product_id):\n",
    "    # Search URL for the product\n",
    "    search_url = f'https://www.zoro.com/search?q={product_id}'\n",
    "    driver.get(search_url)\n",
    "    time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "    # Get the page source after it loads\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the specific product link using a precise CSS selector\n",
    "    product_link = None\n",
    "    for a in soup.select('a.product-card-image__link'):\n",
    "        if product_id.lower() in a['href'].lower():\n",
    "            product_link = urljoin(search_url, a['href'])\n",
    "            break\n",
    "\n",
    "    if product_link:\n",
    "        print(f\"Found product URL: {product_link}\")\n",
    "    else:\n",
    "        print(\"Product URL not found.\")\n",
    "\n",
    "    return product_link\n",
    "\n",
    "product_id = '1VCE9'\n",
    "product_url = search_product(product_id)\n",
    "\n",
    "driver.quit()\n"
   ],
   "id": "7814770bc1fce0cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found product URL: https://www.zoro.com/dayton-standard-duty-industrial-fan-30-non-oscillating-115vac-88009800-cfm-1vce9/i/G2998721/\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T03:35:49.859475Z",
     "start_time": "2024-06-24T03:35:38.211399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "# def extract_reviews(html_content):\n",
    "#     if not html_content:\n",
    "#         print(\"No HTML content to process.\")\n",
    "#         return []\n",
    "# \n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "#     review_sections = soup.find_all('div', class_='pr-review pr-review-condensed')\n",
    "# \n",
    "#     reviews_data = []\n",
    "# \n",
    "#     for idx, review in enumerate(review_sections, start=1):\n",
    "#         print(f\"\\nProcessing review {idx}:\")\n",
    "#         \n",
    "#         # Extracting star rating\n",
    "#         star_rating = review.find('div', class_='pr-rating-stars')\n",
    "#         if star_rating and 'aria-label' in star_rating.attrs:  # Check if 'aria-label' exists in attrs\n",
    "#             print(\"Star Rating:\", star_rating['aria-label'])\n",
    "#         else:\n",
    "#             print(\"Star Rating not found.\")\n",
    "#             continue\n",
    "#         \n",
    "#         # Extracting review headline\n",
    "#         review_headline = review.find('h5', class_='pr-rd-review-headline')\n",
    "#         if review_headline:\n",
    "#             print(\"Review Headline:\", review_headline.text.strip())\n",
    "#         \n",
    "#         # Extracting submission details\n",
    "#         submission_details = review.find('p', class_='pr-rd-author-submission-date')\n",
    "#         if submission_details:\n",
    "#             print(\"Submission Details:\", submission_details.text.strip())\n",
    "#         \n",
    "#         # Extracting reviewer details\n",
    "#         reviewer_details = review.find('p', class_='pr-rd-author-nickname')\n",
    "#         if reviewer_details:\n",
    "#             print(\"Reviewer Details:\", reviewer_details.text.strip())\n",
    "#         \n",
    "#         # Extracting review text\n",
    "#         review_text = review.find('p', class_='pr-rd-description-text')\n",
    "#         if review_text:\n",
    "#             print(\"Review Text:\", review_text.text.strip())\n",
    "#         \n",
    "#         # Adding all data to reviews_data\n",
    "#         reviews_data.append({\n",
    "#             'Star Rating': star_rating['aria-label'].strip() if star_rating and 'aria-label' in star_rating.attrs else None,\n",
    "#             'Review Headline': review_headline.text.strip() if review_headline else None,\n",
    "#             'Submission Details': submission_details.text.strip() if submission_details else None,\n",
    "#             'Reviewer Details': reviewer_details.text.strip() if reviewer_details else None,\n",
    "#             'Review Text': review_text.text.strip() if review_text else None\n",
    "#         })\n",
    "# \n",
    "#     return reviews_data\n",
    "\n",
    "def extract_reviews(html_content):\n",
    "    if not html_content:\n",
    "        print(\"No HTML content to process.\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Extracting star rating\n",
    "    star_rating_container = soup.find('section', class_='pr-review-snapshot-block-snippet')\n",
    "    if star_rating_container:\n",
    "        star_rating = star_rating_container.find('div', class_='pr-snippet-stars')\n",
    "        star_rating_text = star_rating_container.find('div', class_='pr-snippet-rating-decimal').text.strip()\n",
    "        if star_rating:\n",
    "            star_rating_label = star_rating['aria-label']\n",
    "            print(f\"Star Rating: {star_rating_label}, {star_rating_text}\")\n",
    "        else:\n",
    "            print(\"Star Rating not found.\")\n",
    "    else:\n",
    "        print(\"Star Rating container not found.\")\n",
    "\n",
    "    # Extracting recommendation percentage\n",
    "    recommendation_section = soup.find('section', class_='pr-review-snapshot-block-recommend')\n",
    "    if recommendation_section:\n",
    "        recommendation_percent = recommendation_section.find('span', class_='pr-reco-value').text.strip()\n",
    "        print(f\"Recommendation Percentage: {recommendation_percent}\")\n",
    "    else:\n",
    "        print(\"Recommendation section not found.\")\n",
    "\n",
    "    # Extracting reviews\n",
    "    reviews = soup.find_all('section', class_='pr-rd-content-block')\n",
    "    reviews_data = []\n",
    "\n",
    "    for idx, review in enumerate(reviews, start=1):\n",
    "        print(f\"\\nProcessing review {idx}:\")\n",
    "        \n",
    "        # Extracting review text\n",
    "        review_text = review.find('p', class_='pr-rd-description-text')\n",
    "        if review_text:\n",
    "            print(\"Review Text:\", review_text.text.strip())\n",
    "        else:\n",
    "            print(\"Review Text not found.\")\n",
    "        \n",
    "        # Adding review data\n",
    "        reviews_data.append({\n",
    "            'Star Rating': star_rating_label if star_rating else None,\n",
    "            'Rating Text': star_rating_text if star_rating_text else None,\n",
    "            'Review Text': review_text.text.strip() if review_text else None\n",
    "        })\n",
    "\n",
    "    return reviews_data\n",
    "\n",
    "\n",
    "# Selenium setup\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def get_page_soup(url):\n",
    "    print(f\"Getting page soup for URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Wait for the page to load\n",
    "    html = driver.page_source\n",
    "    return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "def search_product(product_id):\n",
    "    print(f\"Searching for product ID: {product_id}\")\n",
    "    search_url = f'https://www.zoro.com/search?q={product_id}'\n",
    "    soup = get_page_soup(search_url)\n",
    "\n",
    "    # Find the specific product link using a precise CSS selector\n",
    "    for a in soup.select('a.product-card-image__link'):\n",
    "        print(f\"Checking link: {a['href']}\")\n",
    "        if product_id.lower() in a['href'].lower():\n",
    "            product_url = urljoin(search_url, a['href'])\n",
    "            print(f\"Found product URL: {product_url}\")\n",
    "            return product_url\n",
    "\n",
    "    print(\"Product URL not found.\")\n",
    "    return None\n",
    "\n",
    "def find_reviews_link(product_url):\n",
    "    print(f\"Finding reviews link on product URL: {product_url}\")\n",
    "    soup = get_page_soup(product_url)\n",
    "\n",
    "    # Find the reviews link using a precise CSS selector\n",
    "    for a in soup.select('a'):\n",
    "        print(f\"Checking link: {a['href']}\")\n",
    "        if 'reviews' in a['href']:\n",
    "            reviews_url = urljoin(product_url, a['href'])\n",
    "            print(f\"Found reviews URL: {reviews_url}\")\n",
    "            return reviews_url\n",
    "\n",
    "    print(\"Reviews URL not found.\")\n",
    "    return None\n",
    "\n",
    "def navigate_to_reviews(product_id):\n",
    "    print(f\"Navigating to reviews for product ID: {product_id}\")\n",
    "    product_url = search_product(product_id)\n",
    "    if product_url:\n",
    "        reviews_url = find_reviews_link(product_url)\n",
    "        if reviews_url:\n",
    "            print(f\"Navigating to reviews URL: {reviews_url}\")\n",
    "            driver.get(reviews_url)\n",
    "            time.sleep(1)  # Wait for the reviews page to load\n",
    "            # Extract page source after page load\n",
    "            html_content = driver.page_source\n",
    "        \n",
    "            # Pass HTML content to extract_reviews function\n",
    "            reviews_data = extract_reviews(html_content)\n",
    "            \n",
    "            # Print extracted reviews data\n",
    "            print(\"\\nExtracted Reviews:\")\n",
    "            for review in reviews_data:\n",
    "                print(review)\n",
    "        else:\n",
    "            print(\"Reviews URL not found.\")\n",
    "    else:\n",
    "        print(\"Product URL not found.\")\n",
    "\n",
    "product_id = '1VCE8'\n",
    "navigate_to_reviews(product_id)\n",
    "\n",
    "driver.quit()\n"
   ],
   "id": "47c96d4b1c533ca6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to reviews for product ID: 1VCE8\n",
      "Searching for product ID: 1VCE8\n",
      "Getting page soup for URL: https://www.zoro.com/search?q=1VCE8\n",
      "Checking link: /dayton-standard-duty-industrial-fan-24-non-oscillating-115vac-38506200-cfm-1vce8/i/G1828802/\n",
      "Found product URL: https://www.zoro.com/dayton-standard-duty-industrial-fan-24-non-oscillating-115vac-38506200-cfm-1vce8/i/G1828802/\n",
      "Finding reviews link on product URL: https://www.zoro.com/dayton-standard-duty-industrial-fan-24-non-oscillating-115vac-38506200-cfm-1vce8/i/G1828802/\n",
      "Getting page soup for URL: https://www.zoro.com/dayton-standard-duty-industrial-fan-24-non-oscillating-115vac-38506200-cfm-1vce8/i/G1828802/\n",
      "Checking link: #main-content\n",
      "Checking link: /safety/c/z1/\n",
      "Checking link: /office-school-shipping-supplies/c/z9/\n",
      "Checking link: /heating-cooling/c/30/\n",
      "Checking link: /tools-machining/c/z2/\n",
      "Checking link: /abrasives-polishers/c/1/\n",
      "Checking link: /adhesives-tapes/c/4/\n",
      "Checking link: /electrical-supplies/c/23/\n",
      "Checking link: /electronics/c/15050/\n",
      "Checking link: /fasteners/c/9/\n",
      "Checking link: /food-service-restaurant-supplies/c/67/\n",
      "Checking link: /furniture-linens-decor/c/530/\n",
      "Checking link: /grounds-outdoor/c/21/\n",
      "Checking link: /hardware-building-materials/c/27/\n",
      "Checking link: /heating-cooling/c/30/\n",
      "Checking link: /janitorial-cleaning-supplies/c/25/\n",
      "Checking link: /lab-equipment-supplies/c/z50/\n",
      "Checking link: /lighting/c/22/\n",
      "Checking link: /material-handling/c/z4/\n",
      "Checking link: /medical-personal-care/c/76/\n",
      "Checking link: /office-school-shipping-supplies/c/z9/\n",
      "Checking link: /paint-coatings-supplies/c/26/\n",
      "Checking link: /plumbing/c/z3/\n",
      "Checking link: /power-transmission/c/z8/\n",
      "Checking link: /pumps/c/39/\n",
      "Checking link: /raw-materials/c/37/\n",
      "Checking link: /safety/c/z1/\n",
      "Checking link: /storage-workspace/c/15/\n",
      "Checking link: /test-instruments-gauges/c/24/\n",
      "Checking link: /tools-machining/c/z2/\n",
      "Checking link: /vehicle-maintenance/c/z10/\n",
      "Checking link: /welding-soldering-brazing/c/6/\n",
      "Checking link: /about/\n",
      "Checking link: /coupons-promo-codes/\n",
      "Checking link: /contact/\n",
      "Checking link: /\n",
      "Checking link: /\n",
      "Checking link: /customer-service\n",
      "Checking link: /cart\n",
      "Checking link: /\n",
      "Checking link: /heating-cooling/c/30/\n",
      "Checking link: /fans/c/4583/\n",
      "Checking link: /air-circulators/c/4961/\n",
      "Checking link: /dayton-standard-duty-industrial-fan-24-non-oscillating-115vac-38506200-cfm-1vce8/i/G1828802/\n",
      "Checking link: /air-circulators/c/4961/\n",
      "Checking link: /b/DAYTON/\n",
      "Checking link: /dayton-standard-duty-industrial-fan-24-non-oscillating-115vac-38506200-cfm-1vce8/i/G1828802/#reviews\n",
      "Found reviews URL: https://www.zoro.com/dayton-standard-duty-industrial-fan-24-non-oscillating-115vac-38506200-cfm-1vce8/i/G1828802/#reviews\n",
      "Navigating to reviews URL: https://www.zoro.com/dayton-standard-duty-industrial-fan-24-non-oscillating-115vac-38506200-cfm-1vce8/i/G1828802/#reviews\n",
      "Star Rating: Rated 4.0 out of 5 stars, 4.0\n",
      "Recommendation Percentage: 100%\n",
      "\n",
      "Processing review 1:\n",
      "Review Text: thank you\n",
      "\n",
      "Processing review 2:\n",
      "Review Text not found.\n",
      "\n",
      "Processing review 3:\n",
      "Review Text not found.\n",
      "\n",
      "Processing review 4:\n",
      "Review Text: this fan moves a lot of air has held up well so far\n",
      "\n",
      "Processing review 5:\n",
      "Review Text not found.\n",
      "\n",
      "Processing review 6:\n",
      "Review Text not found.\n",
      "\n",
      "Extracted Reviews:\n",
      "{'Star Rating': 'Rated 4.0 out of 5 stars', 'Rating Text': '4.0', 'Review Text': 'thank you'}\n",
      "{'Star Rating': 'Rated 4.0 out of 5 stars', 'Rating Text': '4.0', 'Review Text': None}\n",
      "{'Star Rating': 'Rated 4.0 out of 5 stars', 'Rating Text': '4.0', 'Review Text': None}\n",
      "{'Star Rating': 'Rated 4.0 out of 5 stars', 'Rating Text': '4.0', 'Review Text': 'this fan moves a lot of air has held up well so far'}\n",
      "{'Star Rating': 'Rated 4.0 out of 5 stars', 'Rating Text': '4.0', 'Review Text': None}\n",
      "{'Star Rating': 'Rated 4.0 out of 5 stars', 'Rating Text': '4.0', 'Review Text': None}\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T03:08:45.544386Z",
     "start_time": "2024-06-24T03:08:43.921691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# \n",
    "# def navigate_to_reviews(product_id):\n",
    "#     url = f\"https://www.grainger.com/product/{product_id}\"\n",
    "#     headers = {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "#     }\n",
    "#     \n",
    "#     response = requests.get(url, headers=headers)\n",
    "#     \n",
    "#     if response.status_code == 200:\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#         \n",
    "#         reviews_section = soup.find('section', {'id': 'pr-review-display', 'data-testid': 'review-list'})\n",
    "#         \n",
    "#         if reviews_section:\n",
    "#             reviews = reviews_section.find_all('div', {'class': 'pr-review pr-review-condensed'})\n",
    "#             \n",
    "#             for review in reviews:\n",
    "#                 review_text = {}\n",
    "#                 \n",
    "#                 # Extract headline\n",
    "#                 headline = review.find('h5', {'class': 'pr-rd-review-headline'})\n",
    "#                 if headline:\n",
    "#                     review_text['headline'] = headline.text.strip()\n",
    "#                 \n",
    "#                 # Extract submission date\n",
    "#                 submission_date = review.find('span', {'data-datetime': True})\n",
    "#                 if submission_date:\n",
    "#                     review_text['submission_date'] = submission_date.text.strip()\n",
    "#                 \n",
    "#                 # Extract author name\n",
    "#                 author_name = review.find('span', {'class': 'pr-rd-author-nickname'})\n",
    "#                 if author_name:\n",
    "#                     review_text['author'] = author_name.text.strip().split('By ')[-1]\n",
    "#                 \n",
    "#                 # Extract review text\n",
    "#                 review_body = review.find('p', {'class': 'pr-rd-description-text'})\n",
    "#                 if review_body:\n",
    "#                     review_text['review_text'] = review_body.text.strip()\n",
    "#                 \n",
    "#                 print(review_text)  # Print for debugging or store in a list/dictionary\n",
    "#         else:\n",
    "#             print(\"Reviews section not found on the page.\")\n",
    "#     else:\n",
    "#         print(f\"Failed to retrieve page with status code {response.status_code}\")\n",
    "# \n",
    "# # Example usage:\n",
    "# product_id = \"1VCE8\"\n",
    "# navigate_to_reviews(product_id)\n"
   ],
   "id": "b91b56245d8c3cdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews section not found on the page.\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T03:07:32.002459Z",
     "start_time": "2024-06-24T03:07:31.292368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# \n",
    "# def navigate_to_reviews(url):\n",
    "# \n",
    "#     try:\n",
    "#         response = requests.get(url, headers=headers)\n",
    "#         if response.status_code == 200:\n",
    "#             print(f\"Successfully connected to {url}\")\n",
    "#             return response.content\n",
    "#         else:\n",
    "#             print(f\"Failed to connect to {url}. Status code: {response.status_code}\")\n",
    "#             return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {str(e)}\")\n",
    "#         return None\n",
    "# \n",
    "# def extract_reviews(html_content):\n",
    "#     if not html_content:\n",
    "#         print(\"No HTML content to process.\")\n",
    "#         return\n",
    "# \n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "#     review_sections = soup.find_all('div', class_='pr-review pr-review-condensed')\n",
    "# \n",
    "#     reviews_data = []\n",
    "# \n",
    "#     for idx, review in enumerate(review_sections, start=1):\n",
    "#         print(f\"\\nProcessing review {idx}:\")\n",
    "#         \n",
    "#         # Extracting star rating\n",
    "#         star_rating = review.find('div', class_='pr-rating-stars')\n",
    "#         if star_rating:\n",
    "#             print(\"Star Rating:\", star_rating['aria-label'])\n",
    "#         \n",
    "#         # Extracting review headline\n",
    "#         review_headline = review.find('h5', class_='pr-rd-review-headline')\n",
    "#         if review_headline:\n",
    "#             print(\"Review Headline:\", review_headline.text.strip())\n",
    "#         \n",
    "#         # Extracting submission details\n",
    "#         submission_details = review.find('p', class_='pr-rd-author-submission-date')\n",
    "#         if submission_details:\n",
    "#             print(\"Submission Details:\", submission_details.text.strip())\n",
    "#         \n",
    "#         # Extracting reviewer details\n",
    "#         reviewer_details = review.find('p', class_='pr-rd-author-nickname')\n",
    "#         if reviewer_details:\n",
    "#             print(\"Reviewer Details:\", reviewer_details.text.strip())\n",
    "#         \n",
    "#         # Extracting review text\n",
    "#         review_text = review.find('p', class_='pr-rd-description-text')\n",
    "#         if review_text:\n",
    "#             print(\"Review Text:\", review_text.text.strip())\n",
    "#         \n",
    "#         # Adding all data to reviews_data\n",
    "#         reviews_data.append({\n",
    "#             'Star Rating': star_rating['aria-label'].strip() if star_rating else None,\n",
    "#             'Review Headline': review_headline.text.strip() if review_headline else None,\n",
    "#             'Submission Details': submission_details.text.strip() if submission_details else None,\n",
    "#             'Reviewer Details': reviewer_details.text.strip() if reviewer_details else None,\n",
    "#             'Review Text': review_text.text.strip() if review_text else None\n",
    "#         })\n",
    "# \n",
    "#     return reviews_data\n",
    "# # \n",
    "# # # Example usage:\n",
    "# # def main():\n",
    "# #     product_id = '1VCE8'\n",
    "# #     html_content = navigate_to_reviews(product_id)\n",
    "# # \n",
    "# #     if html_content:\n",
    "# #         reviews_data = extract_reviews(html_content)\n",
    "# #         print(\"\\nExtracted Reviews:\")\n",
    "# #         for review in reviews_data:\n",
    "# #             print(review)\n",
    "# # \n",
    "# # if __name__ == \"__main__\":\n",
    "# #     main()\n"
   ],
   "id": "dcc58886993775b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to https://www.grainger.com/product/1VCE8\n",
      "\n",
      "Extracted Reviews:\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T02:16:00.205306Z",
     "start_time": "2024-06-24T02:15:13.226738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "product = '1VCE8'\n",
    "urls = [f'https://www.zoro.com/search?q={product}']  # List of URLs obtained from the search engine\n",
    "reviews = [crawl(url, product) for url in urls]\n",
    "print(reviews)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.zoro.com/search?q=1VCE8\n",
      "https://www.zoro.com/search?q=1VCE8&fqc%3Acategory=30#search-container\n",
      "https://www.zoro.com/search?q=1VCE8&fqc%3Acategory=30%2F4583#search-container\n",
      "https://www.zoro.com/search?q=1VCE8&fqc%3Acategory=30%2F4583%2F4961#search-container\n",
      "https://www.zoro.com/search?fqc%3Acategory=30%2F4583%2F4961&q=1VCE8\n",
      "https://www.zoro.com/search?fqc%3Acategory=30%2F4583&q=1VCE8#search-container\n",
      "https://www.zoro.com/search?fqc%3Acategory=30%2F4583%2F4961&q=1VCE8#search-container\n",
      "https://www.zoro.com/search?fqc%3Acategory=30&q=1VCE8#search-container\n",
      "https://www.zoro.com/search?fqc%3Acategory=30%2F4583&q=1VCE8\n",
      "https://www.zoro.com/search?fqc%3Acategory=30&q=1VCE8\n",
      "[None]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7690dd191b271eac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T01:29:58.878222Z",
     "start_time": "2024-06-24T01:29:58.875462Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1f4b28f423b7ec61",
   "outputs": [],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
